{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Station characterization functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<h3>Import modules and functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebooks with functions: look over and delete the ones that is not used\n",
    "#possibly put all the functions that are used in \n",
    "%run ./STILT_modules_v2.5.ipynb\n",
    "%run ./ICOS_atmObs_modules_v2.5.ipynb\n",
    "\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import math\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import math\n",
    "from netCDF4 import Dataset\n",
    "import textwrap\n",
    "import datetime as dt\n",
    "import seaborn\n",
    "import os\n",
    "import six\n",
    "import requests\n",
    "\n",
    "#Import ICOS tools - will remain. \n",
    "from icoscp.station import station as station_data\n",
    "\n",
    "#for the widgets\n",
    "from IPython.core.display import display, HTML \n",
    "from ipywidgets import interact, interact_manual, Dropdown, SelectMultiple, HBox, VBox, Button, Output, FloatText, IntText, IntRangeSlider, RadioButtons,IntProgress, Checkbox, GridspecLayout\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "#stations that have footprints as well as year and months with footprints. Also altitude. \n",
    "stations = create_STILT_dictionary()\n",
    "\n",
    "#path to footprints\n",
    "pathFP='/data/stiltweb/stations/'\n",
    "\n",
    "#Earth's radius in km (for calculating distances between the station and cells)\n",
    "R = 6373.8\n",
    "\n",
    "#saved distances to the 192 000 cells for all the labeled atmospheric stations\n",
    "#if the selected station is not found in this document, the distances are calculated\n",
    "approved_stations_distances = pd.read_csv('approved_stations_distances.csv')\n",
    "\n",
    "#saved degree angles from the stations to all 192 000 cells for all the labeled atmospheric stations\n",
    "approved_stations_degrees = pd.read_csv('approved_stations_degrees.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Expand the widget</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "<h3>Define date range</h3>\n",
    "<p>different ways depending on if doing it in the widget selection or \"the old way\" where user \n",
    "answer input() questions. The second option is used when running the characterization functions outside the widgets.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the widget selection:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start- and end date is from the widget selection\n",
    "def date_range_station_char(start_date, end_date, timeselect_list):\n",
    "    \n",
    "    date_range = pd.date_range(start_date, end_date, freq='3H')\n",
    "\n",
    "    #depending on how many input (max 8 for 0 3 6 9 12 15 18 21), filter to include hours.\n",
    "    for time_value in timeselect_list:\n",
    "        if len(timeselect_list)==1:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)]\n",
    "            #df_nine = df.loc[(timeselect_list[count_timeselect] == df.index.hour)]\n",
    "        if len(timeselect_list)==2:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]\n",
    "        if len(timeselect_list)==3:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==4:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==5:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==6:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)] | date_range[(timeselect_list[5] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==7:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)] | date_range[(timeselect_list[5] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[6] == date_range.hour)]\n",
    "        \n",
    "        if len(timeselect_list)==8:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)] | date_range[(timeselect_list[5] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[6] == date_range.hour)] | date_range[(timeselect_list[7] == date_range.hour)]\n",
    "          \n",
    "    #consider return timeselect\n",
    "    return date_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When running the characterization functions independently:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to \n",
    "def date_range_date_hour_upd(): \n",
    "    year_start=input(\"Choose start date year (vary for station, earliest 2006): \")\n",
    "    month_start=input(\"Choose start date month (write 1-12): \")\n",
    "    day_start=input(\"Choose start date day (write number): \")\n",
    "\n",
    "    start_date=dt.datetime(int(year_start),int(month_start),int(day_start),0)\n",
    "\n",
    "    year_end=input(\"Choose end date year (vary for station, earliest 2006): \")\n",
    "    month_end=input(\"Choose end date month (write 1-12): \")\n",
    "    day_end=input(\"Choose end date day (write number): \")\n",
    "    \n",
    "    end_date=dt.datetime(int(year_end), int(month_end), int(day_end),0)-dt.timedelta(hours=3)\n",
    "    \n",
    "    date_range = pd.date_range(start_date, end_date, freq='3H')\n",
    "\n",
    "    #have to put it in with blank spaces between each number. \n",
    "    timeselect= input(\"Choose which footrpints to display: (0 3 6 9 12 15 18 21) \")\n",
    "\n",
    "    timeselect_list = timeselect.split()\n",
    "    timeselect_list = [int(a) for a in timeselect_list] \n",
    "\n",
    "    #depending on how many input (max 8 for 0 3 6 9 12 15 18 21), filter to include hours.\n",
    "    for time_value in timeselect_list:\n",
    "        \n",
    "        if len(timeselect_list)==1:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)]\n",
    "            \n",
    "        if len(timeselect_list)==2:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]\n",
    "        \n",
    "        if len(timeselect_list)==3:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==4:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==5:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==6:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)] | date_range[(timeselect_list[5] == date_range.hour)]\n",
    "\n",
    "        if len(timeselect_list)==7:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)] | date_range[(timeselect_list[5] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[6] == date_range.hour)]\n",
    "        \n",
    "        if len(timeselect_list)==8:\n",
    "            date_range = date_range[(timeselect_list[0] == date_range.hour)] | date_range[(timeselect_list[1] == date_range.hour)]  \\\n",
    "            | date_range[(timeselect_list[2] == date_range.hour)] | date_range[(timeselect_list[3] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[4] == date_range.hour)] | date_range[(timeselect_list[5] == date_range.hour)]\\\n",
    "            | date_range[(timeselect_list[6] == date_range.hour)] | date_range[(timeselect_list[7] == date_range.hour)]\n",
    "          \n",
    "    #return everything that could be useful. \n",
    "    return date_range, timeselect, start_date, end_date, timeselect_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "<h3>Functions to import data</h3>\n",
    "<p>Import ancillary data from NetCDF: landcover, population and point source.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_landcover():\n",
    "    all_corine_classes= Dataset('all_corine_except_ocean.nc')\n",
    "\n",
    "    #the \"onceans_finalized\" dataset is seperate: CORINE class 523 (oceans) did not extend beyond exclusive zone\n",
    "    #complemented with Natural Earth data.\n",
    "    #CORINE does not cover the whole area, \"nodata\" area is never ocean, rather landbased data.\n",
    "    oceans_finalized= Dataset('oceans_finalized.nc')\n",
    "\n",
    "    #define lat and lon if want to display the data on a map: \n",
    "    lon=all_corine_classes.variables['lon'][:]\n",
    "    lat=all_corine_classes.variables['lat'][:]\n",
    "\n",
    "    #access all the different land cover classes in the .nc files:\n",
    "    fp_111 = all_corine_classes.variables['area_111'][:,:]\n",
    "    fp_112 = all_corine_classes.variables['area_112'][:,:]\n",
    "    fp_121 = all_corine_classes.variables['area_121'][:,:]\n",
    "    fp_122 = all_corine_classes.variables['area_122'][:,:]\n",
    "    fp_123 = all_corine_classes.variables['area_123'][:,:]\n",
    "    fp_124 = all_corine_classes.variables['area_124'][:,:]\n",
    "    fp_131 = all_corine_classes.variables['area_131'][:,:]\n",
    "    fp_132 = all_corine_classes.variables['area_132'][:,:]\n",
    "    fp_133 = all_corine_classes.variables['area_133'][:,:]\n",
    "    fp_141 = all_corine_classes.variables['area_141'][:,:]\n",
    "    fp_142 = all_corine_classes.variables['area_142'][:,:]\n",
    "    fp_211 = all_corine_classes.variables['area_211'][:,:]\n",
    "    fp_212 = all_corine_classes.variables['area_212'][:,:]\n",
    "    fp_213 = all_corine_classes.variables['area_213'][:,:]\n",
    "    fp_221 = all_corine_classes.variables['area_221'][:,:]\n",
    "    fp_222 = all_corine_classes.variables['area_222'][:,:]\n",
    "    fp_223 = all_corine_classes.variables['area_223'][:,:]\n",
    "    fp_231 = all_corine_classes.variables['area_231'][:,:]\n",
    "    fp_241 = all_corine_classes.variables['area_241'][:,:]\n",
    "    fp_242 = all_corine_classes.variables['area_242'][:,:]\n",
    "    fp_243 = all_corine_classes.variables['area_243'][:,:]\n",
    "    fp_244 = all_corine_classes.variables['area_244'][:,:]\n",
    "    fp_311 = all_corine_classes.variables['area_311'][:,:]\n",
    "    fp_312 = all_corine_classes.variables['area_312'][:,:]\n",
    "    fp_313 = all_corine_classes.variables['area_313'][:,:]\n",
    "    fp_321 = all_corine_classes.variables['area_321'][:,:]\n",
    "    fp_322 = all_corine_classes.variables['area_322'][:,:]\n",
    "    fp_323 = all_corine_classes.variables['area_323'][:,:]\n",
    "    fp_324 = all_corine_classes.variables['area_324'][:,:]\n",
    "    fp_331 = all_corine_classes.variables['area_331'][:,:]\n",
    "    fp_332 = all_corine_classes.variables['area_332'][:,:]\n",
    "    fp_333 = all_corine_classes.variables['area_333'][:,:]\n",
    "    fp_334 = all_corine_classes.variables['area_334'][:,:]\n",
    "    fp_335 = all_corine_classes.variables['area_335'][:,:]\n",
    "    fp_411 = all_corine_classes.variables['area_411'][:,:]\n",
    "    fp_412 = all_corine_classes.variables['area_412'][:,:]\n",
    "    fp_421 = all_corine_classes.variables['area_421'][:,:]\n",
    "    fp_422 = all_corine_classes.variables['area_422'][:,:]\n",
    "    fp_423 = all_corine_classes.variables['area_423'][:,:]\n",
    "    fp_511 = all_corine_classes.variables['area_511'][:,:]\n",
    "    fp_512 = all_corine_classes.variables['area_512'][:,:]\n",
    "    fp_521 = all_corine_classes.variables['area_521'][:,:]\n",
    "    fp_522 = all_corine_classes.variables['area_522'][:,:]\n",
    "\n",
    "    #CORINE combined with natural earth data for oceans:\n",
    "    fp_523 = oceans_finalized.variables['ocean_ar2'][:,:]\n",
    "\n",
    "    #have a variable that represents the whole area of the cell,\n",
    "    #used to get a percentage breakdown of each corine class.\n",
    "    fp_total_area = all_corine_classes.variables['area_stilt'][:,:]\n",
    "\n",
    "    #19 aggregated classes (these are used in the current bar graphs but can be updated by each user)\n",
    "    urban = fp_111+fp_112+fp_141+fp_142\n",
    "    industrial = fp_131 + fp_133 + fp_121 \n",
    "    road_and_rail = fp_122 \n",
    "    ports_and_apirports= fp_123+fp_124\n",
    "    dump_sites = fp_132\n",
    "    staple_cropland_not_rice = fp_211 + fp_212 + fp_241 + fp_242 + fp_243\n",
    "    rice_fields = fp_213\n",
    "    cropland_fruit_berry_grapes_olives = fp_221 + fp_222 + fp_223\n",
    "    pastures = fp_231\n",
    "    broad_leaved_forest = fp_311\n",
    "    coniferous_forest = fp_312\n",
    "    mixed_forest = fp_313 + fp_244\n",
    "    natural_grasslands = fp_321 + fp_322\n",
    "    transitional_woodland_shrub= fp_323 + fp_324\n",
    "    bare_natural_areas = fp_331 + fp_332 + fp_333 + fp_334\n",
    "    glaciers_prepetual_snow = fp_335\n",
    "    wet_area= fp_411 + fp_412 + fp_421 + fp_422\n",
    "    inland_water_bodies = fp_423 + fp_511 + fp_512 + fp_521 + fp_522\n",
    "    oceans = fp_523\n",
    "\n",
    "    #added: the \"missing area\" is out of the CORINE domain. Alltogether add upp to \"fp_total_area\"\n",
    "    out_of_domain=fp_total_area-oceans-inland_water_bodies-wet_area-glaciers_prepetual_snow-bare_natural_areas-transitional_woodland_shrub-natural_grasslands-mixed_forest-coniferous_forest-broad_leaved_forest-pastures-cropland_fruit_berry_grapes_olives-rice_fields-staple_cropland_not_rice-dump_sites-ports_and_apirports-road_and_rail-industrial-urban\n",
    "\n",
    "    #further aggregated classes for the land cover wind polar graph and land cover bar graph\n",
    "    urban_aggreg= urban + industrial + road_and_rail + dump_sites + ports_and_apirports\n",
    "    cropland_aggreg= staple_cropland_not_rice + rice_fields + cropland_fruit_berry_grapes_olives\n",
    "    forests= broad_leaved_forest + coniferous_forest + mixed_forest\n",
    "    pastures_grasslands= pastures + natural_grasslands\n",
    "    oceans=oceans\n",
    "    other=transitional_woodland_shrub+bare_natural_areas+glaciers_prepetual_snow +wet_area + inland_water_bodies\n",
    "\n",
    "    return out_of_domain, urban_aggreg, cropland_aggreg, forests, pastures_grasslands, oceans, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_population_data():\n",
    "    pop_data= Dataset('point_with_pop_data.nc')\n",
    "    fp_pop=pop_data.variables['Sum_TOT_P'][:,:]\n",
    "    \n",
    "    return fp_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_point_source_data():\n",
    "    #point source:\n",
    "    point_source_data= Dataset('final_netcdf_point_source_emission.nc')\n",
    "\n",
    "    lon=point_source_data.variables['lon'][:]\n",
    "    lat=point_source_data.variables['lat'][:]\n",
    "\n",
    "    #emissions in kg/year in the variable \"Sum_Tota_1\"\n",
    "    fp_point_source=point_source_data.variables['Sum_Tota_1'][:,:]\n",
    "\n",
    "    #different from population data: can translate the emissions within each stilt cell to the effect it will have to the final CO2 concentrations at the stations.\n",
    "    #just need to get it in the right unit (micromole/m2s) and multiply by the individual or aggregated footprints\n",
    "\n",
    "    #divide by the molar weight in kg. 12 (C)+16(O)+16(O) =44 0.044 in kg. get number of moles of C this way. Want it in micromole though: 1 mole= 1000000 micromole\n",
    "    fp_point_source_moles_C=fp_point_source/0.044\n",
    "\n",
    "    #how many micro-mole is that? multiply by 1000000\n",
    "    fp_point_source_micromoles_C=fp_point_source_moles_C*1000000\n",
    "\n",
    "    #a NetCDF file with the grid size calues in m2\n",
    "    f_gridarea = cdf.Dataset('gridareaSTILT.nc')\n",
    "\n",
    "    #area stored in \"cell_area\"\n",
    "    gridarea = f_gridarea.variables['cell_area'][:]\n",
    "\n",
    "    fp_point_source_m2= fp_point_source_micromoles_C/gridarea\n",
    "\n",
    "    #how many micro moles let out per second (have yearly data)\n",
    "    fp_point_source_m2_s= fp_point_source_m2/31536000\n",
    "    \n",
    "    return fp_point_source_m2_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "<h3>Maps with cells binned by defined intervals and directions</h3>\n",
    "<p>Functions used in the creating of the maps:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with labels (depending on input to this function)\n",
    "def nondirection_labels(bins, units):   \n",
    "    labels = []\n",
    "    \n",
    "    #for the label - want bin before and after (range)\n",
    "    for left, right in zip(bins[:-1], bins[1:]):\n",
    "        \n",
    "        #if the last object - everything above (>value unit)\n",
    "        if np.isinf(right):\n",
    "            labels.append('>{} {}'.format(left, units))\n",
    "        else:\n",
    "            \n",
    "            #how the labels normally look (value - value unit)\n",
    "            labels.append('{} - {} {}'.format(left, right, units))\n",
    "\n",
    "    return list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_initial_compass_bearing(pointA, pointB):\n",
    "    \"\"\"\n",
    "    Calculates the bearing between two points.\n",
    "    The formulae used is the following:\n",
    "        θ = atan2(sin(Δlong).cos(lat2),\n",
    "                  cos(lat1).sin(lat2) − sin(lat1).cos(lat2).cos(Δlong))\n",
    "    :Parameters:\n",
    "      - `pointA: The tuple representing the latitude/longitude for the\n",
    "        first point. Latitude and longitude must be in decimal degrees\n",
    "      - `pointB: The tuple representing the latitude/longitude for the\n",
    "        second point. Latitude and longitude must be in decimal degrees\n",
    "    :Returns:\n",
    "      The bearing in degrees\n",
    "    :Returns Type:\n",
    "      float\n",
    "    \"\"\"\n",
    "    if (type(pointA) != tuple) or (type(pointB) != tuple):\n",
    "        raise TypeError(\"Only tuples are supported as arguments\")\n",
    "\n",
    "    lat1 = math.radians(pointA[0])\n",
    "    lat2 = math.radians(pointB[0])\n",
    "\n",
    "    diffLong = math.radians(pointB[1] - pointA[1])\n",
    "\n",
    "    x = math.sin(diffLong) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1)\n",
    "            * math.cos(lat2) * math.cos(diffLong))\n",
    "\n",
    "    initial_bearing = math.atan2(x, y)\n",
    "\n",
    "    # Now we have the initial bearing but math.atan2 return values\n",
    "    # from -180° to + 180° which is not what we want for a compass bearing\n",
    "    # The solution is to normalize the initial bearing as shown below\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "\n",
    "    return compass_bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_bins_maprose(km_intervals, bin_size):\n",
    "        \n",
    "    #the number of bins\n",
    "    number_bins=round((5000/km_intervals),0)\n",
    "    \n",
    "    #start at 0 km of the station. Then append to this list\n",
    "    interval_bins=[0]\n",
    "    \n",
    "    #ex 100, 200, 300 if km_intervals=100\n",
    "    for number in range(1, int(number_bins)):\n",
    "        interval_bins.append(km_intervals*number)\n",
    "    \n",
    "    #the last number is infinity - however far the domain stretches marks the end of the last bin.\n",
    "    interval_bins.append(np.inf)\n",
    "    \n",
    "    #labels: not used in map - but used in the grouping\n",
    "    interval_labels = nondirection_labels(interval_bins, units='km')\n",
    "\n",
    "    #direction: using the input (degree_bin) to set the bins so that the first bin has \"north (0 degrees) in the middle\"\n",
    "    #\"from_degree\" becomes a negative value (half of the degree value \"to the left\" of 0)\n",
    "    from_degree=-(bin_size/2)\n",
    "\n",
    "    #\"to_degree\" is a vale to indicate the last bins ending. Must check values all the way to 360 which means the last bin \n",
    "    #will go past 360 and later be joined with the \"0\" bin (replace function in next cell)\n",
    "    to_degree= 360 + (bin_size/2) + 1\n",
    "\n",
    "    #the bin_size is the \"step\". generate an array with all the direction bins\n",
    "    dir_bins = np.arange(from_degree, to_degree, bin_size)\n",
    "\n",
    "    #the direction bin is the first bin + the next bin divided by two:\n",
    "    dir_labels = (dir_bins[:-1] + dir_bins[1:]) / 2\n",
    "    \n",
    "    #return these values to use in the function map_representation_polar_graph\n",
    "    return interval_bins, interval_labels, dir_bins, dir_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot maps (show station location if station is provided and zoom in second plot if zoom is provided)\n",
    "def plot_maps(field, lon, lat, title='', label='', unit='', linlog='linear', station='', zoom='', \n",
    "              vmin=None, vmax=None, colors='GnBu',pngfile=''): \n",
    "\n",
    "    mcolor='m'\n",
    "    \n",
    "    # Set scale for features from Natural Earth\n",
    "    NEscale = '50m'\n",
    "\n",
    "    # Create a feature for Countries at 1:50m from Natural Earth\n",
    "    countries = cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_0_countries',\n",
    "        scale=NEscale,\n",
    "        facecolor='none')\n",
    "\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "\n",
    "    # set up a map\n",
    "    ax = plt.subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "    img_extent = (lon.min(), lon.max(), lat.min(), lat.max())\n",
    "    ax.set_extent([lon.min(), lon.max(), lat.min(), lat.max()],crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(countries, edgecolor='black', linewidth=0.3)\n",
    "\n",
    "    cmap = plt.get_cmap(colors)\n",
    "    \n",
    "    cmap.set_under(color='white')  \n",
    "    if linlog == 'linear':\n",
    "        \n",
    "        im = ax.imshow(field[:,:],interpolation=None,origin='lower', extent=img_extent,cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "        cbar=plt.colorbar(im,orientation='horizontal',pad=0.03,fraction=0.055,extend='both')\n",
    "        cbar.set_label(label+'  '+unit)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        im = ax.imshow(np.log10(field)[:,:],interpolation='none',origin='lower', extent=img_extent,cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "        cbar=plt.colorbar(im,orientation='horizontal',pad=0.03,fraction=0.055,extend='both')\n",
    "        cbar.set_label(label+'  log$_{10}$ '+unit)\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    ax.text(0.01, -0.25, 'min: %.2f' % np.min(field[:,:]), horizontalalignment='left',transform=ax.transAxes)\n",
    "    ax.text(0.99, -0.25, 'max: %.2f' % np.max(field[:,:]), horizontalalignment='right',transform=ax.transAxes)\n",
    "    \n",
    "    #show station location if station is provided\n",
    "    if station != '':\n",
    "        station_lon=[]\n",
    "        station_lat=[]\n",
    "        station_lon.append(stations[station]['lon'])\n",
    "        station_lat.append(stations[station]['lat'])\n",
    "        ax.plot(station_lon,station_lat,'+',color=mcolor,ms=10,markeredgewidth=1,transform=ccrs.PlateCarree())\n",
    "        \n",
    "    zoom=str(zoom)\n",
    "    if zoom != '':\n",
    "        \n",
    "        #grid cell index of station \n",
    "        ix,jy = lonlat_2_ixjy(stations[zoom]['lon'],stations[zoom]['lat'],lon,lat)\n",
    " \n",
    "        # define zoom area \n",
    "        i1 = np.max([ix-35,0])\n",
    "        i2 = np.min([ix+35,400])\n",
    "        j1 = np.max([jy-42,0])\n",
    "        j2 = np.min([jy+42,480])\n",
    "\n",
    "        lon_z=lon[i1:i2]\n",
    "        lat_z=lat[j1:j2]\n",
    "\n",
    "        field_z=field[j1:j2,i1:i2]\n",
    "\n",
    "        # set up a map\n",
    "        ax = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "        img_extent = (lon_z.min(), lon_z.max(), lat_z.min(), lat_z.max())\n",
    "        ax.set_extent([lon_z.min(), lon_z.max(), lat_z.min(), lat_z.max()],crs=ccrs.PlateCarree())\n",
    "        ax.add_feature(countries, edgecolor='black', linewidth=0.3)\n",
    "    \n",
    "        if linlog == 'linear':\n",
    "            im = ax.imshow(field_z,interpolation='none',origin='lower', extent=img_extent,cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "            cbar=plt.colorbar(im,orientation='horizontal',pad=0.03,fraction=0.055,extend='both')\n",
    "            cbar.set_label(label+'  '+unit)\n",
    "        else:\n",
    "            im = ax.imshow(np.log10(field_z),interpolation='none',origin='lower', extent=img_extent,cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "            cbar=plt.colorbar(im,orientation='horizontal',pad=0.03,fraction=0.055,extend='both')\n",
    "            cbar.set_label(label+'  log$_{10}$ '+unit)\n",
    "\n",
    "        #show station location if station is provided\n",
    "        if station != '':\n",
    "            station_lon=[]\n",
    "            station_lat=[]\n",
    "            station_lon.append(stations[station]['lon'])\n",
    "            station_lat.append(stations[station]['lat'])\n",
    "            ax.plot(station_lon,station_lat,'+',color=mcolor,ms=10,markeredgewidth=1,transform=ccrs.PlateCarree())\n",
    "        plt.title(title)\n",
    "        ax.text(0.01, -0.25, 'min: %.2f' % np.min(field[j1:j2,i1:i2]), horizontalalignment='left',transform=ax.transAxes)\n",
    "        ax.text(0.99, -0.25, 'max: %.2f' % np.max(field[j1:j2,i1:i2]), horizontalalignment='right',transform=ax.transAxes)\n",
    "  \n",
    "    plt.show()\n",
    "    if len(pngfile)>0:\n",
    "        plotdir='figures'\n",
    "        if not os.path.exists(plotdir):\n",
    "            os.mkdir(plotdir)\n",
    "        \n",
    "        fig.savefig(plotdir+'/'+pngfile+'.pdf',dpi=100,bbox_inches='tight')\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This function used to create polar graph maps for sensitivity, population sensitivity \n",
    "and point source contribution</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_representation_polar_graph(station, date_range, timeselect, bin_size, unit, rose_type='sensitivity', colorbar='gist_heat_r', km_intervals=200, zoom='', title='', save_figs=''):    \n",
    "    \n",
    "    #bins in terms of interval and direction\n",
    "    interval_bins, interval_labels, dir_bins, dir_labels=define_bins_maprose(km_intervals=km_intervals, bin_size=bin_size)\n",
    "    \n",
    "    st_lon= stations[station]['lon']\n",
    "    st_lat= stations[station]['lat']\n",
    "\n",
    "    #get the aggregated footprint\n",
    "    nfp, fp, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, date_range, timeselect=timeselect)     \n",
    "\n",
    "    #if not saved distances to all 192000 cells, calculate it. \n",
    "    if station not in approved_stations_distances.columns:\n",
    "\n",
    "        x = [math.radians(st_lon-lon)*math.cos(math.radians(st_lat+lat)/2) for lat in fp_lat for lon in fp_lon]\n",
    "\n",
    "        y = [math.radians(st_lat-lat) for lat in fp_lat for lon in fp_lon]\n",
    "\n",
    "        distance=[math.sqrt((x[index]*x[index])+(y[index]*y[index])) * R for index in range(len(x))]\n",
    "\n",
    "    #if in the existing list, access it. \n",
    "    else:\n",
    "\n",
    "        distance=approved_stations_distances[station]\n",
    "        \n",
    "    #same function used to all three types of map\n",
    "    if rose_type=='sensitivity':\n",
    "        \n",
    "        #only want to look at the aggregated footprint - not multiplied by anciallary datalayer \n",
    "        grid_to_display=fp\n",
    "        \n",
    "    elif rose_type=='point source contribution':\n",
    "        \n",
    "        #import the point source data for multiplication with the aggregated footprint\n",
    "        fp_point_source_m2_s = import_point_source_data()\n",
    "        \n",
    "        grid_to_display=fp*fp_point_source_m2_s\n",
    "        \n",
    "    elif rose_type=='population sensitivity':\n",
    "        \n",
    "        #import the population data for multiplication with the aggregated footprint\n",
    "        fp_pop= import_population_data()\n",
    "        \n",
    "        grid_to_display=fp*fp_pop\n",
    "        \n",
    "    #list with 192000 sens values. same place in list for distance to sttion and degree \n",
    "    sens_value=[grid_to_display[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "\n",
    "    #degrees - calculate at what degree each cell is in case not in saved list \n",
    "    if station not in approved_stations_degrees.columns:\n",
    "\n",
    "        degrees_0_360=[calculate_initial_compass_bearing((st_lat, st_lon), (lat, lon)) for lat in fp_lat for lon in fp_lon]\n",
    "    else:\n",
    "\n",
    "        degrees_0_360=approved_stations_degrees[station]\n",
    "\n",
    "    #putting it into a dataframe - to perform groupby etc\n",
    "    df_sensitivity_map = pd.DataFrame()\n",
    "    df_sensitivity_map['distance'] = distance\n",
    "    df_sensitivity_map['sensitivity'] = sens_value\n",
    "    df_sensitivity_map['degrees'] = degrees_0_360\n",
    "\n",
    "    #for % later - sensitivity within certain bin (distance and direction)\n",
    "    total_sensitivity= sum(df_sensitivity_map['sensitivity'])\n",
    "\n",
    "    #binning - by the distace intervals and degree intervals. Summarize these. \n",
    "    rosedata=df_sensitivity_map.assign(WindSpd_bins=lambda df: pd.cut(df['distance'], bins=interval_bins, labels=interval_labels, right=True))\n",
    "\n",
    "    rosedata=rosedata.assign(WindDir_bins=lambda df: pd.cut(df['degrees'], bins=dir_bins, labels=dir_labels, right=False))\n",
    "\n",
    "    #the 360 degree are the same as 0:\n",
    "    rosedata=rosedata.replace({'WindDir_bins': {360: 0}})\n",
    "\n",
    "    #the combination of the distance and direction columns is used to create a unique column value for all cells\n",
    "    #with certain direction/distance combination.\n",
    "    #make it to string to be able to combine.\n",
    "    rosedata['key']=rosedata['WindDir_bins'].astype(str) +rosedata['WindSpd_bins'].astype(str) \n",
    "\n",
    "    #group by the unique combination of direction and distance\n",
    "    rosedata_grouped_key=rosedata.groupby(by=['key'], as_index=False)['sensitivity'].sum().reset_index()\n",
    "\n",
    "    #merge between the 192000 cells and the \"groupedby\" values: each cell in a specific direction and distnace will\n",
    "    #get the sum of the cells in that same specific bin. Same color on the map corresponing to % or absolute sensitivity.\n",
    "    #reset_index() creates a column with the original index of the dataframes that are joined. Needed to sort the dataframe\n",
    "    #in the next spted because default is to sort by the key used. \n",
    "    rosedata_merge=rosedata.reset_index().merge(rosedata_grouped_key, left_on='key', right_on='key', sort=False)\n",
    "\n",
    "    #sort by the original index of the 192000 cells: \n",
    "    rosedata_merge=rosedata_merge.sort_values(by=['index_x'])\n",
    "\n",
    "\n",
    "    #x is the \"fist\" (rosedata.merge) dataframe that was merged (the 192000 individual cells) \n",
    "    #y is the dataframe that is merged to the first. Both columns name \"sensitivity\". \n",
    "    #sensitivity_y is the merged data - the summarized sensitivity value for the whole bin (direction and distance bin)\n",
    "    rosedata_merge_list=rosedata_merge['sensitivity_y'].tolist()\n",
    "\n",
    "    #now starts the process of \"packing it back up\" so that it can be displayed as a map (same format as the netCDF files with 480\n",
    "    #lists of lists - the first list is all tha values that has \"the first\" latitude value and all 400 different longitude values)\n",
    "    #calculate the % sensitivity - can be changed to absolute sensitivity\n",
    "    if unit=='percent':\n",
    "        rosedata_merge_list=[(sensitivity_value/total_sensitivity)*100 for sensitivity_value in rosedata_merge_list]\n",
    "\n",
    "    #the \"netcdf simulation\" (see text above)\n",
    "    rosedata_merge_list_of_lists=[]\n",
    "\n",
    "    index=0\n",
    "    while index<192000:\n",
    "        index_to=index+400\n",
    "\n",
    "        #for each list: need to grab the 400 values that are the combination of the same latitude value\n",
    "        #but different longitude values\n",
    "        rosedata_merge_list_of_lists.append(rosedata_merge_list[index:index_to])\n",
    "\n",
    "        #start at the next 400 in the list in the next turn of the loop:\n",
    "        index=index+400\n",
    "\n",
    "    #numpy array works to display in map\n",
    "    rosedata_merge_list_of_lists_array=np.array(rosedata_merge_list_of_lists) \n",
    "\n",
    "    separator = ', '\n",
    "    interval_labels_upd= separator.join(interval_labels)\n",
    "\n",
    "    #added\n",
    "    date_index_number = (len(date_range) - 1)\n",
    "    \n",
    "    if title=='yes':\n",
    "        for_title=('Station: ' + str(station) + '\\n' + unit + ' ' + rose_type + ' given direction and distance: ' + '\\n' + str(bin_size) + \\\n",
    "                   ' degree bins and ' + str(km_intervals) +' km increments'\n",
    "                 '\\n' + str(date_range[0].year) + '-' + str(date_range[0].month) + '-' + str(date_range[0].day)\\\n",
    "                + ' to ' + str(date_range[date_index_number].year) + '-' + str(date_range[date_index_number].month) + '-' + str(date_range[date_index_number].day)+\\\n",
    "              ' Hour(s): ' + timeselect+ '\\n')\n",
    "    else:\n",
    "        for_title=''\n",
    "\n",
    "    #font\n",
    "    matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "    if unit=='percent':\n",
    "        unit='%'\n",
    "    else:\n",
    "        unit='absolute'\n",
    "\n",
    "\n",
    "    if save_figs=='yes':\n",
    "        \n",
    "        if rose_type=='sensitivity':\n",
    "            figure_number='_figure_1'\n",
    "        if rose_type=='point source contribution':\n",
    "            figure_number='_figure_2'\n",
    "        if rose_type=='population sensitivity':\n",
    "            figure_number='_figure_3'\n",
    "            \n",
    "        string_fig=station+figure_number\n",
    "    else:\n",
    "        string_fig=''\n",
    "\n",
    "\n",
    "    #use the plot_maps function\n",
    "    #need two different?\n",
    "    if unit=='percent':\n",
    "        plot_maps(rosedata_merge_list_of_lists_array, fp_lon, fp_lat, title=for_title, label=rose_type, \n",
    "                  unit=unit, linlog='linear', station=station, \n",
    "                  zoom=zoom, colors=colorbar, pngfile=string_fig)\n",
    "\n",
    "    else:\n",
    "        plot_maps(rosedata_merge_list_of_lists_array, fp_lon, fp_lat, title=for_title, label=rose_type, \n",
    "                  unit=unit, linlog='linear', station=station, \n",
    "                  zoom=zoom, colors=colorbar, pngfile=string_fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Land cover bar graph</h3>\n",
    "<p>Bar graph with 8 directions (N, NE, E, SE, S, SW, W, NW) with stacked bars representing\n",
    "relative land cover within aggregated footprint</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def land_cover_bar_graph(station, date_range, timeselect, title='', save_figs=''):\n",
    "    \n",
    "    #get all the land cover data\n",
    "    out_of_domain, urban_aggreg, cropland_aggreg, forests, pastures_grasslands, oceans, other= import_landcover()\n",
    "    \n",
    "    approved_stations_degrees = pd.read_csv('approved_stations_degrees.csv')\n",
    "    \n",
    "    st_lon= stations[station]['lon']\n",
    "    st_lat= stations[station]['lat']\n",
    "\n",
    "    #selected date_range, aggregated footprint for selected station\n",
    "    nfp, fp, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, date_range, timeselect=timeselect)\n",
    "    \n",
    "    #land cover classes (imported in the land cover section):\n",
    "    cropland_multiplied=fp*cropland_aggreg\n",
    "    urban_multiplied=fp*urban_aggreg\n",
    "    forests_multiplied=fp*forests\n",
    "    pastures_grasslands_multiplied=fp*pastures_grasslands\n",
    "    oceans_multiplied=fp*oceans\n",
    "    other_multiplied=fp*other\n",
    "    out_of_domain_multiplied=fp*out_of_domain\n",
    "    \n",
    "    cropland_values=[cropland_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    urban_values=[urban_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    forests_values=[forests_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    pastures_grasslands_values=[pastures_grasslands_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    oceans_values=[oceans_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    others_values=[other_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "\n",
    "    #added: out_of_domain\n",
    "    out_of_domain_values=[out_of_domain_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "\n",
    "    #the distances to all cells, for all stations with approved labelling --> update with new data? \n",
    "    approved_stations_distances = pd.read_csv('approved_stations_distances.csv')\n",
    "\n",
    "    approved_stations_degrees = pd.read_csv('approved_stations_degrees.csv')\n",
    "    \n",
    "    #degrees (no distance bins for land cover):\n",
    "    if station not in approved_stations_degrees.columns:\n",
    "\n",
    "        degrees_0_360=[calculate_initial_compass_bearing((st_lat, st_lon), (lat, lon)) for lat in fp_lat for lon in fp_lon]\n",
    "    else:  \n",
    "        degrees_0_360=approved_stations_degrees[station]\n",
    "        \n",
    "    #putting it into a dataframe: initially 192000 values (one per cell) for each of the aggregated land cover classes\n",
    "    #into same dataframe - have the same coulmn heading. \"landcover_type\" will be used in \"groupby\" together with the \"slice\" (in degrees)\n",
    "    df_cropland = pd.DataFrame()\n",
    "    df_cropland['landcover_vals'] = cropland_values\n",
    "    df_cropland['degrees'] = degrees_0_360\n",
    "    df_cropland['landcover_type'] = 'Cropland'\n",
    "\n",
    "    df_urban= pd.DataFrame()\n",
    "    df_urban['landcover_vals'] = urban_values\n",
    "    df_urban['degrees'] = degrees_0_360\n",
    "    df_urban['landcover_type'] = 'Urban'\n",
    "\n",
    "    df_forests = pd.DataFrame()\n",
    "    df_forests['landcover_vals'] = forests_values\n",
    "    df_forests['degrees'] = degrees_0_360\n",
    "    df_forests['landcover_type'] = 'Forests'\n",
    "\n",
    "    df_pastures_grassland = pd.DataFrame()\n",
    "    df_pastures_grassland['landcover_vals'] = pastures_grasslands_values\n",
    "    df_pastures_grassland['degrees'] = degrees_0_360\n",
    "    df_pastures_grassland['landcover_type'] = 'Pastures and grassland'\n",
    "\n",
    "    df_oceans = pd.DataFrame()\n",
    "    df_oceans['landcover_vals'] = oceans_values\n",
    "    df_oceans['degrees'] = degrees_0_360\n",
    "    df_oceans['landcover_type'] = 'Oceans'\n",
    "\n",
    "    df_others = pd.DataFrame()\n",
    "    df_others['landcover_vals'] = others_values\n",
    "    df_others['degrees'] = degrees_0_360\n",
    "    df_others['landcover_type'] = 'Other'\n",
    "    \n",
    "    #out of domain\n",
    "    df_out_of_domain = pd.DataFrame()\n",
    "    df_out_of_domain['landcover_vals'] = out_of_domain_values\n",
    "    df_out_of_domain['degrees'] = degrees_0_360\n",
    "    df_out_of_domain['landcover_type'] = 'No data'\n",
    "\n",
    "\n",
    "    #into one dataframe\n",
    "    #possibly add: df_out_of_domain\n",
    "    df_all = df_cropland.append([df_urban, df_forests, df_pastures_grassland, df_oceans, df_others, df_out_of_domain])\n",
    "\n",
    "    #for % later - sensitivity to landcover within certain bin (landcover and direction)\n",
    "    #how works now when have \"no data\" also? \n",
    "    total_all= sum(df_all['landcover_vals'])\n",
    "    \n",
    "    \n",
    "    ##added - for the matplotlib breakdown\n",
    "    dir_bins = np.arange(22.5, 383.5, 45)\n",
    "    #dir_bins = np.asarray(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N'])\n",
    "    \n",
    "    dir_bins= np.asarray([0, 22.5,67.5,112.5,157.5,202.5,247.5,292.5,337.5,383.5])\n",
    "    \n",
    "    #dir_labels= np.asarray([0, 22.5,67.5,112.5,157.5,202.5,247.5,292.5,337.5])\n",
    "    dir_labels= np.asarray([0, 22.5,67.5,112.5,157.5,202.5,247.5,292.5,337.5])\n",
    "    #dir_labels = (dir_bins[:-1] + dir_bins[1:]) / 2\n",
    "    \n",
    "    \n",
    "    #get columns - for each degree\n",
    "    rosedata=df_all.assign(WindDir_bins=lambda df: pd.cut(df['degrees'], bins=dir_bins, labels=dir_labels, right=False))\n",
    "\n",
    "    #the 360 degrees are the same as 0:\n",
    "    #382.5 rather? \n",
    "    #not needed now - in the same class (?)\n",
    "    rosedata=rosedata.replace({'WindDir_bins': {0.0: 337.5}})\n",
    "\n",
    "    #group the data by the distance bins, and again by the direction bins. The value to be summed in the sensitivity values.\n",
    "    rosedata=rosedata.groupby(by=['landcover_type', 'WindDir_bins'])['landcover_vals'].sum()\n",
    "\n",
    "    #changes the format:\n",
    "    rosedata=rosedata.unstack(level='landcover_type')\n",
    "\n",
    "    #test - sort by totals. \n",
    "    total_by_col=[]\n",
    "    columns=[]\n",
    "    for column in rosedata.columns:\n",
    "        columns.append(column)\n",
    "        total_by_col.append(sum(rosedata[column].values))\n",
    "\n",
    "    sorted_columns = [x for _,x in sorted(zip(total_by_col,columns), reverse=True)]\n",
    "\n",
    "    rosedata=rosedata[sorted_columns]\n",
    "\n",
    "    #for all values: want the % of the total sensitivity (one value for each distance for each direction)\n",
    "    rosedata= rosedata.applymap(lambda x: x / total_all * 100)\n",
    "    \n",
    "    \n",
    "    list_land_cover_name=[]\n",
    "    list_land_cover_values=[]\n",
    "    for n, (c1, c2) in enumerate(zip(rosedata.columns[:-1], rosedata.columns[1:])):\n",
    "\n",
    "        # first column only\n",
    "        if n == 0:\n",
    "            list_land_cover_name.append(c1)\n",
    "\n",
    "            list_land_cover_values.append(rosedata[c1].values)\n",
    "\n",
    "        list_land_cover_name.append(c2)\n",
    "        list_land_cover_values.append(rosedata[c2].values)\n",
    "        \n",
    "    matplotlib.rcParams.update({'font.size': 14})\n",
    "    #create the bar graph:\n",
    "    dictionary_color = {'Urban': {'color': 'red'}, 'Cropland':{'color':'darkgoldenrod'}, 'Oceans':{'color':'blue'}, \n",
    "                        'Forests':{'color':'green'}, 'Pastures and grassland':{'color':'yellow'}, 'Other':{'color':'black'}, 'No data':{'color': 'grey'}}\n",
    "\n",
    "    fig = plt.figure(figsize=(11,10)) \n",
    "\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    N = 8\n",
    "\n",
    "    ind = np.arange(N)  \n",
    "\n",
    "    width = 0.35       \n",
    "\n",
    "\n",
    "    p1 = ax.bar(ind, list_land_cover_values[0], width, color=dictionary_color[list_land_cover_name[0]]['color'])\n",
    "    p2 = ax.bar(ind, list_land_cover_values[1], width, color=dictionary_color[list_land_cover_name[1]]['color'],\n",
    "                 bottom=list_land_cover_values[0])\n",
    "\n",
    "    p3 = ax.bar(ind, list_land_cover_values[2], width, color=dictionary_color[list_land_cover_name[2]]['color'],\n",
    "                 bottom=list_land_cover_values[0]+list_land_cover_values[1])\n",
    "    p4 = ax.bar(ind, list_land_cover_values[3], width, color=dictionary_color[list_land_cover_name[3]]['color'],\n",
    "                 bottom=list_land_cover_values[0]+list_land_cover_values[1]+list_land_cover_values[2])\n",
    "    p5 = ax.bar(ind, list_land_cover_values[4], width, color=dictionary_color[list_land_cover_name[4]]['color'],\n",
    "                 bottom=list_land_cover_values[0]+list_land_cover_values[1]+list_land_cover_values[2]+list_land_cover_values[3])\n",
    "    p6 = ax.bar(ind, list_land_cover_values[5], width, color=dictionary_color[list_land_cover_name[5]]['color'],\n",
    "                 bottom=list_land_cover_values[0]+list_land_cover_values[1]+list_land_cover_values[2]+list_land_cover_values[3]+list_land_cover_values[4])\n",
    "    p7 = ax.bar(ind, list_land_cover_values[6], width, color=dictionary_color[list_land_cover_name[6]]['color'],\n",
    "                 bottom=list_land_cover_values[0]+list_land_cover_values[1]+list_land_cover_values[2]+list_land_cover_values[3]+list_land_cover_values[4]+list_land_cover_values[5])\n",
    "\n",
    "    #want to reverese the order (ex if oceans at the \"bottom\" in the graph - ocean label should be furthest down)\n",
    "    handles=(p1[0], p2[0], p3[0], p4[0], p5[0], p6[0], p7[0])\n",
    "\n",
    "    index=0\n",
    "    list_labels=[]\n",
    "    for land_cover_name in list_land_cover_name:\n",
    "\n",
    "        for_lable= (land_cover_name + ' (' + str(\"%.1f\" % sum(list_land_cover_values[index])) + '%)')\n",
    "        list_labels.append(for_lable)\n",
    "        index=index+1\n",
    "\n",
    "    date_index_number = (len(date_range) - 1)\n",
    "    if title=='yes':\n",
    "        for_title=('Station: ' + str(station) + '\\n' + 'Land cover within average footprint by direction'+\n",
    "                 '\\n' + str(date_range[0].year) + '-' + str(date_range[0].month) + '-' + str(date_range[0].day)\\\n",
    "                + ' to ' + str(date_range[date_index_number].year) + '-' + str(date_range[date_index_number].month) + '-' + str(date_range[date_index_number].day)+\\\n",
    "              ' Hour(s): ' + timeselect+ '\\n')\n",
    "    else:\n",
    "        for_title=''\n",
    "\n",
    "    labels=[textwrap.fill(text,20) for text in list_labels]\n",
    "    #(1,-0.05)\n",
    "    #1.3, 0.2\n",
    "    #1,1\n",
    "    plt.legend(handles[::-1], labels[::-1],bbox_to_anchor=(1, 0.33))\n",
    "    #leg = ax.legend(labels, bbox_to_anchor=(1, -0.05), ncol=2)\n",
    "\n",
    "    plt.ylabel('Percent')\n",
    "    plt.title(for_title)\n",
    "    \n",
    "    #first one is not north (in rosedata - rather 22.5 to 67.5 (NE). \n",
    "    plt.xticks(ind, ('NE', 'E','SE', 'S', 'SW','W', 'NW', 'N'))\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "    #plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if save_figs=='yes':\n",
    "        \n",
    "        plotdir='figures'\n",
    "        pngfile=station+'_figure_7'\n",
    "        fig.savefig(plotdir+'/'+pngfile+'.pdf',dpi=100, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Seasonal variations table</h3>\n",
    "<p>Table where the average 2018 values in terms of sensitivity, population sensitivity, point source contribution,\n",
    "anthropogenic contribution, GEE and respiration are compared to the different parts of the year \n",
    "(Jan-Mar, Apr-Jun, Jul-Sep, Okt-Dec)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seasonal_table(station, year, save_figs=''):\n",
    "    \n",
    "    available_STILT= available_STILT_dictionary()\n",
    "    \n",
    "    #check what months available for the year\n",
    "    months= available_STILT[station][str(year)]['months']\n",
    "\n",
    "    #need there to be 12 months of data avaiable to move on with the code - create a table for whole year\n",
    "    #the months + text\n",
    "    if len(months)==13:\n",
    "        start_date=dt.datetime(year,1,1,0)\n",
    "\n",
    "        #can change end date here.\n",
    "        end_date=dt.datetime(year+1, 12, 31,0)-dt.timedelta(hours=3)\n",
    "\n",
    "        date_range = pd.date_range(start_date, end_date, freq='3H')\n",
    "\n",
    "\n",
    "        winter_date_range=pd.date_range(dt.datetime(year,1,1,0), (dt.datetime(year, 4, 1,0)-dt.timedelta(hours=3)), freq='3H')\n",
    "        spring_date_range=pd.date_range(dt.datetime(year,4,1,0), (dt.datetime(year, 7, 1,0)-dt.timedelta(hours=3)), freq='3H')\n",
    "        summer_date_range=pd.date_range(dt.datetime(year,7,1,0), (dt.datetime(year, 10, 1,0)-dt.timedelta(hours=3)), freq='3H')\n",
    "        fall_date_range=pd.date_range(dt.datetime(year,10,1,0), (dt.datetime(year+1, 1, 1,0)-dt.timedelta(hours=3)), freq='3H')\n",
    "\n",
    "        #always all footprints, irregardless of what selection made. \n",
    "        timeselect='0, 3, 6, 9, 12, 15, 18, 21'\n",
    "\n",
    "        #the average footprints given the selected date range\n",
    "        #whole year to compare with \n",
    "        nfp, fp_whole, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, date_range, timeselect=timeselect)\n",
    "\n",
    "        nfp, fp_winter, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, winter_date_range, timeselect=timeselect)\n",
    "\n",
    "        nfp, fp_spring, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, spring_date_range, timeselect=timeselect)\n",
    "\n",
    "        nfp, fp_summer, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, summer_date_range, timeselect=timeselect)\n",
    "\n",
    "        nfp, fp_fall, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, fall_date_range, timeselect=timeselect)\n",
    "\n",
    "        #sensitivity section\n",
    "        sensitivity_whole=fp_whole[0].sum()\n",
    "\n",
    "        sensitivity_diff_winter=((fp_winter[0].sum()/sensitivity_whole)*100)-100\n",
    "\n",
    "        sensitivity_diff_spring=((fp_spring[0].sum()/sensitivity_whole)*100)-100\n",
    "\n",
    "        sensitivity_diff_summer=((fp_summer[0].sum()/sensitivity_whole)*100)-100\n",
    "\n",
    "        sensitivity_diff_fall=((fp_fall[0].sum()/sensitivity_whole)*100)-100\n",
    "\n",
    "\n",
    "        #point source section\n",
    "        fp_point_source_m2_s = import_point_source_data()\n",
    "\n",
    "        point_source_whole=(fp_whole*fp_point_source_m2_s)[0].sum()\n",
    "\n",
    "        pointsource_diff_winter=(((fp_winter*fp_point_source_m2_s)[0].sum()/point_source_whole)*100)-100\n",
    "\n",
    "        pointsource_diff_spring=(((fp_spring*fp_point_source_m2_s)[0].sum()/point_source_whole)*100)-100\n",
    "\n",
    "        pointsource_diff_summer=(((fp_summer*fp_point_source_m2_s)[0].sum()/point_source_whole)*100)-100\n",
    "\n",
    "        pointsource_diff_fall=(((fp_fall*fp_point_source_m2_s)[0].sum()/point_source_whole)*100)-100\n",
    "\n",
    "        #population section\n",
    "\n",
    "        fp_pop = import_population_data()\n",
    "        \n",
    "        population_whole=(fp_whole*fp_pop)[0].sum()\n",
    "\n",
    "        population_diff_winter=(((fp_winter*fp_pop)[0].sum()/population_whole)*100)-100\n",
    "\n",
    "        population_diff_spring=(((fp_spring*fp_pop)[0].sum()/population_whole)*100)-100\n",
    "\n",
    "        population_diff_summer=(((fp_summer*fp_pop)[0].sum()/population_whole)*100)-100\n",
    "\n",
    "        population_diff_fall=(((fp_fall*fp_pop)[0].sum()/population_whole)*100)-100\n",
    "\n",
    "        #GEE\n",
    "        timeselect_list=[0, 3, 6, 9, 12, 15, 18, 21]\n",
    "        df_whole = read_stilt_timeseries_upd(station, date_range, timeselect_list)\n",
    "\n",
    "        df_winter = read_stilt_timeseries_upd(station, winter_date_range, timeselect_list)\n",
    "        df_spring = read_stilt_timeseries_upd(station, spring_date_range, timeselect_list)\n",
    "        df_summer = read_stilt_timeseries_upd(station, summer_date_range, timeselect_list)\n",
    "        df_fall = read_stilt_timeseries_upd(station, fall_date_range, timeselect_list)\n",
    "\n",
    "\n",
    "        df_whole_mean=df_whole.mean()\n",
    "\n",
    "        df_winter_mean=df_winter.mean()\n",
    "        df_spring_mean=df_spring.mean()\n",
    "        df_summer_mean=df_summer.mean()\n",
    "        df_fall_mean=df_fall.mean()\n",
    "\n",
    "        gee_whole=abs(df_whole_mean['co2.bio.gee'])\n",
    "\n",
    "        gee_diff_winter=((abs(df_winter_mean['co2.bio.gee'])/gee_whole)*100)-100\n",
    "\n",
    "        gee_diff_spring=((abs(df_spring_mean['co2.bio.gee'])/gee_whole)*100)-100\n",
    "\n",
    "        gee_diff_summer=((abs(df_summer_mean['co2.bio.gee'])/gee_whole)*100)-100\n",
    "\n",
    "        gee_diff_fall=((abs(df_fall_mean['co2.bio.gee'])/gee_whole)*100)-100\n",
    "\n",
    "        #respiration\n",
    "\n",
    "        resp_whole=df_whole_mean['co2.bio.resp']\n",
    "\n",
    "        resp_diff_winter=((df_winter_mean['co2.bio.resp']/resp_whole)*100)-100\n",
    "\n",
    "        resp_diff_spring=((df_spring_mean['co2.bio.resp']/resp_whole)*100)-100\n",
    "\n",
    "        resp_diff_summer=((df_summer_mean['co2.bio.resp']/resp_whole)*100)-100\n",
    "\n",
    "        resp_diff_fall=((df_fall_mean['co2.bio.resp']/resp_whole)*100)-100\n",
    "\n",
    "        #anthropogenic\n",
    "\n",
    "        anthro_whole=df_whole_mean['co2.industry']+df_whole_mean['co2.energy']+ df_whole_mean['co2.transport']+ df_whole_mean['co2.others']\n",
    "\n",
    "        anthro_diff_winter=(((df_winter_mean['co2.industry']+df_winter_mean['co2.energy']+ df_winter_mean['co2.transport']+ df_winter_mean['co2.others'])/anthro_whole)*100)-100\n",
    "\n",
    "        anthro_diff_spring=(((df_spring_mean['co2.industry']+df_spring_mean['co2.energy']+ df_spring_mean['co2.transport']+ df_spring_mean['co2.others'])/anthro_whole)*100)-100\n",
    "\n",
    "        anthro_diff_summer=(((df_summer_mean['co2.industry']+df_summer_mean['co2.energy']+ df_summer_mean['co2.transport']+ df_summer_mean['co2.others'])/anthro_whole)*100)-100\n",
    "\n",
    "        anthro_diff_fall=(((df_fall_mean['co2.industry']+df_fall_mean['co2.energy']+ df_fall_mean['co2.transport']+ df_fall_mean['co2.others'])/anthro_whole)*100)-100\n",
    "\n",
    "\n",
    "\n",
    "        df_seasonal_table = pd.DataFrame(columns=['Variable', str(year), 'Jan-Mar', 'Apr-Jun', 'Jul-Sep','Oct-Dec', 'Unit'], index=['Sensitivity', 'Population','Point source', 'GEE', 'Respiration', 'Anthropogenic'])\n",
    "\n",
    "\n",
    "        df_seasonal_table.loc['Sensitivity'] = pd.Series({'Variable': 'Sensitivity', str(year):(\"%.2f\" % sensitivity_whole), 'Jan-Mar':(\"%.2f\" % sensitivity_diff_winter), 'Apr-Jun':(\"%.2f\" % sensitivity_diff_spring), \n",
    "                                                              'Jul-Sep':(\"%.2f\" % sensitivity_diff_summer), 'Oct-Dec':(\"%.2f\" % sensitivity_diff_fall), 'Unit': 'ppm / ($\\mu$mol / m$^{2}$s)'})\n",
    "\n",
    "        df_seasonal_table.loc['Population'] = pd.Series({'Variable': 'Population', str(year):(\"%.0f\" % population_whole), 'Jan-Mar':(\"%.2f\" % population_diff_winter), 'Apr-Jun':(\"%.2f\" % population_diff_spring), \n",
    "                                                              'Jul-Sep':(\"%.2f\" % population_diff_summer), 'Oct-Dec':(\"%.2f\" % population_diff_fall), 'Unit': 'pop*(ppm / ($\\mu$mol / m$^{2}$s))'})\n",
    "\n",
    "        \n",
    "        df_seasonal_table.loc['Point source'] = pd.Series({'Variable': 'Point source', str(year):(\"%.2f\" % point_source_whole), 'Jan-Mar':(\"%.2f\" % pointsource_diff_winter), 'Apr-Jun':(\"%.2f\" % pointsource_diff_spring), \n",
    "                                                              'Jul-Sep':(\"%.2f\" % pointsource_diff_summer), 'Oct-Dec':(\"%.2f\" % pointsource_diff_fall), 'Unit': 'ppm'})\n",
    "\n",
    "\n",
    "        df_seasonal_table.loc['GEE'] = pd.Series({'Variable': 'GEE','Unit': 'ppm (uptake)', str(year):(\"%.2f\" % gee_whole), 'Jan-Mar':(\"%.2f\" % gee_diff_winter), 'Apr-Jun':(\"%.2f\" % gee_diff_spring), \n",
    "                                                              'Jul-Sep':(\"%.2f\" % gee_diff_summer), 'Oct-Dec':(\"%.2f\" % gee_diff_fall), 'Unit': 'ppm (uptake)'})\n",
    "\n",
    "        df_seasonal_table.loc['Respiration'] = pd.Series({'Variable': 'Respiration', str(year):(\"%.2f\" % resp_whole), 'Jan-Mar':(\"%.2f\" % resp_diff_winter), 'Apr-Jun':(\"%.2f\" % resp_diff_spring), \n",
    "                                                              'Jul-Sep':(\"%.2f\" % resp_diff_summer), 'Oct-Dec':(\"%.2f\" % resp_diff_fall), 'Unit': 'ppm'})\n",
    "\n",
    "\n",
    "        df_seasonal_table.loc['Anthropogenic'] = pd.Series({'Variable': 'Anthropogenic', str(year):(\"%.2f\" % anthro_whole), 'Jan-Mar':(\"%.2f\" % anthro_diff_winter), 'Apr-Jun':(\"%.2f\" % anthro_diff_spring), \n",
    "                                                              'Jul-Sep':(\"%.2f\" % anthro_diff_summer), 'Oct-Dec':(\"%.2f\" % anthro_diff_fall), 'Unit': 'ppm'})\n",
    "\n",
    "\n",
    "        display(HTML('<p style=\"font-size:16px;\">Seasonal variation during the start year of specified date range </p>'))\n",
    "\n",
    "        #14 font before\n",
    "        def render_mpl_table(data, col_width=2, row_height=0.625, font_size=16,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "            if ax is None:\n",
    "                size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "                fig, ax = plt.subplots(figsize=size)\n",
    "                ax.axis('off')\n",
    "\n",
    "\n",
    "            mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, colWidths=[4,2,2,2,2,2,5])\n",
    "            #else:\n",
    "            #mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, colWidths=[2,1.5,1.5,1.5,1.5,1.5,4.5])\n",
    "\n",
    "\n",
    "            mpl_table.auto_set_font_size(True)\n",
    "            mpl_table.set_fontsize(font_size)\n",
    "\n",
    "            for k, cell in  six.iteritems(mpl_table._cells):\n",
    "                cell.set_edgecolor(edge_color)\n",
    "                if k[0] == 0 or k[1] < header_columns:\n",
    "                    cell.set_text_props(weight='bold', color='w')\n",
    "                    cell.set_facecolor(header_color)\n",
    "                else:\n",
    "                    cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "\n",
    "            if save_figs=='yes':\n",
    "                fig = ax.get_figure()\n",
    "                plotdir='figures'\n",
    "                pngfile=station+'_figure_6'\n",
    "\n",
    "                #.pdf\n",
    "                fig.savefig(plotdir+'/'+pngfile+'.pdf',dpi=100, bbox_inches='tight')\n",
    "\n",
    "            plt.show(fig)\n",
    "\n",
    "            \n",
    "        render_mpl_table(df_seasonal_table, header_columns=0, col_width=2.5)\n",
    "      \n",
    "    #if not 12 months:\n",
    "    else:\n",
    "        \n",
    "        display(HTML('<p style=\"font-size:12px;\">Footprints not available for the whole year and therefore no seasonal variations table is shown</p>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Land cover polar graph</h2>\n",
    "<p>Write here\n",
    "Functions:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_bins_landcover_polar_graph(bin_size):\n",
    "    \n",
    "    #spd_bins = ['Cropland', 'Urban', 'Forests', 'Pastures and grassland', 'Oceans', 'Others']\n",
    "\n",
    "    #no need for a fuction for the labels this time.\n",
    "    #possibly add 'No CORINE data'\n",
    "    #spd_labels= ['Cropland', 'Urban', 'Forests', 'Pastures and grassland', 'Oceans', 'Others']\n",
    "\n",
    "    #direction: using the input (degree_bin) to set the bins so that the first bin has \"north (0 degrees) in the middle\"\n",
    "    #\"from_degree\" becomes a negative value (half of the degree value \"to the left\" of 0)\n",
    "    from_degree=-(bin_size/2)\n",
    "\n",
    "    #\"to_degree\" is a vale to indicate the last bins ending. Must check values all the way to 360 which means the last bin \n",
    "    #will go past 360 and later be joined with the \"0\" bin (replace function in next cell)\n",
    "    to_degree= 360 + (bin_size/2) + 1\n",
    "\n",
    "    #the \"degree_bin\" is the \"step\".\n",
    "    dir_bins = np.arange(from_degree, to_degree, bin_size)\n",
    "\n",
    "    #the direction bin is the first bin + the next bin divided by two:\n",
    "    dir_labels = (dir_bins[:-1] + dir_bins[1:]) / 2\n",
    "    \n",
    "    return dir_bins, dir_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the directions (and number of them), get the direction of the bars and their width\n",
    "#function used in the final step of generating a graph\n",
    "def _convert_dir(directions, N=None):\n",
    "    if N is None:\n",
    "        N = directions.shape[0]\n",
    "    barDir = directions * np.pi/180. - np.pi/N\n",
    "    barWidth = 2 * np.pi / N\n",
    "    return barDir, barWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar to polar maps (don't need distance this time, and rather sensitivity one value for each land cover class.)\n",
    "#group by land cover class rather than distance - sum of sensitivity*individual land cover class\n",
    "def landcover_polar_graph(station, date_range, timeselect, bin_size, label='', title='', percent_label='', save_figs=''):\n",
    "    \n",
    "    #get these first so answer that question right away for user (bin size in degrees)\n",
    "    \n",
    "    dir_bins, dir_labels=define_bins_landcover_polar_graph(bin_size=bin_size)\n",
    "    \n",
    "    out_of_domain, urban_aggreg, cropland_aggreg, forests, pastures_grasslands, oceans, other= import_landcover()\n",
    "\n",
    "    st_lon= stations[station]['lon']\n",
    "    st_lat= stations[station]['lat']\n",
    "\n",
    "    #selected date_range, aggregated footprint for selected station\n",
    "    nfp, fp, fp_lon, fp_lat, title_not_used = read_aggreg_footprints(station, date_range, timeselect=timeselect)\n",
    "    \n",
    "    if nfp==0:\n",
    "        print('no footprints: go to https://stilt.icos-cp.eu/worker/ to process footprints for desired date range')\n",
    "\n",
    "    #land cover classes (imported in the land cover section):\n",
    "    cropland_multiplied=fp*cropland_aggreg\n",
    "    urban_multiplied=fp*urban_aggreg\n",
    "    forests_multiplied=fp*forests\n",
    "    pastures_grasslands_multiplied=fp*pastures_grasslands\n",
    "    oceans_multiplied=fp*oceans\n",
    "    other_multiplied=fp*other\n",
    "    \n",
    "    #added: out of domain\n",
    "    out_of_domain_multiplied=fp*out_of_domain\n",
    "\n",
    "    cropland_values=[cropland_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    urban_values=[urban_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    forests_values=[forests_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    pastures_grasslands_values=[pastures_grasslands_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    oceans_values=[oceans_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "    others_values=[other_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "\n",
    "    #added: out_of_domain\n",
    "    out_of_domain_values=[out_of_domain_multiplied[0][lat_value][lon_value] for lat_value in range(len(fp_lat)) for lon_value in range(len(fp_lon))]\n",
    "\n",
    "    #the distances to all cells, for all stations with approved labelling --> update with new data? \n",
    "    approved_stations_distances = pd.read_csv('approved_stations_distances.csv')\n",
    "\n",
    "    approved_stations_degrees = pd.read_csv('approved_stations_degrees.csv')\n",
    "    \n",
    "    #degrees (no distance bins for land cover):\n",
    "    if station not in approved_stations_degrees.columns:\n",
    "\n",
    "        degrees_0_360=[calculate_initial_compass_bearing((st_lat, st_lon), (lat, lon)) for lat in fp_lat for lon in fp_lon]\n",
    "    else:  \n",
    "        degrees_0_360=approved_stations_degrees[station]\n",
    "\n",
    "    #putting it into a dataframe: initially 192000 values (one per cell) for each of the aggregated land cover classes\n",
    "    #into same dataframe - have the same coulmn heading. \"landcover_type\" will be used in \"groupby\" together with the \"slice\" (in degrees)\n",
    "    df_cropland = pd.DataFrame()\n",
    "    df_cropland['landcover_vals'] = cropland_values\n",
    "    df_cropland['degrees'] = degrees_0_360\n",
    "    df_cropland['landcover_type'] = 'Cropland'\n",
    "\n",
    "    df_urban= pd.DataFrame()\n",
    "    df_urban['landcover_vals'] = urban_values\n",
    "    df_urban['degrees'] = degrees_0_360\n",
    "    df_urban['landcover_type'] = 'Urban'\n",
    "\n",
    "    df_forests = pd.DataFrame()\n",
    "    df_forests['landcover_vals'] = forests_values\n",
    "    df_forests['degrees'] = degrees_0_360\n",
    "    df_forests['landcover_type'] = 'Forests'\n",
    "\n",
    "    df_pastures_grassland = pd.DataFrame()\n",
    "    df_pastures_grassland['landcover_vals'] = pastures_grasslands_values\n",
    "    df_pastures_grassland['degrees'] = degrees_0_360\n",
    "    df_pastures_grassland['landcover_type'] = 'Pastures and grassland'\n",
    "\n",
    "    df_oceans = pd.DataFrame()\n",
    "    df_oceans['landcover_vals'] = oceans_values\n",
    "    df_oceans['degrees'] = degrees_0_360\n",
    "    df_oceans['landcover_type'] = 'Oceans'\n",
    "\n",
    "    df_others = pd.DataFrame()\n",
    "    df_others['landcover_vals'] = others_values\n",
    "    df_others['degrees'] = degrees_0_360\n",
    "    df_others['landcover_type'] = 'Other'\n",
    "    \n",
    "    #out of domain\n",
    "    df_out_of_domain = pd.DataFrame()\n",
    "    df_out_of_domain['landcover_vals'] = out_of_domain_values\n",
    "    df_out_of_domain['degrees'] = degrees_0_360\n",
    "    df_out_of_domain['landcover_type'] = 'No data'\n",
    "\n",
    "\n",
    "    #into one dataframe\n",
    "    #possibly add: df_out_of_domain\n",
    "    df_all = df_cropland.append([df_urban, df_forests, df_pastures_grassland, df_oceans, df_others, df_out_of_domain])\n",
    "\n",
    "    #for % later - sensitivity to landcover within certain bin (landcover and direction)\n",
    "    #how works now when have \"no data\" also? \n",
    "    total_all= sum(df_all['landcover_vals'])\n",
    "    \n",
    "    #already have the different land cover classes in one cell (no need to use \"pandas.cut\" to generate new column with information for groupby)\n",
    "    #still need a column with the different direction bins - defined in last cell - to use for the groupby (slice)\n",
    "    rosedata=df_all.assign(WindDir_bins=lambda df: pd.cut(df['degrees'], bins=dir_bins, labels=dir_labels, right=False))\n",
    "\n",
    "    #the 360 degrees are the same as 0:\n",
    "    rosedata=rosedata.replace({'WindDir_bins': {360: 0}})\n",
    "\n",
    "    #group the data by the distance bins, and again by the direction bins. The value to be summed in the sensitivity values.\n",
    "    rosedata=rosedata.groupby(by=['landcover_type', 'WindDir_bins'])['landcover_vals'].sum()\n",
    "\n",
    "    #changes the format:\n",
    "    rosedata=rosedata.unstack(level='landcover_type')\n",
    "\n",
    "    #test - sort by totals. \n",
    "    total_by_col=[]\n",
    "    columns=[]\n",
    "    for column in rosedata.columns:\n",
    "        columns.append(column)\n",
    "        total_by_col.append(sum(rosedata[column].values))\n",
    "\n",
    "    sorted_columns = [x for _,x in sorted(zip(total_by_col,columns), reverse=True)]\n",
    "\n",
    "    rosedata=rosedata[sorted_columns]\n",
    "\n",
    "    #for all values: want the % of the total sensitivity (one value for each distance for each direction)\n",
    "    rosedata= rosedata.applymap(lambda x: x / total_all * 100)\n",
    "    \n",
    "    #remove from here (maybe imported as just matplotlib?):\n",
    "    #import matplotlib as mpl\n",
    "    \n",
    "    directions = np.arange(0, 360, bin_size)\n",
    "    date_index_number = (len(date_range) - 1)\n",
    "    \n",
    "    if title=='yes':\n",
    "\n",
    "        for_title=('Station: ' + str(station) + '\\n' + 'Area corresponding to land cover sensitivity (%)' +\n",
    "                 '\\n'  + str(date_range[0].year) + '-' + str(date_range[0].month) + '-' + str(date_range[0].day)\\\n",
    "                + ' to ' + str(date_range[date_index_number].year) + '-' + str(date_range[date_index_number].month) + '-' + str(date_range[date_index_number].day)+\\\n",
    "              ' Hour(s): ' + timeselect+ '\\n')\n",
    "    else:\n",
    "        for_title=''\n",
    "    \n",
    "    matplotlib.rcParams.update({'font.size': 14})\n",
    "    \n",
    "    #want to be able to run it with input \"rosedata\" several times without changing the variable\n",
    "    rosedata_alt= rosedata.copy()\n",
    "    \n",
    "    #update with color for out of corine domain optional: 'No CORINE data':{'color':'red'}\n",
    "    dictionary_color = {'Urban': {'color': 'red'}, 'Cropland':{'color':'darkgoldenrod'}, 'Oceans':{'color':'blue'}, \n",
    "                        'Forests':{'color':'green'}, 'Pastures and grassland':{'color':'yellow'}, 'Other':{'color':'black'}, 'No data':{'color': 'grey'}}\n",
    "    \n",
    "    colors=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for column in rosedata_alt.columns:\n",
    "        colors.append(dictionary_color[column]['color'])\n",
    "        \n",
    "        sum_land_cover=sum(rosedata_alt[column])\n",
    "        \n",
    "        #for_title = (station + ' at ' + str(date)+ '\\n'+ 'Modelled CO2 concentration: ' + str(\"%.2f\" % df['co2.stilt'][date]))\n",
    "        #for_lable= (column + ' (' + str(\"%.1f\" % sum_land_cover) + '%)')\n",
    "        #want to only have the land cover types names. when have the other graph (land_cover_bar_graph)\n",
    "        for_lable=column\n",
    "        labels.append(for_lable)\n",
    "        \n",
    "    palette=colors\n",
    "    \n",
    "    #max 20 characters in lable - if more it will be on a new line\n",
    "    labels=[textwrap.fill(text,20) for text in labels]\n",
    "    \n",
    "    #change the data so that each % values is mapped as area:\n",
    "    #first step - make the \"cumsum\" value for each direction.\n",
    "    #ex if the innermost value represent cropland, that remains, the next class will be that value + the cropland value\n",
    "    #and so on...\n",
    "    index=0\n",
    "    \n",
    "    for col_number in range(1,len(rosedata_alt.columns)):\n",
    "    \n",
    "        rosedata_alt[rosedata_alt.columns[col_number]]=rosedata_alt[rosedata_alt.columns[col_number-1]].values+rosedata_alt[rosedata_alt.columns[col_number]].values\n",
    "  \n",
    "    #the max radius is the max value in the last column (ex the \"others\" column if that one is the one with the smalles contribution = mapped the furthest from the station)\n",
    "    max_radius=max(rosedata_alt[rosedata_alt.columns[len(rosedata_alt.columns)-1]].values)\n",
    "    \n",
    "    #the area given the \"max radius\" (area of that slice by dividing it by number of directions)\n",
    "    area_max=(math.pi*max_radius*max_radius)/len(directions)\n",
    "\n",
    "    #all other values mapped in relation to this: \n",
    "    #first: what is the \"area value\" for specific class given the max area\n",
    "    rosedata_alt=rosedata_alt.applymap(lambda x: (x/max_radius)*area_max)\n",
    "    \n",
    "    #second: given that area value, what is the radius? (=where it should be placed in the graph)\n",
    "    rosedata_alt=rosedata_alt.applymap(lambda x: math.sqrt(x / math.pi))\n",
    "         \n",
    "    #bar direction and height\n",
    "    bar_dir, bar_width = _convert_dir(directions)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(polar=True))\n",
    "    ax.set_theta_direction('clockwise')\n",
    "    ax.set_theta_zero_location('N')\n",
    "    \n",
    "    #added - takes away the xaxis grid (showing directions- interfereces with the look of the division into slices)\n",
    "    #ax.xaxis.grid(True,color='b',linestyle=':', linewidth=0)\n",
    "\n",
    "    \n",
    "    #can specify a max here (ex needed when compare season maps - make this interactive as optional input to this function):\n",
    "    #ax.set_ylim(0,10)\n",
    "    #matplotlib.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    def update_yticks(x, pos):\n",
    "        area=x*x*math.pi\n",
    "        area_part=area/area_max\n",
    "        label=max_radius*area_part\n",
    "        #value=math.sqrt(value/math.pi)\n",
    "        \n",
    "        return (str(\"%.2f\" % label) + '%')\n",
    "    \n",
    "    def update_yticks_none(x, pos):\n",
    "        area=x*x*math.pi\n",
    "        area_part=area/area_max\n",
    "        label=max_radius*area_part\n",
    "        #value=math.sqrt(value/math.pi)\n",
    "        \n",
    "        return (str(''))\n",
    "    \n",
    "\n",
    "    if percent_label=='yes':\n",
    "        ax.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(update_yticks))\n",
    "        #ax.get_yaxis().set_visible(True)\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(update_yticks_none))\n",
    "        #ax.grid(True)\n",
    "        #ax.set_yticklabels([])\n",
    "        #ax.set_xticklabels([])\n",
    "        #ax.get_yaxis().set_visible(False)\n",
    "        #ax.yaxis.set_yticklabels([])\n",
    "        \n",
    "    \n",
    "    #\"the column before\" (c1) and \"the column after\" (c2)\n",
    "    for n, (c1, c2) in enumerate(zip(rosedata_alt.columns[:-1], rosedata_alt.columns[1:])):\n",
    "\n",
    "        if n == 0:\n",
    "            # first column only\n",
    "            ax.bar(bar_dir, rosedata_alt[c1].values,\n",
    "\n",
    "                   #bar width always the same --> depending on how many slices. Each slice same size.\n",
    "                   width=bar_width,\n",
    "                   #first column, first color\n",
    "                   color=palette[0],\n",
    "                   edgecolor='none',\n",
    "                   label=c1,\n",
    "                   linewidth=0)\n",
    "\n",
    "        # all other columns\n",
    "        ax.bar(bar_dir, (rosedata_alt[c2].values-rosedata_alt\n",
    "                         [c1].values), \n",
    "               width=bar_width, \n",
    "\n",
    "               #all the values \"leading up\" to this one\n",
    "               #bottom=rosedata_alt.cumsum(axis=1)[c1].values,\n",
    "               bottom=rosedata_alt[c1].values,\n",
    "               #n increases for each axis... n+1 is the next color\n",
    "               color=palette[n+1],\n",
    "               edgecolor='none',\n",
    "               #\"the one after\" - first label is at 0. \n",
    "               label=c2,\n",
    "               linewidth=0)\n",
    "        \n",
    "\n",
    "    leg = ax.legend(labels, bbox_to_anchor=(1.9, 0.25), ncol=2)\n",
    "    xtl = ax.set_xticklabels(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\n",
    "\n",
    "    \n",
    "    #size = fig.get_size_inches()*fig.dpi\n",
    "    ax.set_title(for_title)\n",
    "    \n",
    "    if save_figs=='yes':\n",
    "        \n",
    "        plotdir='figures'\n",
    "        pngfile=station+'_figure_4'\n",
    "        fig.savefig(plotdir+'/'+pngfile+'.pdf',dpi=100, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "    plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multiple vatiable graph</h3>\n",
    "<p>Averages for sensitivity, population sensitivity, point source contribution, GEE, respiration\n",
    "and anthropogenic contribution are plotted for the selected station relative to 10 reference station.\n",
    "There is pre-calculated data for several of the atmospheric stations for all of 2017 or 2018. Else, these\n",
    "have to be computed with \"compute_values_multiple_variables_graph\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values_multiple_variable_graph(all_stations, selected_station, date_range, timeselect_list, save_figs=''):\n",
    "\n",
    "    stilt_stations=create_STILT_dictionary()\n",
    "    list_stations=[]\n",
    "    list_of_lists_all=[]\n",
    "\n",
    "    list_stations_without_footprints=[]\n",
    "    list_stations_with_less_than_half_footprints=[]\n",
    "    \n",
    "    #need to compute for all stations that will be shown together with the \"selected stations\"\n",
    "    for station in all_stations:\n",
    "\n",
    "        #one list with all values per station\n",
    "        list_all=[]\n",
    "\n",
    "        #create aggregated footprint: use for multiplication by ancillary datasets (and get total average sensitivity)\n",
    "        fp=[]\n",
    "        nfp=0\n",
    "        first = True\n",
    "\n",
    "\n",
    "        #add processing of radiocarbon here \n",
    "        for date in date_range:\n",
    "\n",
    "            filename=(pathFP+station+'/'+str(date.year)+'/'+str(date.month).zfill(2)+'/'\n",
    "             +str(date.year)+'x'+str(date.month).zfill(2)+'x'+str(date.day).zfill(2)+'x'+str(date.hour).zfill(2)+'/foot')\n",
    "\n",
    "\n",
    "            if os.path.isfile(filename):\n",
    "\n",
    "                #get this date's foorptins\n",
    "                f_fp = cdf.Dataset(filename)\n",
    "\n",
    "                if (first):\n",
    "                    fp=f_fp.variables['foot'][:,:,:]\n",
    "                    lon=f_fp.variables['lon'][:]\n",
    "                    lat=f_fp.variables['lat'][:]\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    fp=fp+f_fp.variables['foot'][:,:,:]\n",
    "                f_fp.close()\n",
    "                nfp+=1\n",
    "\n",
    "        if nfp > 0:\n",
    "            fp=fp/nfp\n",
    "            #if \"valid\" station = append (ex has footprints )\n",
    "            list_stations.append(station)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            list_stations_without_footprints.append(stilt_stations[station]['name'])\n",
    "            \n",
    "            continue \n",
    "            \n",
    "        if nfp!=0:\n",
    "            percent_footprints=(nfp/len(date_range))*100\n",
    "            if percent_footprints<75:\n",
    "                \n",
    "                #if no name? \n",
    "                \n",
    "                if len(stilt_stations[station]['name'])>0:\n",
    "                    display(HTML('<p style=\"font-size:12px;\">' + stilt_stations[station]['name'] + ' (' + str(nfp) + '/' + str(len(date_range)) +' footprints)</p>'))\n",
    "                else: \n",
    "                    display(HTML('<p style=\"font-size:12px;\">' + selected_station + ' (' + str(nfp) + '/' + str(len(date_range)) +' footprints)</p>'))\n",
    "                    \n",
    "\n",
    "        #total average sensitivity for specific station:\n",
    "        total=fp[0].sum()\n",
    "\n",
    "        #total sensitivity\n",
    "        list_all.append(total)\n",
    "        \n",
    "        #read the modelled concentration data - for anthro and bio values\n",
    "        #using the updated version of read_stilt_timeseries allows for filtering out different hours of the days\n",
    "        df = read_stilt_timeseries_upd(station, date_range, timeselect_list)\n",
    "\n",
    "        #averages of the values --> default skip nan\n",
    "        df_mean=df.mean()\n",
    "\n",
    "\n",
    "        average_gee=df_mean['co2.bio.gee']\n",
    "        list_all.append(average_gee)\n",
    "\n",
    "        average_respiration=df_mean['co2.bio.resp']\n",
    "        list_all.append(average_respiration)\n",
    "\n",
    "        #anthro:\n",
    "        average_anthro=(df_mean['co2.industry']+df_mean['co2.energy']+ df_mean['co2.transport']+ df_mean['co2.others'])\n",
    "        list_all.append(average_anthro)\n",
    "\n",
    "        #import point source and population data from netcdf\n",
    "        fp_pop= import_population_data()\n",
    "        fp_point_source_m2_s = import_point_source_data()\n",
    "        \n",
    "        #point source for specific station \n",
    "        fp_pointsource_multiplied=fp*fp_point_source_m2_s\n",
    "        sum_fp_pointsource_multiplied=fp_pointsource_multiplied.sum()\n",
    "        list_all.append(sum_fp_pointsource_multiplied)\n",
    "\n",
    "        #population for specific station\n",
    "        fp_pop_multiplied=fp*fp_pop\n",
    "        sum_fp_pop_multiplied=fp_pop_multiplied.sum()\n",
    "        list_all.append(sum_fp_pop_multiplied)\n",
    "\n",
    "\n",
    "        list_of_lists_all.append(list_all)\n",
    "    \n",
    "    #list the reference stations without footprints\n",
    "    if len(list_stations_without_footprints)>0:\n",
    "        \n",
    "        stations_without_footprints_string = ', '.join(list_stations_without_footprints)\n",
    "                 \n",
    "        display(HTML('<p style=\"font-size:12px;\">Reference stations without footprints and not included for the multiple variables graph: ' + stations_without_footprints_string + '</p>'))\n",
    "       \n",
    "    #these are returned to the function \"multiple_variables_graph\"\n",
    "    return list_of_lists_all, list_stations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_variables_graph(all_stations, selected_station, station_name, date_range, start_date, end_date, timeselect_list, timeselect, title='', save_figs=''):\n",
    "    \n",
    "    #if the user selection is to use all footprints of 2017 or 2018, use saved values for all the \n",
    "    #reference stations (and a few more- all the stations used in Storm(2020))\n",
    "    if start_date.year==2018 and start_date.month==1 and start_date.day==1 and end_date.year==2018 and end_date.month==12 and end_date.day==31 and len(timeselect_list)==8:\n",
    "        df_saved=pd.read_csv('condensed_multiple_values_all_2018.csv')\n",
    "        predefined=True\n",
    "        \n",
    "        \n",
    "    elif start_date.year==2017 and start_date.month==1 and start_date.day==1 and end_date.year==2017 and end_date.month==12 and end_date.day==31 and len(timeselect_list)==8:\n",
    "        df_saved=pd.read_csv('condensed_multiple_values_all_2017_upd.csv')\n",
    "        predefined=True\n",
    "\n",
    "    #if different date-range, compute these values.\n",
    "    else:\n",
    "        #\"all_stations\" contain all reference stations as well as the selected station (possibly one of the reference stations)\n",
    "        list_of_lists_all, list_stations= compute_values_multiple_variable_graph(all_stations, selected_station, date_range, timeselect_list, save_figs=save_figs)\n",
    "        \n",
    "        predefined=False\n",
    "           \n",
    "    #if all of 2017 or all of 2018 - create the list_of_lists_all mainly from the saved dataframes \n",
    "    if predefined==True:\n",
    "        \n",
    "        #dataframe to list\n",
    "        lists_from_csv=df_saved.values.tolist()\n",
    "\n",
    "        #in list_of_lists_all, do not want the station names:\n",
    "        list_of_lists_all=[list_item[1:] for list_item in lists_from_csv]\n",
    "\n",
    "        #for the list of stations to loop over - only want the station names in a list\n",
    "        list_stations=[list_item[0] for list_item in lists_from_csv]\n",
    "        \n",
    "        #check so all stations we want (defined in all_stations - not interactive currently)\n",
    "        #are in the imported csv-file\n",
    "        for station in all_stations:\n",
    "            \n",
    "            #if a station is not in the list with pre-computed data, need to compute it\n",
    "            if station not in list_stations:\n",
    "                \n",
    "                #added \"_extra\" - don't want to replace the current lists\n",
    "                list_of_lists_all_extra, list_stations_extra= compute_values_multiple_variable_graph([station],selected_station,date_range, timeselect_list, save_figs=save_figs)\n",
    "                \n",
    "                #only one value to append - don't want two lists\n",
    "                list_stations.append(list_stations_extra[0])\n",
    "                list_of_lists_all.append(list_of_lists_all_extra[0])\n",
    "                \n",
    "        #check so there are not any stations in the csv-file which are not in the all_stations list \n",
    "        index=0\n",
    "        \n",
    "        list_index_delete=[]\n",
    "        \n",
    "        for station_csv_list in list_stations:\n",
    "\n",
    "            #if a station among the pre-computed values is not in the list of all_stations, it should be removed\n",
    "            if station_csv_list not in all_stations:\n",
    "\n",
    "                if station_csv_list!=selected_station[0]:\n",
    "\n",
    "                    list_index_delete.append(index)\n",
    "     \n",
    "            index=index+1\n",
    "\n",
    "        list_stations = [i for j, i in enumerate(list_stations) if j not in list_index_delete]\n",
    "        \n",
    "        list_of_lists_all = [i for j, i in enumerate(list_of_lists_all) if j not in list_index_delete]\n",
    "\n",
    "    \n",
    "    #DONE GETTING ALL THE DATA: \n",
    "    #now loop that generates the multiple variables graph\n",
    "    #often only one loop - one graph. \n",
    "    for selected_station in selected_station:\n",
    "\n",
    "        #sensitivity is the first attribut (list_item[0]) in the each of the lists (one list per station)\n",
    "        min_sens=min([list_item[0] for list_item in list_of_lists_all])\n",
    "        range_sens=max([list_item[0] for list_item in list_of_lists_all])-min_sens\n",
    "        \n",
    "        #these lists (list_sensitivity, list_population, list_point_source) will be used to generate texts \n",
    "        #for the station characterization PDFs (if choose to create a PDF)\n",
    "        #--> hence into list here, and not for GEE, respiration and anthropogenic contribution\n",
    "        list_sensitivity=[list_item[0] for list_item in list_of_lists_all]\n",
    "\n",
    "        min_gee=min([abs(list_item[1]) for list_item in list_of_lists_all])\n",
    "        range_gee=max([abs(list_item[1]) for list_item in list_of_lists_all])-min_gee\n",
    "\n",
    "        min_resp=min([list_item[2] for list_item in list_of_lists_all])\n",
    "        range_resp=max([list_item[2] for list_item in list_of_lists_all])-min_resp\n",
    "\n",
    "        min_anthro=min([list_item[3] for list_item in list_of_lists_all])\n",
    "        range_anthro=max([list_item[3] for list_item in list_of_lists_all])-min_anthro\n",
    "\n",
    "        min_pointsource=min([list_item[4] for list_item in list_of_lists_all])\n",
    "        range_pointsource=max([list_item[4] for list_item in list_of_lists_all])-min_pointsource\n",
    "        list_point_source=[list_item[4] for list_item in list_of_lists_all]\n",
    "\n",
    "        min_population=min([list_item[5] for list_item in list_of_lists_all])\n",
    "        range_population=max([list_item[5] for list_item in list_of_lists_all])-min_population\n",
    "        list_population=[list_item[5] for list_item in list_of_lists_all]\n",
    "\n",
    "\n",
    "        list_of_lists_all_normalized=[]\n",
    "        index=0\n",
    "        for list_item in list_of_lists_all:\n",
    "\n",
    "            \n",
    "            #one list_normalized per station\n",
    "            list_normalized=[]\n",
    "            \n",
    "            #for station x-ticks.\n",
    "            #first station that is looped over end at the 0% placement on the y-axis\n",
    "            if index==0:\n",
    "                list_normalized.append(0)\n",
    "            #the remaining stations end up gradually higher on the y-axis. Last station end at 100%.\n",
    "            else:\n",
    "                list_normalized.append((index/(len(list_of_lists_all)-1))*100)\n",
    "            \n",
    "            #for station - extra space (see x-ticks - spanning accross two ticks)\n",
    "            #same value as above.\n",
    "            if index==0:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                list_normalized.append((index/(len(list_of_lists_all)-1))*100)\n",
    "            \n",
    "            #if it is the station with the lowest sensitivity out of all reference stations (+selected station)\n",
    "            #add 0 (the minimum value has 0% of the maximum value)\n",
    "            if list_item[0]==min_sens:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                norm_sens=((list_item[0]-min_sens)/range_sens)*100\n",
    "                list_normalized.append(norm_sens)\n",
    "\n",
    "            #changed order here because want this order in the graph (list_item[5])\n",
    "            if list_item[5]==min_population:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                norm_population=((list_item[5]-min_population)/range_population)*100\n",
    "                list_normalized.append(norm_population)\n",
    "                \n",
    "            if list_item[4]==min_pointsource:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                norm_pointsource=((list_item[4]-min_pointsource)/range_pointsource)*100\n",
    "                list_normalized.append(norm_pointsource)\n",
    "                \n",
    "\n",
    "            if abs(list_item[1])==min_gee:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                norm_gee=((abs(list_item[1])-min_gee)/range_gee)*100\n",
    "                list_normalized.append(norm_gee)\n",
    "\n",
    "            if list_item[2]==min_resp:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                norm_resp=((list_item[2]-min_resp)/range_resp)*100\n",
    "                list_normalized.append(norm_resp)\n",
    "\n",
    "            if list_item[3]==min_anthro:\n",
    "                list_normalized.append(0)\n",
    "            else:\n",
    "                norm_anthro=((list_item[3]-min_anthro)/range_anthro)*100\n",
    "                list_normalized.append(norm_anthro)\n",
    "\n",
    "            list_of_lists_all_normalized.append(list_normalized)\n",
    "            index=index+1\n",
    "\n",
    "        list_attributes=['Station', '', 'Sensitivity', 'Population', 'Point source contribution', 'GEE', 'Respiration', 'Anthropogenic contribution']\n",
    "        \n",
    "        #max 15 characters in lable\n",
    "        list_attributes=[textwrap.fill(text,15) for text in list_attributes]\n",
    "\n",
    "        #create the figure\n",
    "        matplotlib.rcParams.update({'font.size': 14})\n",
    "        fig = plt.figure(figsize=(10,9)) \n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        #added - get on the right side of the plot\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        \n",
    "        #remove the ticks (lenght 0 - keep the names)\n",
    "        ax.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "        #get correct station information as loop though lists\n",
    "        index=0\n",
    "        max_sens_val=[]\n",
    "        order=len(all_stations)-1\n",
    "        \n",
    "        first=True\n",
    "        \n",
    "        #add position for station in list_of_lists_all_normalized\n",
    "        for each_station_list in list_of_lists_all_normalized:\n",
    "\n",
    "            station=list_stations[index]\n",
    "\n",
    "            if station==selected_station:\n",
    "                \n",
    "                #if at the same index...? Take all the population_sens values and all stations. Sort both. \n",
    "                #get what index the selected station is at. \n",
    "\n",
    "                plt.plot(list_attributes, each_station_list, linestyle='-', marker='o', lw=3, color= 'black', label=station_name, zorder=len(all_stations))\n",
    "\n",
    "                max_sens_val.append(each_station_list[0])\n",
    "                \n",
    "                ax.text('Station', each_station_list[0]+1, station)\n",
    "\n",
    "            else:\n",
    "                if first==True:\n",
    "\n",
    "                    plt.plot(list_attributes, each_station_list, linestyle=':', lw=0.5,color='blue', zorder=order, label='All other stations')\n",
    "                    ax.text('Station', each_station_list[0], station)\n",
    "                    \n",
    "                else:\n",
    "                    plt.plot(list_attributes, each_station_list, linestyle=':', lw=0.5,color='blue', zorder=order, label='_nolegend_')\n",
    "                    ax.text('Station', each_station_list[0], station)\n",
    "\n",
    "                    \n",
    "                first=False\n",
    "\n",
    "                order=order-1\n",
    "\n",
    "            index=index+1\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        #sorted according to sensitivity\n",
    "        labels_sorted = [x for _,x in sorted(zip(max_sens_val,labels), reverse=True)]\n",
    "        handles_sorted = [x for _,x in sorted(zip(max_sens_val,handles), reverse=True)]\n",
    "        \n",
    "        if title=='yes':\n",
    "            if predefined==False:\n",
    "                date_index_number = (len(date_range) - 1)\n",
    "                plt.title(selected_station  + ' relative to max of all atmospheric stations' + '\\n' + str(date_range[0].year) \\\n",
    "                        + '-' + str(date_range[0].month) + '-' + str(date_range[0].day)\\\n",
    "                        + ' to ' + str(date_range[date_index_number].year) + '-' + str(date_range[date_index_number].month) + '-' + str(date_range[date_index_number].day)\\\n",
    "                        + ', Hour(s): ' + timeselect)\n",
    "            else:\n",
    "                plt.title(selected_station  + ' relative to max of all atmospheric stations' + '\\n' + 'Year ' + str(start_date.year))\n",
    "\n",
    "        ax.set_ylabel('% of max')\n",
    "\n",
    "        ax.tick_params(axis='y')\n",
    "\n",
    "        list_attributes_upd=list_attributes\n",
    "        \n",
    "        list_attributes_upd[0]=''\n",
    "    \n",
    "        ax.set_xticklabels(list_attributes_upd, rotation='vertical')\n",
    "        \n",
    "        #add text- station\n",
    "        ax.text(0, -10, 'Station', fontsize=15,weight = 'bold')\n",
    "\n",
    "        ax.yaxis.grid(True)\n",
    "        \n",
    "        #plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        if save_figs=='yes':\n",
    "            \n",
    "            plotdir='figures'\n",
    "            pngfile=selected_station + '_figure_5'\n",
    "            fig.savefig(plotdir+'/'+pngfile+'.pdf',dpi=110, bbox_inches='tight')   \n",
    "            \n",
    "            #save the texts:\n",
    "            #values saved in the same order as list_stations\n",
    "            list_stations_sorted_population = [x for _,x in sorted(zip(list_population,list_stations))]\n",
    "\n",
    "            list_stations_sorted_sensitivity = [x for _,x in sorted(zip(list_sensitivity,list_stations))]\n",
    "\n",
    "            list_stations_sorted_point_source = [x for _,x in sorted(zip(list_point_source,list_stations))]\n",
    "\n",
    "            #ranking of the selected station (index)\n",
    "            index_station_population=list_stations_sorted_population.index(selected_station)\n",
    "\n",
    "            index_station_sensitivity=list_stations_sorted_sensitivity.index(selected_station)\n",
    "\n",
    "            index_station_point_source=list_stations_sorted_point_source.index(selected_station)\n",
    "            \n",
    "            #get their ranking in % (lowest to highest)\n",
    "            percent_population=((index_station_population+1)/len(list_stations))*100\n",
    "\n",
    "            percent_sensitivity=((index_station_sensitivity+1)/len(list_stations))*100\n",
    "\n",
    "            percent_point_source=((index_station_point_source+1)/len(list_stations))*100\n",
    "\n",
    "\n",
    "            if percent_population<25:\n",
    "                population_text='first quartile'\n",
    "            elif percent_population>=25 and percent_population<50:\n",
    "                population_text='second quartile'\n",
    "\n",
    "            elif percent_population>=50 and percent_population<75:\n",
    "                population_text='third quartile'\n",
    "            else:\n",
    "                population_text='fourth quartile'\n",
    "            \n",
    "            #got an error and worked to add this (something like \"variable referenced before assigned\")\n",
    "            sensitivity_text=''\n",
    "            if percent_sensitivity<25:\n",
    "                sensitivity_text='first quartile'\n",
    "            elif percent_sensitivity>=25 and percent_sensitivity<50:\n",
    "                sensitivity_text='second quartile'\n",
    "\n",
    "            elif percent_sensitivity>=50 and percent_sensitivity<75:\n",
    "                sensitivity_text='third quartile'\n",
    "            else:\n",
    "                sensitivity_text=='fourth quartile'\n",
    "\n",
    "\n",
    "            if percent_point_source<25:\n",
    "                point_source_text_1='first quartile'\n",
    "            elif percent_point_source>=25 and percent_point_source<50:\n",
    "                point_source_text_1='second quartile'\n",
    "            elif percent_point_source>=50 and percent_point_source<75:\n",
    "                point_source_text_1='third quartile'\n",
    "            else:\n",
    "                point_source_text_1='fourth quartile'\n",
    "\n",
    "            #create the text-files:\n",
    "            file_sensitivity='texts/' + selected_station + '_text_7.txt'\n",
    "            open_file= open(file_sensitivity, \"w\")\n",
    "            open_file.write(sensitivity_text)\n",
    "            open_file.close() \n",
    " \n",
    "            file_population='texts/' + selected_station + '_text_8.txt'\n",
    "            open_file= open(file_population, \"w\")\n",
    "            open_file.write(population_text)\n",
    "            open_file.close() \n",
    "\n",
    "            file_point_source_1='texts/' + selected_station + '_text_9.txt'\n",
    "            open_file= open(file_point_source_1, \"w\")\n",
    "            open_file.write(point_source_text_1)\n",
    "            open_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Widget selection</h3>\n",
    "<p>Sets up the widgets with selection of station, date range etc.\n",
    "The selection is in turn used as variables when running the different functions in this Notebook. \n",
    "The function outputs, along with text, is shown in output widgets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widgets for selection (ICOS data only)\n",
    "def whole_characterization_widgets_upd_savefigs_stilt_stations_option():\n",
    "     \n",
    "    #for changes in the dropdowns for start- and end date\n",
    "    stilt_stations = create_STILT_dictionary()\n",
    "    available_STILT= available_STILT_dictionary()\n",
    "    \n",
    "    def change_stn_type(c):\n",
    "        if station_type.value=='STILT stations':\n",
    "            \n",
    "            list_of_name_tuples=[]\n",
    "            for station in stilt_stations:   \n",
    "                if len(stilt_stations[station]['name'])>0:\n",
    "                    \n",
    "                    #if elevation already in the name, don't want to include it twide. Check is last part of string is 'm' for meter and a digit for the elevation\n",
    "                    if stilt_stations[station]['name'][-1]=='m' and stilt_stations[station]['name'][-2].isdigit():\n",
    "                        name_elevation=stilt_stations[station]['name'] \n",
    "                        \n",
    "                    else:\n",
    "                    \n",
    "                        name_elevation= stilt_stations[station]['name'] + ' ' + str(stilt_stations[station]['alt']) +'m'\n",
    "                    \n",
    "                    name_tuple=(name_elevation, station)\n",
    "                else:\n",
    "                    name_elevation= station  + ' ' + str(stilt_stations[station]['alt']) +'m'\n",
    "                    name_tuple=(name_elevation, station)\n",
    "                list_of_name_tuples.append(name_tuple)\n",
    "                \n",
    "            station_choice.options=list_of_name_tuples\n",
    "            station_choice.value=None\n",
    "        else:\n",
    "            \n",
    "            station_choice.options=labeled_list_of_name_tuples\n",
    "            station_choice.value=None\n",
    "     \n",
    "\n",
    "    def change_stn(c):\n",
    " \n",
    "        if station_choice.value is not None:\n",
    "     \n",
    "            #same as station.value\n",
    "            selected_station= station_choice.value\n",
    "\n",
    "            options_station_specific=available_STILT[selected_station]['years']\n",
    "\n",
    "\n",
    "            for i in range(0, len(options_station_specific)): \n",
    "                options_station_specific[i] = int(options_station_specific[i]) \n",
    "\n",
    "            options_station_specific=sorted(options_station_specific)\n",
    "\n",
    "\n",
    "            s_year.options=options_station_specific\n",
    "\n",
    "\n",
    "            e_year.options=options_station_specific\n",
    "        \n",
    "\n",
    "    def change_yr(c):\n",
    "\n",
    "        #selected_station = [key for (key, value) in stilt_stations.items() if value['name'] == station.value]\n",
    "        \n",
    "        selected_station=station_choice.value\n",
    "                                             \n",
    "        #str(c['new']) replaced with str(s_year.value)\n",
    "        months= available_STILT[selected_station][str(s_year.value)]['months']\n",
    "\n",
    "        months_int=[]\n",
    "        #last entry in the list of string is not a month\n",
    "        for i in range(0, (len(months)-1)): \n",
    "            months_int.append(int(months[i]))\n",
    "\n",
    "        months_int=sorted(months_int)\n",
    "\n",
    "        s_month.options=months_int\n",
    "\n",
    "        #when change start year - should also update end_year. Not earlier than the selected year\n",
    "\n",
    "        options_station_specific=available_STILT[selected_station]['years']\n",
    "\n",
    "        for i in range(0, len(options_station_specific)): \n",
    "            options_station_specific[i] = int(options_station_specific[i]) \n",
    "\n",
    "        options_station_specific=sorted(options_station_specific)\n",
    "\n",
    "        #if the e_year value is smaller than the new start year value, it must be updated.\n",
    "        #check if any of the options are smaller than selected start year\n",
    "        updated_list=[]\n",
    "        for item in s_year.options:\n",
    "                                                 \n",
    "            #c['new'] replaced with s_year.value \n",
    "            if item>=int(s_year.value):\n",
    "\n",
    "                updated_list.append(item)\n",
    "\n",
    "        e_year.options=updated_list\n",
    "\n",
    "\n",
    "    def change_mt(c):\n",
    "\n",
    "        #the day widget populated depending on what month it is (different number of days)\n",
    "        month_days_29=[2]\n",
    "\n",
    "        month_days_30=[4,6,9,11]\n",
    "\n",
    "        month_days_31=[1,3,5,7,8,10,12]\n",
    "\n",
    "        if c['new'] in month_days_31:\n",
    "            s_day.options=list(range(1,32))\n",
    "\n",
    "        elif c['new'] in month_days_30:\n",
    "            s_day.options=list(range(1,31))\n",
    "\n",
    "        else:\n",
    "            s_day.options=list(range(1,29))\n",
    "\n",
    "        #when change start_month - change end month also (if same year)\n",
    "        if s_year.value==e_year.value or len(e_month.options)==0:\n",
    "\n",
    "            updated_list=[]\n",
    "            for item in s_month.options:\n",
    "                if item>=int(c['new']):\n",
    "\n",
    "                    updated_list.append(item)\n",
    "\n",
    "            e_month.options=updated_list\n",
    "\n",
    "            \n",
    "        #when change start_month - change end day also (if same year and month OR the first time)\n",
    "        if s_year.value==e_year.value and s_month.value==e_month.value or len(e_day.options)==0:\n",
    "\n",
    "            updated_list=[]\n",
    "            for item in s_day.options:\n",
    "                if item>=int(s_day.value):\n",
    "\n",
    "                    updated_list.append(item)\n",
    "\n",
    "\n",
    "            e_day.options=updated_list\n",
    "\n",
    "\n",
    "    def change_yr_end(c):\n",
    "\n",
    "        selected_station = [key for (key, value) in stilt_stations.items() if value['name'] == station_choice.value]\n",
    "\n",
    "        #changed from c['new']\n",
    "        if s_year.value==e_year.value:\n",
    "            \n",
    "            updated_list=[]\n",
    "\n",
    "            for item in s_month.options:\n",
    "\n",
    "                if item>=s_month.value:\n",
    "                    updated_list.append(item)\n",
    "\n",
    "            e_month.options=updated_list\n",
    "\n",
    "        else:\n",
    "\n",
    "            #available months given specified end year (different from start year) --> then all those months are up for choice!\n",
    "            \n",
    "            #changed str(c['new']) to e_year.value\n",
    "            months= available_STILT[selected_station][str(e_year.value)]['months']\n",
    "\n",
    "            months_int=[]\n",
    "            \n",
    "            #last entry in the list of string is not a month\n",
    "            for i in range(0, (len(months)-1)): \n",
    "                months_int.append(int(months[i]))\n",
    "\n",
    "            months_int=sorted(months_int)\n",
    "\n",
    "\n",
    "            #get the available months for given year\n",
    "            e_month.options=months_int\n",
    "\n",
    "    def change_day(c):\n",
    "\n",
    "        #when change the day... if the same month and year (start) - update\n",
    "        if s_year.value==e_year.value and s_month.value==e_month.value:\n",
    "\n",
    "            updated_list=[]\n",
    "\n",
    "            for item in s_day.options:\n",
    "\n",
    "                if item>=s_day.value:\n",
    "                    updated_list.append(item)\n",
    "\n",
    "            e_day.options=updated_list\n",
    "\n",
    "\n",
    "    def change_month_end(c):\n",
    "\n",
    "        if s_year.value==e_year.value and e_month.value==s_month.value:\n",
    "\n",
    "            updated_list=[]\n",
    "\n",
    "            for item in s_day.options:\n",
    "\n",
    "                if item>=s_day.value:\n",
    "                    updated_list.append(item)\n",
    "\n",
    "            e_day.options=updated_list\n",
    "\n",
    "        else:\n",
    "\n",
    "            month_days_29=[2]\n",
    "\n",
    "            month_days_30=[4,6,9,11]\n",
    "\n",
    "            month_days_31=[1,3,5,7,8,10,12]\n",
    "\n",
    "            if c['new'] in month_days_31:\n",
    "                e_day.options=list(range(1,32))\n",
    "\n",
    "            elif c['new'] in month_days_30:\n",
    "                e_day.options=list(range(1,31))\n",
    "\n",
    "            else:\n",
    "                e_day.options=list(range(1,29))\n",
    "\n",
    "\n",
    "    #get the station codes of all atmospheric stations in the labelling app:\n",
    "    stationList = station_data.getList(['as'])\n",
    "    \n",
    "    list_station_id=[]\n",
    "\n",
    "    for station in stationList:\n",
    "        list_station_id.append(station.stationId) \n",
    "        \n",
    "    #which stilt stations have the first three letters mathcing those of the atmospheric stations (often several per height)\n",
    "    list_labeled_stilt_stations=[]\n",
    "    for station_id in list_station_id:\n",
    "\n",
    "        for stilt_station in stilt_stations:\n",
    "            if stilt_station[0:3]==station_id:\n",
    "                list_labeled_stilt_stations.append(stilt_station)\n",
    "\n",
    "    \n",
    "    labeled_list_of_name_tuples=[]\n",
    "    \n",
    "    for station in list_labeled_stilt_stations:   \n",
    "        if len(stilt_stations[station]['name'])>0:\n",
    "\n",
    "            #if elevation already in the name, don't want to include it twice. Check is last part of string is 'm' for meter and a digit for the elevation\n",
    "            if stilt_stations[station]['name'][-1]=='m' and stilt_stations[station]['name'][-2].isdigit():\n",
    "                name_elevation=stilt_stations[station]['name'] \n",
    "\n",
    "            else:\n",
    "\n",
    "                name_elevation= stilt_stations[station]['name'] + ' ' + str(stilt_stations[station]['alt']) +'m'\n",
    "\n",
    "            name_tuple=(name_elevation, station)\n",
    "        else:\n",
    "            name_elevation= station  + ' ' + str(stilt_stations[station]['alt']) +'m'\n",
    "            name_tuple=(name_elevation, station)\n",
    "        labeled_list_of_name_tuples.append(name_tuple)\n",
    "\n",
    "    \n",
    "    #make it possible to see whole name in ex the text of a radio button \n",
    "    style_bin = {'description_width': 'initial'}\n",
    "    \n",
    "    #Create a Dropdown widget with station names:\n",
    "    #maybe let it be coded (ex GAT344), but shown options \n",
    "    \n",
    "    #added\n",
    "    station_type=RadioButtons(\n",
    "            options=['ICOS stations', 'STILT stations'],\n",
    "            value='ICOS stations',\n",
    "            description=' ',\n",
    "            disabled=False)\n",
    "    \n",
    "    \n",
    "    #prev: station_name_code_for_dropdown\n",
    "    station_choice = Dropdown(options = labeled_list_of_name_tuples,\n",
    "                       description = 'Station',\n",
    "                       value=None,\n",
    "                       disabled= False,)\n",
    "    \n",
    "    #Create a Dropdown widget with year values (start year):\n",
    "    s_year = Dropdown(options = [],\n",
    "                      description = 'Start Year',\n",
    "                      disabled= False,)\n",
    "    \n",
    "    #Create a Dropdown widget with month values (start month):\n",
    "    s_month = Dropdown(options = [],\n",
    "                       description = 'Start Month',\n",
    "                       disabled= False,)\n",
    "    \n",
    "    #Create a Dropdown widget with year values (end year):\n",
    "    e_year = Dropdown(options = [],\n",
    "                      description = 'End Year',\n",
    "                      disabled= False,)\n",
    "    \n",
    "    #Create a Dropdown widget with month values (end month):\n",
    "    e_month = Dropdown(options = [],\n",
    "                       description = 'End Month',\n",
    "                       disabled= False,)\n",
    "    \n",
    "    s_day = Dropdown(options = [],\n",
    "                    description = 'Start Day',\n",
    "                    disabled = False,)\n",
    "    \n",
    "    e_day = Dropdown(options = [],\n",
    "                description = 'End Day',\n",
    "                disabled = False,)\n",
    "    \n",
    "    options_time_selection=[('0:00', 0), ('3:00', 3), ('06:00', 6), ('09:00', 9), ('12:00', 12), ('15:00', 15), ('18:00', 18), ('21:00', 21)]\n",
    "    \n",
    "    time_selection= SelectMultiple(\n",
    "        options=options_time_selection,\n",
    "        value=[0, 3, 6, 9, 12, 15, 18, 21],\n",
    "        style=style_bin,\n",
    "        description='Time of day',\n",
    "        disabled=False)\n",
    "    \n",
    "    \n",
    "    bin_size = Dropdown(options = [15, 30, 60, 90, 180, 360],\n",
    "                description = 'Bin size (degrees)', style=style_bin,\n",
    "                disabled = False,)\n",
    "\n",
    "    interval = IntText(\n",
    "            value=100,\n",
    "            min=50,\n",
    "            max=500,\n",
    "            description='Interval (km)',\n",
    "            disabled=False,\n",
    "            step=50)\n",
    "    \n",
    "    \n",
    "    #selection percent/absolut: \n",
    "    \n",
    "    unit_value=RadioButtons(\n",
    "            options=['percent', 'absolute'],\n",
    "            value='percent',\n",
    "            style=style_bin,\n",
    "            disabled=False)\n",
    "    \n",
    "\n",
    "\n",
    "    #selection label landcover windrose: \n",
    "    \n",
    "    landcover_windrose_label =RadioButtons(\n",
    "            options=['yes', 'no'],\n",
    "            value='yes',\n",
    "            description='Add labels to the land cover polar graph:',\n",
    "            style=style_bin,\n",
    "            disabled=False)\n",
    "\n",
    "    #selection include titles or not\n",
    "    include_labels =RadioButtons(\n",
    "            options=['yes', 'no'],\n",
    "            value='yes',\n",
    "            description='Add titles to the figures:',\n",
    "            style=style_bin,\n",
    "            disabled=False)\n",
    "    \n",
    "\n",
    "    save_figs=RadioButtons(\n",
    "            options=['yes', 'no'],\n",
    "            style=style_bin,\n",
    "            value='no',\n",
    "            description= 'Do you want to save the figures:',\n",
    "            disabled=False)\n",
    "    \n",
    "    #Create a Button widget to control execution:\n",
    "    update_button = Button(description='Update',\n",
    "                           disabled=False,\n",
    "                           button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                           tooltip='Click me',)\n",
    "    \n",
    "\n",
    "    #this is just text that is put in a Vbox (vertical box) ABOVE (verticla) the station selection\n",
    "    #(\"Select here station and time range\")\n",
    "    header_station = Output()\n",
    "    with header_station:\n",
    "        display(HTML('<p style=\"font-size:15px;font-weight:bold;\">Select station: </p>'))\n",
    "        \n",
    "    header_date_time = Output()\n",
    "    \n",
    "    \n",
    "    with header_date_time:\n",
    "        display(HTML('<p style=\"font-size:15px;font-weight:bold;\"><br>Select date and time: </p>'))\n",
    "        \n",
    "\n",
    "    \n",
    "    #added\n",
    "    header_bin_specifications = Output()\n",
    "    \n",
    "    with header_bin_specifications:\n",
    "        display(HTML('<p style=\"font-size:15px;font-weight:bold;\"><br>Select bin size and intervals: </p>'))\n",
    "\n",
    "    \n",
    "    header_unit = Output()\n",
    "    \n",
    "    with header_unit:\n",
    "        display(HTML('<p style=\"font-size:15px;font-weight:bold;\"><br>Unit: </p><p style=\"font-size:12px;width: 250px;\">\\\n",
    "        Select representation of surface influence in <b>percent</b> for optimal display of a single station or <b>absolute</b> values for \\\n",
    "        intercomparison between stations </p>'))\n",
    "                 \n",
    "    header_style = Output()\n",
    "    \n",
    "    with header_style:\n",
    "        display(HTML('<p style=\"font-size:15px;font-weight:bold;\"><br><br></p>'))\n",
    "        \n",
    "    header_save_figs = Output()\n",
    "    \n",
    "    #to make it align with the other texts.\n",
    "    with header_save_figs:\n",
    "        display(HTML('<p style=\"font-size:15px;font-weight:bold;\"><br><br></p>'))\n",
    "            \n",
    "    #vertical box with the heading (header_station) and the station dropdown\n",
    "    station_box = VBox([header_station,station_type,station_choice, header_date_time])\n",
    "    \n",
    "    #NOTE vertical - start year above end year\n",
    "    year_box = VBox([s_year, e_year])\n",
    "    month_box = VBox([s_month, e_month])\n",
    "    day_box = VBox([s_day, e_day])\n",
    "\n",
    "    #the two vertical boxes next to each other in a horizontal box\n",
    "    #Add both time-related VBoxes to a HBox:\n",
    "    time_box = HBox([year_box, month_box, day_box])\n",
    "    \n",
    "    #added\n",
    "    bin_box_1 = HBox([bin_size, interval])\n",
    "    \n",
    "    h_box_1 = HBox([header_unit, header_style])\n",
    "    \n",
    "    \n",
    "    v_box_1 = VBox([header_unit, unit_value])\n",
    "    \n",
    "    v_box_2 = VBox([header_style, include_labels, landcover_windrose_label])\n",
    "    \n",
    "    v_box_3 = VBox([header_save_figs, save_figs, update_button])\n",
    "    \n",
    "    bin_box_2 = HBox([v_box_1, v_box_2, v_box_3])\n",
    "\n",
    "    #Add all widgets to a VBox:\n",
    "    form = VBox([station_box, time_box, time_selection, header_bin_specifications, bin_box_1,bin_box_2])\n",
    "\n",
    "    #Set font of all widgets in the form:\n",
    "    station_choice.layout.width = '603px'\n",
    "    time_box.layout.margin = '25px 0px 10px 0px'\n",
    "    year_box.layout.margin = '0px 0px 0px 0px'\n",
    "    update_button.layout.margin = '50px 0px 0px 50px' #top, right, bottom, left\n",
    "    royal='#4169E1'\n",
    "    update_button.style.button_color=royal\n",
    "\n",
    "    #Initialize form output:\n",
    "    form_out = Output()\n",
    "\n",
    "    #Initialize results output widgets:\n",
    "    header_output = Output()\n",
    "    \n",
    "    result_sensitivity = Output()\n",
    "    \n",
    "    result_population = Output()\n",
    "    \n",
    "    result_pointsource = Output()\n",
    "    \n",
    "    result_land_cover_bar_graph = Output()\n",
    "    \n",
    "    result_seasonal_table = Output()\n",
    "    \n",
    "    header_advanced = Output()\n",
    "    \n",
    "    result_landcover_windrose = Output()\n",
    "    \n",
    "    result_multiple_variables_graph = Output()\n",
    "\n",
    "    \n",
    "    ########OBSERVERS - what happens when change ex. change start year (s_year)\n",
    "    \n",
    "    station_type.observe(change_stn_type, 'value')\n",
    "    \n",
    "    station_choice.observe(change_stn, 'value')\n",
    "\n",
    "\n",
    "    s_year.observe(change_yr, 'value')\n",
    "\n",
    "\n",
    "    s_month.observe(change_mt, 'value')\n",
    "\n",
    "\n",
    "    e_year.observe(change_yr_end, 'value')\n",
    "\n",
    "\n",
    "    s_day.observe(change_day, 'value')\n",
    "\n",
    "\n",
    "    e_month.observe(change_month_end, 'value')\n",
    "    \n",
    "    #Define update function (happends when click the button)\n",
    "    def update_func(button_c):        \n",
    "        \n",
    "        selected_station=station_choice.value\n",
    "        \n",
    "        station_code_stripped=selected_station[0:3]\n",
    "        \n",
    "        station_info = station_data.get(station_code_stripped)\n",
    "        \n",
    "        if station_info.valid==True:\n",
    "            station_name=station_info.name\n",
    "        else:\n",
    "            station_name=stilt_stations[station_code_stripped]['name']\n",
    "            \n",
    "        if save_figs.value=='yes':\n",
    "         \n",
    "            #using Claudio's station class to reterive necessary station information that will go\n",
    "            #in the station characterization PDFs\n",
    "            if station_info.valid==True:\n",
    "\n",
    "                station_lat=float(station_info.lat)\n",
    "\n",
    "                station_lon=float(station_info.lon)\n",
    "\n",
    "                station_country_code=station_info.country\n",
    "\n",
    "                station_site_type=station_info.siteType\n",
    "                \n",
    "                if station_country_code is not None:\n",
    "                    \n",
    "                    #API to reterive country name using country code. \n",
    "                    url='https://restcountries.eu/rest/v2/alpha/' + station_country_code\n",
    "\n",
    "                    resp = requests.get(url=url)\n",
    "\n",
    "                    country_information=resp.json()\n",
    "    \n",
    "                    station_country=country_information['name']\n",
    "                    \n",
    "                #text fits into the document. Long text because different with not an ICOS certified station\n",
    "                station_class='a class ' + station_info.icosclass + ' ICOS atmospheric station of the type '\n",
    "             \n",
    "            #if not in labeling app - check stilt dictionary.\n",
    "            else:\n",
    "                \n",
    "                #get what country the station is given latitude and longitude\n",
    "                #used in reverse geocoding\n",
    "                station_lat=float(stilt_stations[station_code_stripped]['lat'])\n",
    "                \n",
    "                station_lon=float(stilt_stations[station_code_stripped]['lon'])\n",
    "                \n",
    "                #reverse geocoding API - get country name from lat and lon\n",
    "                url='https://api.bigdatacloud.net/data/reverse-geocode-client?latitude=' + str(station_lat) + '&longitude=' + str(station_lon) + '12&localityLanguage=en'\n",
    "                \n",
    "                resp = requests.get(url=url)\n",
    "                \n",
    "                country_information=resp.json()\n",
    "                \n",
    "                station_country=country_information['countryName']\n",
    "                \n",
    "                #if not a certified ICOS station - save as empty strings. \n",
    "                #need to stay consistent because don't want errors when running latex.\n",
    "                station_class=''\n",
    "                \n",
    "                station_site_type=''\n",
    "            \n",
    "            #check if folder \"texts\" exists, else create it. \n",
    "            if not os.path.exists('texts'):\n",
    "                os.mkdir('texts')\n",
    "            \n",
    "            #save all the text files\n",
    "            file_station_name='texts/' + selected_station + '_text_1.txt'\n",
    "            open_file= open(file_station_name, \"w\")\n",
    "            open_file.write(station_name)\n",
    "            open_file.close() \n",
    "            \n",
    "            file_station_class='texts/' + selected_station + '_text_2.txt'\n",
    "            open_file= open(file_station_class, \"w\")\n",
    "            open_file.write(str(station_class))\n",
    "            open_file.close() \n",
    "    \n",
    "            file_station_type='texts/' + selected_station + '_text_3.txt'\n",
    "            open_file= open(file_station_type, \"w\")\n",
    "            open_file.write(station_site_type)\n",
    "            open_file.close() \n",
    "        \n",
    "            file_station_country='texts/' + selected_station + '_text_4.txt'\n",
    "            open_file= open(file_station_country, \"w\")\n",
    "            open_file.write(station_country)\n",
    "            open_file.close() \n",
    "                        \n",
    "            file_station_lat='texts/' + selected_station + '_text_5.txt'\n",
    "            open_file= open(file_station_lat, \"w\")\n",
    "            open_file.write(str(\"%.2f\" %station_lat))\n",
    "            open_file.close() \n",
    "            \n",
    "            file_station_lon='texts/' + selected_station + '_text_6.txt'\n",
    "            open_file= open(file_station_lon, \"w\")\n",
    "            open_file.write(str(\"%.2f\" %station_lon))\n",
    "            open_file.close()        \n",
    "        \n",
    "        \n",
    "        start_date=dt.datetime(s_year.value,s_month.value,s_day.value,0)\n",
    "\n",
    "        end_date=dt.datetime(e_year.value, e_month.value, e_day.value,0)\n",
    "\n",
    "        timeselect_list=list(time_selection.value)\n",
    "        \n",
    "        date_range=date_range_station_char(start_date, end_date, timeselect_list)\n",
    "        \n",
    "        timeselect=[str(value) for value in timeselect_list]\n",
    "        \n",
    "        timeselect=' '.join(timeselect)\n",
    "        \n",
    "        with header_output:\n",
    "            \n",
    "            clear_output()\n",
    "            \n",
    "            selected_station=station_choice.value\n",
    "\n",
    "            station_code_stripped=selected_station[0:3]\n",
    "\n",
    "            station_info = station_data.get(station_code_stripped)\n",
    "\n",
    "            if station_info.valid==True:\n",
    "                station_name=station_info.name\n",
    "            else:\n",
    "                station_name=stilt_stations[station_code_stripped]['name']\n",
    "            \n",
    "            display(HTML('<p style=\"font-size:35px;font-weight:bold;\"><br>' + station_name + ' station characterization</p><p style=\"font-size:16px;\"><br>Download\\\n",
    "            the specifications about the output figures and graphs <a href=\"specifications.pdf\">here</a></p>'))\n",
    "        \n",
    "            f = IntProgress(min=0, max=9, description='Loading:')\n",
    "\n",
    "            display(f) \n",
    "            \n",
    "            f.value += 1\n",
    "        \n",
    "        with result_sensitivity:\n",
    "            \n",
    "            clear_output()\n",
    "            \n",
    "            map_representation_polar_graph(selected_station, date_range, timeselect, bin_size=bin_size.value, unit=unit_value.value, \n",
    "                                        rose_type='sensitivity', colorbar='gist_heat_r', km_intervals=interval.value, title=include_labels.value,\n",
    "                                       save_figs=save_figs.value)\n",
    "            \n",
    " \n",
    "            f.value += 1\n",
    "            \n",
    "            \n",
    "        with result_pointsource:\n",
    "     \n",
    "            clear_output()\n",
    "            \n",
    "            map_representation_polar_graph(selected_station, date_range, timeselect, bin_size=bin_size.value, unit=unit_value.value, \n",
    "                                        rose_type='point source contribution', colorbar='Purples', km_intervals=interval.value, title=include_labels.value,\n",
    "                                       save_figs=save_figs.value)\n",
    "    \n",
    "        \n",
    "            f.value += 1\n",
    "            \n",
    "        with result_population:\n",
    "     \n",
    "            clear_output()\n",
    "            \n",
    "            map_representation_polar_graph(selected_station, date_range, timeselect, bin_size=bin_size.value, unit=unit_value.value, rose_type='population sensitivity', \n",
    "                                        colorbar='Greens', km_intervals=interval.value, title=include_labels.value,\n",
    "                                       save_figs=save_figs.value)\n",
    "\n",
    "            f.value += 1\n",
    "            \n",
    "            \n",
    "        with result_land_cover_bar_graph:\n",
    "            \n",
    "            clear_output()\n",
    "            \n",
    "            land_cover_bar_graph(selected_station, date_range, timeselect, title=include_labels.value, save_figs=save_figs.value)\n",
    "            \n",
    "            f.value +=1\n",
    "            \n",
    "        with result_seasonal_table:\n",
    "            clear_output()\n",
    "            \n",
    "            #timeselect automatically all hours\n",
    "            create_seasonal_table(selected_station, s_year.value, save_figs=save_figs.value)\n",
    "            \n",
    "            \n",
    "            f.value += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "        with header_advanced:\n",
    "            clear_output()\n",
    "            \n",
    "            display(HTML('<p style=\"font-size:35px;font-weight:bold;\">Advanced figures</p><p style=\"font-size:16px;\"><br>\\\n",
    "            We advice careful reading of the specifications before attempting to understand the following figures.</p>'))\n",
    "            \n",
    "        with result_landcover_windrose:\n",
    "                     \n",
    "            clear_output()\n",
    "\n",
    "            landcover_polar_graph(selected_station, date_range, timeselect, bin_size=bin_size.value, title=include_labels.value, percent_label=landcover_windrose_label.value, save_figs=save_figs.value)\n",
    "            \n",
    "            f.value += 1\n",
    "            \n",
    "        with result_multiple_variables_graph:\n",
    "            clear_output()\n",
    "            \n",
    "            #\"reference stations\" choosen by Ute\n",
    "            all_stations=['TRN180', 'SVB150', 'TOH147', 'SMR125', 'LUT', 'KRE250', 'IPR100', 'JFJ', 'KIT200', 'GAT344']\n",
    "            \n",
    "            #if selected_station not in above list, append it.\n",
    "            if selected_station not in all_stations:\n",
    "                all_stations.append(selected_station)\n",
    "                \n",
    "            selected_station_list=[]\n",
    "            selected_station_list.append(selected_station)\n",
    "            multiple_variables_graph(all_stations, selected_station_list, station_name, date_range, start_date, end_date, timeselect_list, timeselect, title=include_labels.value, save_figs=save_figs.value)\n",
    "            \n",
    "            #result_multiple_variables_graph_code(selected_station, start_date, end_date)\n",
    "            f.value = 9\n",
    "                \n",
    "    #Call update-function when button is clicked:\n",
    "    update_button.on_click(update_func)\n",
    "\n",
    "    \n",
    "    #Open form object:\n",
    "    with form_out:\n",
    "        \n",
    "        h_box_1=HBox([header_output])\n",
    "\n",
    "        grid=GridspecLayout(2, 2)\n",
    "\n",
    "        grid[0:1, 0:1] = result_sensitivity\n",
    "\n",
    "        grid[0:1, 1:2] = result_population\n",
    "        \n",
    "        grid[1:2, 0:1] = result_pointsource\n",
    "        \n",
    "        grid[1:2, 1:2] = result_land_cover_bar_graph\n",
    "\n",
    "        #table much \"thinner\" - make HBox rather than in grid \n",
    "        h_box_2=HBox([result_seasonal_table])\n",
    "        \n",
    "        #grid for the last two:\n",
    "            \n",
    "        h_box_3=HBox([header_advanced])\n",
    "            \n",
    "        grid_2 = GridspecLayout(1, 4)\n",
    "        grid_2[0:1, 0:2] = result_landcover_windrose\n",
    "        grid_2[0:1, 2:4] = result_multiple_variables_graph\n",
    "        \n",
    "    \n",
    "        display(form, h_box_1, grid, h_box_2, h_box_3, grid_2)\n",
    "\n",
    "    #Display form:\n",
    "    display(form_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
