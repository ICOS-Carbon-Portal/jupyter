{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook with modules for reading ICOS atmospheric observations\n",
    "\n",
    "Code to read ICOS atmospheric data provided by Karolina Pantazatou \n",
    "on 28 Nov 2018 in icos_timeseries_static_py2.ipynb\n",
    "\n",
    "Code to handle sparql queries and download from Carbon Portal provided by Claudio D'Onofrio\n",
    "on 22 Nov 2018 in sparqls.py, atc_co2_l2.py, and helper_functions.py\n",
    "\n",
    "<br />    \n",
    "<font color='red'> \n",
    "**Please copy the notebook to your own directory before making changes.** \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "#### Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import modules:\n",
    "import sys   \n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from operator import methodcaller\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import warnings\n",
    "import datetime as dt\n",
    "from datetime import datetime#, timedelta, date, time\n",
    "#import matplotlib\n",
    "#from matplotlib.ticker import FuncFormatter\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates and adds a column with datetime objects in a dataframe:\n",
    "def createDatetimeObjList(df_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Mon Nov 05 10:27:00 2019\n",
    "    Last Changed:     Mon Nov 05 10:27:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Add a column with datetime objects to the input dataframe. The datetime objects are created \n",
    "                      based on the content of the 'Year', 'Month', 'Day', 'Hour' and 'Minute' columns of every row.\n",
    "    Input parameters: ICOS data pandas dataframe (Atmospheric Level-1 or Level-2 Data)\n",
    "    Output:           pandas dataframe\n",
    "                      columns: \n",
    "                            1.  Station 3-character Code (var_name: 'Site', var_type: String)\n",
    "                            2.  Sampling Height (var_name: 'SamplingHeight', var_type: String)\n",
    "                            3.  Sampling Year (var_name: 'Year', var_type: String)\n",
    "                            4.  Sampling Month (var_name: 'Month', var_type: String)\n",
    "                            5.  Sampling Day (var_name: 'Day', var_type: String)\n",
    "                            6.  Sampling Hour (var_name: 'Hour', var_type: String)\n",
    "                            7.  Sampling Minute (var_name: 'Minute', var_type: String)\n",
    "                            8.  Sampling Decimal Date (var_name: 'DecimalDate', var_type: String)\n",
    "                            9.  Tracer/Gas concentration (var_name: 'ch4' or 'co2' or 'co', var_type: String)\n",
    "                            10. Standard Deviation (var_name: 'Stdev', var_type: String)\n",
    "                            11. Number of Points used for the measurment(var_name: 'NbPoints', var_type: String)\n",
    "                            12. Quality Flag (var_name: 'Flag', var_type: String)\n",
    "                            13. Instrument ID (var_name: 'InstrumentId', var_type: String)\n",
    "                            14. Quality ID (var_name: 'QualityId', var_type: String)\n",
    "                            15. Internal Flag: only for Level-1 data\n",
    "                                (var_name: 'InternalFlag', var_type: String)\n",
    "                            15. LTR: only for Level-2 data (var_name: 'LTR', var_type: String)\n",
    "                            16. Auto-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'AutoDescriptiveFlag', var_type: String)\n",
    "                            16. CMR: only for Level-2 data (var_name: 'CMR', var_type: String)\n",
    "                            17. Manual-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'ManualDescriptiveFlag', var_type: String)\n",
    "                            17: STTB: only for Level-2 data (var_name: 'STTB', var_type: String)\n",
    "                            18: Datetime object (var_name: 'DateTime', var_type: Stri)\n",
    "                            \n",
    "    \"\"\"\n",
    "    \n",
    "    #Create a list with datetime obj:\n",
    "    datetimeObj_list = [datetime.strptime((df_data['Year'][i]+\"/\"+\n",
    "                                           df_data['Month'][i]+\"/\"+\n",
    "                                           df_data['Day'][i]+\" \"+\n",
    "                                           df_data['Hour'][i] +\":\"+\n",
    "                                           df_data['Minute'][i]),'%Y/%m/%d %H:%M')\n",
    "                        for i in range(len(df_data))]\n",
    "    \n",
    "    #Add list with datetime objects to the data dataframe:\n",
    "    df_data['DateTime'] =  datetimeObj_list\n",
    "    \n",
    "    #Return dataframe:\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates a dataframe with the data values:\n",
    "def icosDatadf(data, tracer, level=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Mon Oct 08 09:27:00 2019\n",
    "    Last Changed:     Mon Oct 08 09:27:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Read in observations from ICOS Atmosphere Data File (Level-1 or Level-2) to a pandas dataframe.\n",
    "    \n",
    "    Input parameters: 1. Text containing column names and all observations (var_name: 'data', var_type: String)\n",
    "                      2. Name of gas/tracer (var_name: 'tracer', var_type: String) - e.g. 'co2' or 'co' or 'ch4'\n",
    "                      3. Data level [optional] (var_name: 'level', var_type: Integer)\n",
    "    \n",
    "    Default value for level: The default value for data level is \"2\". Function calls for Level-2 data do not have\n",
    "                             to include a value for the level input parameter.\n",
    "    \n",
    "    Output:           pandas dataframe\n",
    "    \n",
    "                      columns: \n",
    "                            1.  Station 3-character Code (var_name: 'Site', var_type: String)\n",
    "                            2.  Sampling Height (var_name: 'SamplingHeight', var_type: String)\n",
    "                            3.  Sampling Year (var_name: 'Year', var_type: String)\n",
    "                            4.  Sampling Month (var_name: 'Month', var_type: String)\n",
    "                            5.  Sampling Day (var_name: 'Day', var_type: String)\n",
    "                            6.  Sampling Hour (var_name: 'Hour', var_type: String)\n",
    "                            7.  Sampling Minute (var_name: 'Minute', var_type: String)\n",
    "                            8.  Sampling Decimal Date (var_name: 'DecimalDate', var_type: String)\n",
    "                            9.  Tracer/Gas concentration (var_name: 'ch4' or 'co2' or 'co', var_type: String)\n",
    "                            10. Standard Deviation (var_name: 'Stdev', var_type: String)\n",
    "                            11. Number of Points used for the measurment(var_name: 'NbPoints', var_type: String)\n",
    "                            12. Quality Flag (var_name: 'Flag', var_type: String)\n",
    "                            13. Instrument ID (var_name: 'InstrumentId', var_type: String)\n",
    "                            14. Quality ID (var_name: 'QualityId', var_type: String)\n",
    "                            15. Internal Flag: only for Level-1 data\n",
    "                                (var_name: 'InternalFlag', var_type: String)\n",
    "                            15. LTR: only for Level-2 data (var_name: 'LTR', var_type: String)\n",
    "                            16. Auto-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'AutoDescriptiveFlag', var_type: String)\n",
    "                            16. CMR: only for Level-2 data (var_name: 'CMR', var_type: String)\n",
    "                            17. Manual-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'ManualDescriptiveFlag', var_type: String)\n",
    "                            17: STTB: only for Level-2 data (var_name: 'STTB', var_type: String)\n",
    "                            18: Datetime object (var_name: 'DateTime', var_type: Stri)\n",
    "                            \n",
    "    \"\"\"\n",
    "    \n",
    "    #Split data to rows:\n",
    "    data_rows = data.split('\\n')\n",
    "    \n",
    "    #Remove the first row (contains the column names)\n",
    "    #split its contents and add them to a list:\n",
    "    data_labels = data_rows.pop(0).split(';')\n",
    "    \n",
    "    #Sepparate the columns of the remaining rows:\n",
    "    d_rows_list = [data_rows[x].split(';') for x in range(len(data_rows))] \n",
    "    \n",
    "    #Create dataframe:\n",
    "    df_data = pd.DataFrame.from_records(d_rows_list, columns=data_labels)\n",
    "    \n",
    "    #Add column with DateTime objects:\n",
    "    df_data = createDatetimeObjList(df_data) \n",
    "    \n",
    "    #Check what Level the data belong to:\n",
    "    if(level==2):\n",
    "        \n",
    "        #Filter values based on flag 'O', which corresponds to quality controlled data:\n",
    "        df_data = df_data.loc[df_data['Flag']=='O']\n",
    "    \n",
    "    else:\n",
    "        #Set missing values (e.g. \"-999.99\") to NaN:\n",
    "        df_data.loc[df_data[tracer].astype(np.float16)<0, tracer] = np.nan\n",
    "        df_data.loc[df_data['Stdev'].astype(np.float16)<0, 'Stdev'] = np.nan\n",
    "    \n",
    "    #Return Data dataframe:\n",
    "    return df_data.set_index('DateTime') #set column 'DateTime' as index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates a dataframe with the metadata values:\n",
    "def icosMetadatadf(data):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Mon Oct 08 10:27:00 2019\n",
    "    Last Changed:     Mon Oct 08 10:27:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Read in metadata from ICOS Atmosphere Data File (Level-1 or Level-2) to a pandas dataframe.\n",
    "    \n",
    "    Input parameters: Text containing column names and all observations (var_name: 'data', var_type: String)\n",
    "    \n",
    "    Output:           pandas dataframe\n",
    "    \n",
    "                      columns: \n",
    "                            1. MetadataLabel (var_name: 'MetadataLabel', var_type: String)\n",
    "                                i.       Dataset Title (var_name: 'TITLE', var_type: String)\n",
    "                                ii.      Dataset File Name (var_name: 'FILE NAME', var_type: String)\n",
    "                                iii.     Data Format (var_name: 'DATA FORMAT', var_type: String)\n",
    "                                iv.      Total num of records (var_name: 'TOTAL LINES', var_type: String)\n",
    "                                v.       Metadata Header Lines (var_name: 'HEADER LINES', var_type: String)\n",
    "                                vi.      Project Data Version (var_name: 'PROJECT DATA VERSION', var_type: String)\n",
    "                                vii.     Data Product Type (var_name: 'DATA PRODUCT TYPE', var_type: String)\n",
    "                                viii.    Station 3-character Code (var_name: 'STATION CODE', var_type: String)\n",
    "                                ix.      Station Full Name (var_name: 'STATION NAME', var_type: String)\n",
    "                                x.       Station Category (var_name: 'STATION CATEGORY', var_type: String)\n",
    "                                xi.      Observation Category (var_name: 'OBSERVATION CATEGORY', var_type: String)\n",
    "                                xii.     Country (var_name: 'COUNTRY/TERRITORY', var_type: String)\n",
    "                                xiii.    Contributor (var_name: 'CONTRIBUTOR', var_type: String)\n",
    "                                xiv.     Latitude (var_name: 'LATITUDE', var_type: String)\n",
    "                                xv.      Longitude (var_name: 'LONGITUDE', var_type: String)\n",
    "                                xvi.     Altitude (var_name: 'ALTITUDE', var_type: String)\n",
    "                                xvii.    Number of Sampling Heights\n",
    "                                         (var_name: 'NUMBER OF SAMPLING HEIGHTS', var_type: String)\n",
    "                                xviii.   Sampling Height (var_name: 'SAMPLING HEIGHTS', var_type: String)\n",
    "                                xix.     Contact Information - email (var_name: 'CONTACT POINT', var_type: String)\n",
    "                                xx.      Observation Parameter - e.g. 'CO' (var_name: 'PARAMETER', var_type: String)\n",
    "                                xxi.     Covering Time Period (var_name: 'COVERING PERIOD', var_type: String)\n",
    "                                xxii.    Sampling Freequency (var_name: 'TIME INTERVAL', var_type: String)\n",
    "                                xxiii.   Measurement Unit (var_name: 'MEASUREMENT UNIT', var_type: String)\n",
    "                                xxiv.    Measurement Method (var_name: 'MEASUREMENT METHOD', var_type: String)\n",
    "                                xxv.     Sampling Type (var_name: 'SAMPLING TYPE', var_type: String)\n",
    "                                xxvi.    Time Zone (var_name: 'TIME ZONE', var_type: String)\n",
    "                                xxvii.   Measurement Scale (var_name: 'MEASUREMENT SCALE', var_type: String)\n",
    "                                xxviii.  Data Policy (var_name: 'DATA POLICY', var_type: String)\n",
    "                                xxix.    Comment Notes (var_name: 'COMMENT', var_type: String)\n",
    "                                \n",
    "                            2. MetadataInfo (var_name: 'MetadataInfo', var_type: String)\n",
    "                            \n",
    "                            \n",
    "    \"\"\"\n",
    "    \n",
    "    #Split the metadata values for label \"comment\":\n",
    "    metadata_split = data.split('\\n#   ')\n",
    "    \n",
    "    #Get the metadata rows:\n",
    "    metadata_rows = metadata_split.pop(0).split('\\n')\n",
    "    \n",
    "    #Remove \"# \" from rows:\n",
    "    metadata_rows = [metadata_rows[i].replace(\"# \", \"\") for i in range(len(metadata_rows))]\n",
    "    \n",
    "    #Split labels from values:\n",
    "    metadata_rows_split = [metadata_rows[i].split(': ') for i in range(len(metadata_rows))]\n",
    "    \n",
    "    #Get the metadata labels:\n",
    "    metadata_labels = [metadata_rows_split[i].pop(0) for i in range(len(metadata_rows_split))]\n",
    "    \n",
    "    #Remove \":\" from label \"COMMENT:\"\n",
    "    metadata_labels[-1]= metadata_labels[-1].replace(\":\", \"\")\n",
    "    \n",
    "    #Join the metadata values for the label \"comment\" to one string:\n",
    "    comment_values = \", \".join(metadata_split)\n",
    "    \n",
    "    #Add the metadata values for the label \"comment\" to the metadata-values list:\n",
    "    metadata_rows_split[-1] = [comment_values]\n",
    "    \n",
    "    #Construct the metadata-values list as a list of strings instead of a list of lists:\n",
    "    metadata_values = [metadata_rows_split[i].pop(0) for i in range(len(metadata_rows_split))]\n",
    "    \n",
    "    #Create a dictionary with the metadata-label and -values lists:\n",
    "    metadata_dict = {'MetadataLabel':metadata_labels,\n",
    "                     'MetadataInfo':metadata_values}\n",
    "    \n",
    "    #Create Metadata dataframe:\n",
    "    df_metadata = pd.DataFrame(metadata_dict,columns=['MetadataLabel', 'MetadataInfo'])\n",
    "    \n",
    "    #Make the 'MetadataLabel'-column an index:\n",
    "    meta_df_i = df_metadata.set_index(keys=['MetadataLabel'],inplace=False)\n",
    "\n",
    "    #Return Metadata dataframe:\n",
    "    return meta_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2dataframe(data, tracer, level=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:40:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:40:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Split an ICOS Level-1 or Level-2 Amtosheric Data text file to metadata and data parts.\n",
    "                      Call functions to read-in the metadata-text to a pandas dataframe and the data-text to\n",
    "                      another pandas dataframe. Return the newly created dataframes as output.\n",
    "                      \n",
    "    Input parameters: 1. Text containing ICOS Level-1 or Level-2 Atmospheric metadata and observation-data\n",
    "                         (var_name: 'data', var_type: String)\n",
    "                      2. Name of gas/tracer - e.g. 'co2' or 'co' or 'ch4'\n",
    "                         (var_name: 'tracer', var_type: String)\n",
    "                      3. Data level [optional]\n",
    "                         (var_name: 'level', var_type: Integer)\n",
    "    \n",
    "    Default value for level: The default value for data level is \"2\". Function calls for Level-2 data do not have\n",
    "                             to include a value for the level input parameter.\n",
    "    \n",
    "    Output:           2 pandas dataframes: metadata pandas dataframe, data pandas dataframe \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Split data to a list containing metadata and data values:\n",
    "    data_split = data.split('\\n#\\n#')\n",
    "    \n",
    "    #datasplit[0] -- > contains metadata\n",
    "    #datasplit[1] -- > contains data\n",
    "    \n",
    "    #Call function to create the ICOS Metadata dataframe:\n",
    "    df_metadata = icosMetadatadf(data_split[0])\n",
    "    \n",
    "    #Call function to create the ICOS Data dataframe:\n",
    "    df_data = icosDatadf(data_split[1], tracer, level)\n",
    "    \n",
    "    return df_metadata, df_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that converts data with data type \"bytes\" to data type \"string\":\n",
    "def byte2string(databytes):\n",
    "   \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:40:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:40:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Convert data from bytes to string.\n",
    "                      \n",
    "    Input parameters: Binary data containing ICOS Level-1 or Level-2 Atmospheric metadata and observation-data \n",
    "                      (var_name: 'data', var_type: String)\n",
    "    \n",
    "    Output:           Text data containing ICOS Level-1 or Level-2 Atmospheric metadata and observation-data.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert data to string:\n",
    "    datastring = databytes.decode(\"utf-8\")\n",
    "    \n",
    "    #Return converted data:\n",
    "    return datastring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that unzips a file at a given directory:\n",
    "def unzip(fullpath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:35:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:35:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Unzip zipped ICOS Level-1 or Level-2 Atmospheric Data Files.\n",
    "                      \n",
    "    Input parameters: Path to ICOS Level-1 or Level-2 Atmospheric Data file (var_name: 'fullpath', var_type: String)\n",
    "    \n",
    "    Output:           Unzipped data file.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Open zipfile in reading mode:\n",
    "    with zipfile.ZipFile(fullpath, 'r') as zf:\n",
    "        \n",
    "        #Store the unzipped file in the same directory as the zipped one\n",
    "        #zip_ref.extractall(pathtodir)\n",
    "        try:\n",
    "            data = zf.read(zf.namelist()[0])\n",
    "        except KeyError:\n",
    "            print('ERROR: Did not find %s in zip file' % zf.namelist()[0])\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ICOS_zipfile(filename, tracer, level=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:30:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:30:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Function that unzips an ICOS data file, checks if the unzipped file contain binary or text data,\n",
    "                      converts the binary data to to text and returns a pandas dataframe with metadata & \n",
    "                      a pandas dataframe with observation data.\n",
    "                      \n",
    "    Input parameters: 1. File name for ICOS Level-1 or Level-2 Atmospheric Data File\n",
    "                         (var_name: 'filename', var_type: String).\n",
    "                      2. Name of gas/tracer - e.g. 'co2' or 'co' or 'ch4'\n",
    "                         (var_name: 'tracer', var_type: String)\n",
    "                      3. Data level [optional]\n",
    "                         (var_name: 'level', var_type: Integer)\n",
    "    \n",
    "    Default value for level: The default value for data level is \"2\". Function calls for Level-2 data do not have\n",
    "                             to include a value for the level input parameter.                      2. \n",
    "    \n",
    "    Output:           2 pandas dataframes: metadata pandas dataframe, data pandas dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Unzip file: \n",
    "    data = unzip(filename) #retunrs data type bytes in python 3.x (instead of string)\n",
    "\n",
    "    #Check data type & convert to string:\n",
    "    if (type(data) == bytes):\n",
    "        data = byte2string(data)\n",
    "\n",
    "    #Call function to create a pandas dataframe for metadata & one for data:\n",
    "    df_metadata, df_data = str2dataframe(data, tracer, level)\n",
    "    \n",
    "    #Return data & metadata dataframes:\n",
    "    return df_metadata, df_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ute/Stations/Claudio/helper_functions.py\n",
    "\"\"\"\n",
    "Created on Wed Oct  3 2018\n",
    "Last change on Nov  1 2018\n",
    "@author: Claudio D'Onofrio\n",
    "\"\"\"\n",
    "\n",
    "__version__= \"0.1.0\"\n",
    "\n",
    "# create helper functions\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "def is_number(num):\n",
    "    \"\"\" check if we deal with a number \"\"\"\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "def checklib(module):\n",
    "    \"\"\" load a list of modoules if available, otherwise throw exception \"\"\"\n",
    "    import imp\n",
    "    for mod in module:\n",
    "        try:\n",
    "            imp.find_module(mod)\n",
    "            ret = 1\n",
    "        except ImportError as imperror:\n",
    "            print(imperror)\n",
    "            ret = 0\n",
    "    return ret\n",
    "\n",
    "#---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ute/Stations/Claudio/sparqls.py\n",
    "\"\"\"\n",
    "Created on Thu Nov 22 16:35:50 2018\n",
    "contains functions, returning complete sparql queries,\n",
    "to run against the ICOS Carbon Portal RDF Triple Store\n",
    "@author: Claudio D'Onofrio\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__version__ = \"0.1.0\"\n",
    "\n",
    "# personal storage of sparql queries \n",
    "# -----------------------------------------\n",
    "\n",
    "def atc_query(tracer,level=2):\n",
    "    \"\"\"\n",
    "        Return SPARQL query to get a list of\n",
    "        ICOS Atmospheric CO2, CO or MTO, level 2 or level 1 (=NRT) data objects\n",
    "       :return: SPARQL query to get all ATC Level <level> products for tracer <tracer>\n",
    "       :rtype: string \n",
    "    \"\"\"\n",
    "    tracer = tracer.lower().title()\n",
    "    dataobject = [\"NrtGrowingDataObject\",\"L2DataObject\"]\n",
    "    \n",
    "    query = \"\"\"\n",
    "        prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        select ?dobj ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd\n",
    "        FROM <http://meta.icos-cp.eu/resources/atmprodcsv/>\n",
    "        where {\n",
    "                BIND(<http://meta.icos-cp.eu/resources/cpmeta/atc\"\"\"+tracer+dataobject[level-1]+\"\"\"> AS ?spec)\n",
    "                ?dobj cpmeta:hasObjectSpec ?spec .\n",
    "\t\n",
    "                FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}\n",
    "                ?dobj cpmeta:hasSizeInBytes ?size .\n",
    "                ?dobj cpmeta:hasName ?fileName .\n",
    "                ?dobj cpmeta:wasSubmittedBy [\n",
    "                prov:endedAtTime ?submTime ;\n",
    "                prov:wasAssociatedWith ?submitter\n",
    "                ] .\n",
    "                ?dobj cpmeta:hasStartTime | (cpmeta:wasAcquiredBy / prov:startedAtTime) ?timeStart .\n",
    "                ?dobj cpmeta:hasEndTime | (cpmeta:wasAcquiredBy / prov:endedAtTime) ?timeEnd .\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "    return query\n",
    "##------------------------------------------------------------------------------\n",
    "    \n",
    "def atc_stationlist(station,tracer='co2',level=2):\n",
    "    \"\"\"\n",
    "        Return SPARQL query to get a list of\n",
    "        ICOS Atmospheric CO2, CO or MTO, level 2 or level 1 (=NRT) data objects\n",
    "        for all stations in list\n",
    "       :return: SPARQL query to get all ATC products for specific stations, tracer and ICOS-level\n",
    "       :rtype: string \n",
    "    \"\"\"\n",
    "    tracer = tracer.lower().title()\n",
    "    dataobject = [\"NrtGrowingDataObject\",\"L2DataObject\"]\n",
    "    \n",
    "    if type(station) == str:\n",
    "        station = [station]\n",
    "    strUrl=\" \"\n",
    "    for ist in station:\n",
    "        strUrl = strUrl + \" \" + \"\"\"<http://meta.icos-cp.eu/resources/stations/AS_\"\"\"+ist+\"\"\">\"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/>\n",
    "    prefix prov: <http://www.w3.org/ns/prov#>\n",
    "    select ?dobj ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd\n",
    "    FROM <http://meta.icos-cp.eu/resources/atmprodcsv/>\n",
    "    where {\n",
    "        BIND(<http://meta.icos-cp.eu/resources/cpmeta/atc\"\"\"+tracer+dataobject[level-1]+\"\"\"> AS ?spec)\n",
    "        ?dobj cpmeta:hasObjectSpec ?spec .\n",
    "        VALUES ?station {\"\"\"+strUrl+\"\"\"} ?dobj cpmeta:wasAcquiredBy/prov:wasAssociatedWith ?station .\n",
    "        FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}\n",
    "        ?dobj cpmeta:hasSizeInBytes ?size .\n",
    "        ?dobj cpmeta:hasName ?fileName .\n",
    "        ?dobj cpmeta:wasSubmittedBy [\n",
    "            prov:endedAtTime ?submTime ;\n",
    "            prov:wasAssociatedWith ?submitter\n",
    "        ] .\n",
    "        ?dobj cpmeta:hasStartTime | (cpmeta:wasAcquiredBy / prov:startedAtTime) ?timeStart .\n",
    "        ?dobj cpmeta:hasEndTime | (cpmeta:wasAcquiredBy / prov:endedAtTime) ?timeEnd .\n",
    "        }\n",
    "        \"\"\"\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ICOS_filename(station,tracer='co2',level=2,download=False):\n",
    "    # %load /home/ute/Stations/Claudio/atc_co2_l2.py\n",
    "    \"\"\"\n",
    "    hack the carbon portal\n",
    "    download data files directly from \n",
    "    the carbon portal. \n",
    "    Created on Thu Nov 22 17:17:27 2018\n",
    "\n",
    "    @author: Claudio\n",
    "    \"\"\"\n",
    "\n",
    "    __version__ = \"0.1.0\"\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "    import sys\n",
    "    #import sparqls\n",
    "    #import helper_functions as h\n",
    "\n",
    "    # set the list of necessary modules to run the code\n",
    "    modules = [\"os\", \"requests\", \"pandas\", \"tqdm\"]\n",
    "\n",
    "    # check if the modules are available and load them, otherwise stop execution\n",
    "    #if not h.checklib(modules):\n",
    "    if not checklib(modules):\n",
    "        sys.exit(\"module dependencies are not fulfilled\")\n",
    "\n",
    "    else:\n",
    "        import os\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "        from tqdm import tqdm\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # this is the bit, where the sparql query is sent and we expect\n",
    "    # a list of dataobject    \n",
    "    url = 'https://meta.icos-cp.eu/sparql'\n",
    "\n",
    "    r = requests.get(url, params={\n",
    "        'format': 'json',\n",
    "        'query': atc_stationlist(station,tracer=tracer,level=level)})\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # convert the the result into a table\n",
    "    # output is an array, where each row contains\n",
    "    # information about the data object\n",
    "\n",
    "    cols = data['head']['vars']\n",
    "    datatable = []\n",
    "\n",
    "    for row in data['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "\n",
    "        datatable.append(item)\n",
    "\n",
    "    # print the table if you want\n",
    "    dt = pd.DataFrame(datatable, columns=cols)\n",
    "    #print(dt.head(5))\n",
    "    #print(dt.fileName)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # download all ATC CO2 L2 files\n",
    "\n",
    "    if download:\n",
    "        print(\"download all files: \",dt.fileName)\n",
    "\n",
    "        # now loop through the results, and download the corresponding file\n",
    "        # files are download directly to the folder where this script is\n",
    "        # located. If the file does already exist it will be skipped.\n",
    "\n",
    "        for idx in dt.index:\n",
    "            if os.path.isfile(dt.fileName[idx]):\n",
    "                print(\"file already exists, skip...\"+dt.fileName[idx])\n",
    "            else:    \n",
    "                # a little hack to provide \"yes\" to the licence agreement\n",
    "                prefix = \"https://data.icos-cp.eu/licence_accept?ids=%5B%22\"\n",
    "                suffix = \"%22%5D\"\n",
    "                url = dt.dobj[idx]\n",
    "                url = url.split(\"/\")\n",
    "                url = prefix + url[4] + suffix\n",
    "        \n",
    "                # print(url)\n",
    "                response = requests.get(url, stream=True)\n",
    "                with open(dt.fileName[idx], \"wb\") as handle:\n",
    "                    for data in tqdm(response.iter_content()):\n",
    "                        handle.write(data)\n",
    "\n",
    "        print(\"all done\")\n",
    "\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ICOS_list(tracer,level=2):\n",
    "    # %load /home/ute/Stations/Claudio/atc_co2_l2.py\n",
    "    \"\"\"\n",
    "    hack the carbon portal\n",
    "    download data files directly from \n",
    "    the carbon portal. \n",
    "    Created on Thu Nov 22 17:17:27 2018\n",
    "\n",
    "    @author: Claudio\n",
    "    \"\"\"\n",
    "\n",
    "    __version__ = \"0.1.0\"\n",
    "\n",
    "    # download all ATC CO2 L2 files\n",
    "    # ---------------------------------------\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    import sys\n",
    "    #import sparqls\n",
    "    #import helper_functions as h\n",
    "\n",
    "    # set the list of necessary modules to run the code\n",
    "    modules = [\"os\", \"requests\", \"pandas\", \"tqdm\"]\n",
    "\n",
    "    # check if the modules are available and load them, otherwise stop execution\n",
    "    if not checklib(modules):\n",
    "        sys.exit(\"module dependencies are not fulfilled\")\n",
    "\n",
    "    else:\n",
    "        import os\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "        from tqdm import tqdm\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # this is the bit, where the sparql query is sent and we expect\n",
    "    # a list of dataobject    \n",
    "    url = 'https://meta.icos-cp.eu/sparql'\n",
    "    r = requests.get(url, params={\n",
    "        'format': 'json',\n",
    "        'query': atc_query(tracer,level)})\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # convert the the result into a table\n",
    "    # output is an array, where each row contains\n",
    "    # information about the data object\n",
    "\n",
    "    cols = data['head']['vars']\n",
    "    datatable = []\n",
    "\n",
    "    for row in data['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "\n",
    "        datatable.append(item)\n",
    "\n",
    "    # print the table if you want\n",
    "    dt = pd.DataFrame(datatable, columns=cols)\n",
    "    #print(dt.head(5))\n",
    "\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### outdated modules for reading ICOS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Markdown, display# Function that takes a 4-digit integer as input (representing a year) and returns an integer with 2-digits (representing a year with its last 2 digits - omitting millenium and century).\n",
    "def func_YYYY_to_YY(yyyy):\n",
    "    yy = abs(int(yyyy))%100\n",
    "    \n",
    "    return yy\n",
    "\n",
    "\n",
    "\n",
    "# Function that takes 5 string parameters as input -representing year (YYYY), month (MM), day (DD), hour (HH) and minute (HH)- and returns a formatted string object ('YY/MM/DD HH:MM'):\n",
    "def func_date_format_str(year, month, day, hour, minute):\n",
    "    \n",
    "    #Create the formatted string:\n",
    "    yy_mm_dd_hh_mm = year + '/' + month + '/' + day + ' ' + hour + ':' + minute\n",
    "    \n",
    "    #Return string\n",
    "    return yy_mm_dd_hh_mm\n",
    "\n",
    "\n",
    "\n",
    "# Function that takes a dataframe containing date-columns as input- and returns a list of datetime objects:\n",
    "def func_list_datetimeObj(df_data):\n",
    "    \n",
    "    #Call function to make every year-value appear only with its last 2 digits and save the results in a list:\n",
    "    yy_list = list(map(func_YYYY_to_YY, df_data['Year']))\n",
    "    \n",
    "    #Create the list that will store the formatted date strings:\n",
    "    lst_datetimeObj = []\n",
    "    \n",
    "    #Loop that calls the date string-formatting function for every record in the dataframe.\n",
    "    #The string-formatting function results are then used as inputs to the datetime-function, that creates a datetime object for every call.\n",
    "    #All datetime objects are then stored in a list.\n",
    "    for i in range(len(df_data)):\n",
    "        lst_datetimeObj.append(dt.datetime.strptime(func_date_format_str(str(yy_list[i]), df_data['Month'][i], df_data['Day'][i], df_data['Hour'][i], df_data['Minute'][i]),'%y/%m/%d %H:%M'))\n",
    "    \n",
    "    #Return list with datetime object:\n",
    "    return lst_datetimeObj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ICOS_atm_downloaded(filename):\n",
    "    \n",
    "    #test if zip file or data file \n",
    "    if filename.lower().endswith('.zip'):\n",
    "        \n",
    "        #Import zip file and print the files stored in it:\n",
    "        zf = zipfile.ZipFile(filename, 'r')\n",
    "        print ('Zipfile: ',filename)\n",
    "        #print (zf.namelist())\n",
    "        # Extract archived files from the ZIP archive:\n",
    "        for filename in zf.namelist():\n",
    "            try:\n",
    "                data = zf.read(filename)\n",
    "            except KeyError:\n",
    "                print('ERROR: Did not find %s in zip file' % filename)\n",
    "            else:\n",
    "                print('Filename: ', filename)\n",
    "                #print (data)\n",
    "    else:\n",
    "        print('Filename: ', filename)\n",
    "        af = open(filename, 'r')\n",
    "        data = af.read()\n",
    "    \n",
    "    #Split the string whenever the '#' is found, to separate the Metadata headers from the Measurement data:\n",
    "    #data_split = data.split('#')\n",
    "    \n",
    "    #updated:\n",
    "    data_split = data.decode().split('#')\n",
    "\n",
    "    #Display the list of the splitted data:\n",
    "    #data_split\n",
    "    \n",
    "    #The following list item contains the Measurement data header and the Measurement data:\n",
    "    #data_split[39]\n",
    "    #Create a new list that will store the Measurment data and its header as text:\n",
    "    data_txt = data_split.pop(39)\n",
    "\n",
    "    #Display list:\n",
    "    ##data_txt\n",
    "    \n",
    "    #----------Format the measurement txt data---------------\n",
    "    #Split the text data at every '\\n' to get the rows:\n",
    "    data_txt_split = data_txt.split('\\n')\n",
    "\n",
    "    #Create a list that will store the rows of the measurement data\n",
    "    measurement_data = []\n",
    "\n",
    "    #Loop that splits the data for every measurement (row):\n",
    "    measurement_data = list(map(methodcaller(\"split\", \";\"), data_txt_split))\n",
    "\n",
    "    #Display the content of the splitted Measurement data list:\n",
    "    #measurement_data\n",
    "\n",
    "    #Remove the first list item that contains the Measurement data headers:\n",
    "    data_labels = measurement_data.pop(0)\n",
    "\n",
    "    #Display the Measurement data headers:\n",
    "    #data_labels\n",
    "\n",
    "    #Create a list for every variable in a measurement.\n",
    "    #Every list will store the value of its corresponding variable for all measurements.\n",
    "    site_list = []            #0\n",
    "    samplingHeight_list = []  #1\n",
    "    year_list = []            #2\n",
    "    month_list = []           #3\n",
    "    day_list = []             #4\n",
    "    hour_list = []            #5\n",
    "    minute_list = []          #6\n",
    "    decimalDate_list = []     #7\n",
    "    co2_list = []             #8\n",
    "    stdev_list = []           #9\n",
    "    nbPoints_list = []        #10\n",
    "    flag_list = []            #11\n",
    "    instrumentID_list = []    #12\n",
    "    qualityID_list = []       #13\n",
    "    LTR_list = []             #14\n",
    "    CMR_list = []             #15\n",
    "    STTB_list = []            #16\n",
    "\n",
    "    #Loop that will populate the variable lists with data:\n",
    "    for var_row in range(len(measurement_data)):\n",
    "        for var_value in range(len(measurement_data[var_row])):\n",
    "            if var_value == 0:\n",
    "                site_list.append(measurement_data[var_row][var_value])\n",
    "                #print ('1st condition st, ', 'var_row: ', var_row, ', var_value: ', var_value, ', row_data: ', measurement_data[var_row], ', measurement_data: ', measurement_data[var_row][var_value])\n",
    "            elif var_value == 1:\n",
    "                samplingHeight_list.append(float(measurement_data[var_row][var_value])) #Convert string to float\n",
    "                #print ('2nd condition st, ', 'var_row: ', var_row, ', var_value: ', var_value, ', row_data: ', measurement_data[var_row], ', measurement_data: ', measurement_data[var_row][var_value])\n",
    "            elif var_value == 2:\n",
    "                year_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 3:\n",
    "                month_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 4:\n",
    "                day_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 5:\n",
    "                hour_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 6:\n",
    "                minute_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 7:\n",
    "                decimalDate_list.append(float(measurement_data[var_row][var_value])) #Convert string to float\n",
    "            elif var_value == 8:\n",
    "                co2_list.append(float(measurement_data[var_row][var_value])) #Convert string to float\n",
    "            elif var_value == 9:\n",
    "                stdev_list.append(float(measurement_data[var_row][var_value])) #Convert string to float\n",
    "            elif var_value == 10:\n",
    "                nbPoints_list.append(int(measurement_data[var_row][var_value]))\n",
    "            elif var_value == 11:\n",
    "                flag_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 12:\n",
    "                instrumentID_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 13:\n",
    "                qualityID_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 14:\n",
    "                LTR_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 15:\n",
    "                CMR_list.append(measurement_data[var_row][var_value])\n",
    "            elif var_value == 16:\n",
    "                STTB_list.append(measurement_data[var_row][var_value])\n",
    "            else:\n",
    "                print ('Exception!!!\\nNumber of categories exceed the hardcoded number of categories.')\n",
    "\n",
    "    #Create a dictionary containing the variable lists:\n",
    "    d_data = {'Site':site_list,\n",
    "         'SamplingHeight':samplingHeight_list,\n",
    "         'Year':year_list,\n",
    "         'Month':month_list,\n",
    "         'Day':day_list,\n",
    "         'Hour':hour_list,\n",
    "         'Minute':minute_list,\n",
    "         'DecimalDate':decimalDate_list,\n",
    "         'CO2':co2_list,\n",
    "         'Stdev':stdev_list,\n",
    "         'NbPoints':nbPoints_list,\n",
    "         'Flag':flag_list,\n",
    "         'InstrumentID':instrumentID_list,\n",
    "         'QualityID':qualityID_list,\n",
    "         'LTR':LTR_list,\n",
    "         'CMR':CMR_list,\n",
    "         'STTB':STTB_list\n",
    "        }\n",
    "\n",
    "    #Create a pandas data frame from the dictionary:\n",
    "    #df_data = pd.DataFrame(d_data)\n",
    "\n",
    "    #Python pandas order the columns in an alphabetical order.\n",
    "    #If you wish to keep the order of the columns as it was in the initial txt file, run the code bellow:\n",
    "    df_data = pd.DataFrame(d_data,columns=['Site', 'SamplingHeight', 'Year', 'Month', 'Day', 'Hour', 'Minute', 'DecimalDate', 'CO2', 'Stdev', 'NbPoints', 'Flag', 'InstrumentID', 'QualityID', 'LTR', 'CMR', 'STTB'])\n",
    "\n",
    "    #Display dataframe:\n",
    "    #df_data\n",
    "\n",
    "    #Display the remaining contents of the list (only metadata headers are left after the split): \n",
    "    #data_split\n",
    "\n",
    "    ############################################# Create a function of this!\n",
    "    #Remove 1st and last item from the list:\n",
    "    del data_split[0], data_split[-1]\n",
    "\n",
    "    #Show the contents of the list after the removal of the 1st and last list item:\n",
    "    #data_split\n",
    "\n",
    "    #Create a list to store the extracted metadata:\n",
    "    data_split_repl = []\n",
    "\n",
    "    #Loop that removes the first empty-space from the left in a string as well as the occurence of \"\\n\":\n",
    "    for i in range(len(data_split)):\n",
    "        data_split_repl.append(data_split[i].lstrip().replace('\\n', ''))\n",
    "\n",
    "    #Display list\n",
    "    #data_split_repl\n",
    "    \n",
    "    #In the metadata text, separate labels from information\n",
    "    #Create list to store formatted data that separates the metadata labels from their corresponding metadata info:\n",
    "    label_info = []\n",
    "    label_list = []\n",
    "    info_list = []\n",
    "    comment_list = []\n",
    "\n",
    "    #Loop that splits every list item to \"label-part\" and \"info-part\":\n",
    "    for lbl_info_row in range(len(data_split_repl)):\n",
    "        label_info.append(data_split_repl[lbl_info_row].split(': '))\n",
    "    \n",
    "        #Store every list-item's \"label-part\" in the label_list and every \"info_part\" in the info_list:\n",
    "        if(lbl_info_row < 28):\n",
    "            label_list.append(label_info[lbl_info_row][0])\n",
    "            info_list.append(label_info[lbl_info_row][1])\n",
    "        #Handle the special case of the \"comment-label\":\n",
    "        elif(lbl_info_row == 28):\n",
    "            label_list.append(label_info[lbl_info_row][0].replace(':', ''))\n",
    "        else:\n",
    "            #Store the comment-label's info data in the comment_list:\n",
    "            comment_list.append(data_split_repl[lbl_info_row])\n",
    "\n",
    "    #Add the comment_list to the info_list:\n",
    "    info_list.append(comment_list)\n",
    "\n",
    "    #Add the data_columns list to the info_list:\n",
    "    info_list.append(data_labels)\n",
    "\n",
    "    #Add a label for the data-columns in the label_list:\n",
    "    label_list.append('DATA COLUMNS')\n",
    "\n",
    "    ####CONTROL RESULTS####\n",
    "    #print (label_list, '\\n')\n",
    "    #print (info_list)\n",
    "    \n",
    "    #Create a dictionary containing the metadata-label & metadata-info lists:\n",
    "    d_metadata = {'MetadataLabel':label_list,\n",
    "                  'MetadataInfo':info_list,\n",
    "                 }\n",
    "\n",
    "    #Create a pandas data frame from the dictionary:\n",
    "    #df_metadata = pd.DataFrame(d_metadata)\n",
    "\n",
    "    #Python pandas order the columns in an alphabetical order.\n",
    "    #If you wish to keep the order of the columns as it was in the initial txt file, run the code bellow:\n",
    "    df_metadata = pd.DataFrame(d_metadata,columns=['MetadataLabel', 'MetadataInfo'])\n",
    "\n",
    "    #Display dataframe:\n",
    "    #df_metadata\n",
    "    \n",
    "    # Convert CO2 missing values (assigned the value -999.990 or -999.999) to NaN\n",
    "    mask_co2 = df_data.CO2 < 0\n",
    "    column_name_co2 = 'CO2'\n",
    "    df_data.loc[mask_co2, column_name_co2] = nan\n",
    "\n",
    "    # Convert Stdev-values of CO2 missing values (assigned the value -9.990 or -9.999) to NaN\n",
    "    mask_stdev = df_data.Stdev < 0\n",
    "    column_name_stdev = 'Stdev'\n",
    "    df_data.loc[mask_stdev, column_name_stdev] = nan\n",
    "\n",
    "    #Display updated df_data:\n",
    "    #df_data\n",
    "\n",
    "    #df_data.loc[df_data['Stdev']<0] \n",
    "    #Call function to add a datetime obj to the data dataframe:\n",
    "    datetime_obj_list = func_list_datetimeObj(df_data)    \n",
    "    #Add list with datetime objects to the data dataframe:\n",
    "    df_data['DateTime'] =  datetime_obj_list\n",
    "\n",
    "    #df_data\n",
    "    \n",
    "    return df_data, df_metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
