{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook with a collection of functions for processing ICOS measurement time series and STILT results\n",
    "\n",
    "### Import this notebook with \n",
    "### %run '~/modules/STILT_modules_plus.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tools and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import sys   \n",
    "import netCDF4 as cdf\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import datetime as dt\n",
    "from datetime import datetime#, timedelta, date, time\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import fnmatch\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from operator import methodcaller\n",
    "import matplotlib.pyplot as p\n",
    "import matplotlib.colors as mcolors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define colors\n",
    "orange='#ff8c00'\n",
    "lime='#00ff00'\n",
    "aqua='#00ffff'\n",
    "brown='#663300'\n",
    "lightgray=\"#C0C0C0\"\n",
    "gray=\"#808080\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of ICOS class 1 and class 2 stations from Carbon Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_class():\n",
    "    # Query the ICOS SPARQL endpoint for a station list\n",
    "    # query stationId, class, lng name and country\n",
    "    # output is an object \"data\" containing the results in JSON\n",
    "\n",
    "    url = 'https://meta.icos-cp.eu/sparql'\n",
    "\n",
    "    query = \"\"\"\n",
    "    prefix st: <http://meta.icos-cp.eu/ontologies/stationentry/>\n",
    "    select distinct ?stationId ?stationClass ?country ?longName\n",
    "    from <http://meta.icos-cp.eu/resources/stationentry/>\n",
    "    where{\n",
    "      ?s a st:AS .\n",
    "      ?s st:hasShortName ?stationId .\n",
    "      ?s st:hasStationClass ?stationClass .\n",
    "      ?s st:hasCountry ?country .\n",
    "      ?s st:hasLongName ?longName .\n",
    "      filter (?stationClass = \"1\" || ?stationClass = \"2\")\n",
    "    }\n",
    "    ORDER BY ?stationClass ?stationId \n",
    "    \"\"\"\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    data = r.json()\n",
    "\n",
    "    # convert the the result into a table\n",
    "    # output is an array, where each row contains \n",
    "    # information about the station\n",
    "\n",
    "    cols = data['head']['vars']\n",
    "    datatable = []\n",
    "\n",
    "    for row in data['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        \n",
    "        datatable.append(item)\n",
    "\n",
    "    # print the table \n",
    "    df_datatable = pd.DataFrame(datatable, columns=cols)\n",
    "    #df_datatable.head(5)\n",
    "    return df_datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all STILT station information in a dictionary\n",
    "Dictionary contains information on\n",
    "- STILT station id\n",
    "- Station coordinates (latitude, longitude)\n",
    "- Altitude of tracer release in STILT simultation\n",
    "- STILT location identifier\n",
    "- Station name - if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_STILT_dictionary(path_tmp):\n",
    "    # store all STILT station information in a dictionary \n",
    "\n",
    "    # get all ICOS station IDs by listing subdirectories in stiltweb\n",
    "    # extract location from filename of link\n",
    "\n",
    "    pathStations='/data/stiltweb/stations/'\n",
    "    allStations = os.listdir(pathStations)\n",
    "\n",
    "    # empty dictionary\n",
    "    stations = {}\n",
    "\n",
    "    # fill dictionary with ICOS station id, latitude, longitude and altitude\n",
    "    for ist in sorted(list(set(allStations))):\n",
    "        stations[ist] = {}\n",
    "        # get filename of link (original stiltweb directory structure) \n",
    "        # and extract location information\n",
    "        if os.path.exists(pathStations+ist):\n",
    "            loc_ident = os.readlink(pathStations+ist)\n",
    "            clon = loc_ident[-13:-6]\n",
    "            lon = np.float(clon[:-1])\n",
    "            if clon[-1:] == 'W':\n",
    "                lon = -lon\n",
    "            clat = loc_ident[-20:-14]\n",
    "            lat = np.float(clat[:-1])\n",
    "            if clat[-1:] == 'S':\n",
    "                lat = -lat\n",
    "            alt = np.int(loc_ident[-5:])\n",
    "\n",
    "            stations[ist]['lat']=lat\n",
    "            stations[ist]['lon']=lon\n",
    "            stations[ist]['alt']=alt\n",
    "            stations[ist]['locIdent']=os.path.split(loc_ident)[-1]\n",
    "\n",
    "        \n",
    "    # add information on station name (and new STILT station id) from stations.csv file used in stiltweb \n",
    "\n",
    "    url=\"https://stilt.icos-cp.eu/viewer/stationinfo\"\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    for ist in sorted(list(set(stations))):\n",
    "        stationName = df.loc[df['STILT id'] == ist]['STILT name']\n",
    "        if len(stationName.value_counts()) > 0:\n",
    "            stations[ist]['name'] = stationName.item()\n",
    "        else:\n",
    "            stations[ist]['name'] = ''\n",
    "\n",
    "    # Get list of ICOS class 1 and class 2 stations from Carbon Portal\n",
    "    df_datatable = get_station_class()\n",
    "\n",
    "    # add information if ICOS class 1 or class 2 site\n",
    "    for ist in sorted(list(set(stations))):\n",
    "        stations[ist]['stationClass'] = np.nan\n",
    "        for istICOS in df_datatable['stationId']:\n",
    "            ic = int(df_datatable[df_datatable['stationId']==istICOS].index.values)\n",
    "            if istICOS in ist:\n",
    "                stations[ist]['stationClass'] = df_datatable['stationClass'][ic]\n",
    "\n",
    "    # print dictionary\n",
    "    #for ist in sorted(stations):\n",
    "    #    print ('station:', ist)\n",
    "    #    for k in stations[ist]:\n",
    "    #        print (k,':', stations[ist][k])\n",
    "\n",
    "    # write dictionary to pickle file for further use\n",
    "    if not os.path.exists(path_tmp):\n",
    "        os.makedirs(path_tmp, exist_ok=True)\n",
    "    pickle.dump( stations, open( path_tmp+\"/stationsDict.pickle\", \"wb\" ) )\n",
    "\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dictionary with all stations\n",
    "Dictionary contains information on\n",
    "- STILT station id\n",
    "- Station coordinates (latitude, longitude)\n",
    "- Altitude of tracer release in STILT simultation\n",
    "- STILT location identifier\n",
    "- Station name - if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_STILT_dictionary(path_tmp):\n",
    "    # read STILT station dictionary from pickle file\n",
    "\n",
    "    filename = 'stationsDict.pickle'\n",
    "    if not os.path.isfile(path_tmp+filename):\n",
    "        stations = create_STILT_dictionary(path_tmp)\n",
    "    else:\n",
    "        stations = pd.read_pickle(path_tmp+filename)\n",
    "\n",
    "    # print dictionary\n",
    "    for ist in sorted(stations):\n",
    "        print ('station:', ist)\n",
    "        for k in stations[ist]:\n",
    "            print (k,':', stations[ist][k])\n",
    "\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available footprints and store them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_STILT_dictionary():\n",
    "    # store availability of STILT footprints in a dictionary \n",
    "\n",
    "    # get all ICOS station IDs by listing subdirectories in stiltweb\n",
    "    # extract availability from directory structure\n",
    "\n",
    "    pathStations='/data/stiltweb/stations/'\n",
    "    allStations = os.listdir(pathStations)\n",
    "\n",
    "    # empty dictionary\n",
    "    available = {}\n",
    "\n",
    "    # fill dictionary with station name, years and months for each year\n",
    "    for ist in sorted(list(set(allStations))):\n",
    "        if os.path.exists(pathStations+'/'+ist):\n",
    "            #print ('directory '+pathStations+'/'+ist+' exits')\n",
    "            available[ist] = {}\n",
    "            years = os.listdir(pathStations+'/'+ist)\n",
    "            available[ist]['years'] = years\n",
    "            for yy in sorted(available[ist]['years']):\n",
    "                available[ist][yy] = {}\n",
    "                months = os.listdir(pathStations+'/'+ist+'/'+yy)\n",
    "                available[ist][yy]['months'] = months\n",
    "                available[ist][yy]['nmonths'] = len(available[ist][yy]['months'])\n",
    "        #else:\n",
    "        #    print ('directory '+pathStations+'/'+ist+' does not exit')\n",
    "\n",
    "    # Get list of ICOS class 1 and class 2 stations from Carbon Portal\n",
    "    df_datatable = get_station_class()\n",
    "\n",
    "    # add information if ICOS class 1 or class 2 site\n",
    "    for ist in sorted(available):\n",
    "        available[ist]['stationClass'] = np.nan\n",
    "        for istICOS in df_datatable['stationId']:\n",
    "            ic = int(df_datatable[df_datatable['stationId']==istICOS].index.values)\n",
    "            if istICOS in ist:\n",
    "                available[ist]['stationClass'] = df_datatable['stationClass'][ic]\n",
    "    # find latest year with STILT results\n",
    "    yymax = (max([max(available[ist]['years']) for ist in available]))\n",
    "\n",
    "    # print availability\n",
    "    #for ist in sorted(available):\n",
    "    #    print ('station:', ist)\n",
    "    #    for k in available[ist]:\n",
    "    #        print (k,':', available[ist][k])\n",
    "    return available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot STILT footprint availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_available_STILT(pngfile=''):\n",
    "    \n",
    "    print ('run available_STILT_dictionary()')\n",
    "    available = available_STILT_dictionary()\n",
    "    \n",
    "    # Plot availability\n",
    "    # Each dot in the figure below represents one year. \n",
    "    # The size of the dot is proportional to the number of months per year for which footprints are available. \n",
    "\n",
    "    startyear = 2006\n",
    "    endyear = int(max([max(available[ist]['years']) for ist in available]))\n",
    "\n",
    "    ny = endyear - startyear + 1\n",
    "    yy = np.arange(ny) + startyear\n",
    "    nm = np.zeros(ny)\n",
    "    dy = 0.5\n",
    "\n",
    "    fig = p.figure(figsize=(15, 32))\n",
    "    for i, ist in enumerate(sorted(available, reverse=True)) :\n",
    "        # available number of months per available year\n",
    "        nm = [available[ist][str(yy[j])]['nmonths'] if str(yy[j]) in available[ist].keys() else 0 for j in np.arange(ny)]\n",
    "        if available[ist]['stationClass'] == '1':\n",
    "            x = p.scatter(yy, np.ones(np.size(yy))*i+dy,c='r',marker='D', s=30*np.sqrt(np.asarray(nm)))\n",
    "            p.text(startyear-2+0.2, i+dy/2, ist, color='r', fontsize=14)         \n",
    "        elif available[ist]['stationClass'] == '2':\n",
    "            x = p.scatter(yy, np.ones(np.size(yy))*i+dy,c='b', marker='^', s=40*np.sqrt(np.asarray(nm)))\n",
    "            p.text(startyear-2+0.2, i+dy/2, ist, color='b', fontsize=14)\n",
    "        else:\n",
    "            x = p.scatter(yy, np.ones(np.size(yy))*i+dy,c='k',s=40*np.sqrt(np.asarray(nm)))\n",
    "            p.text(startyear-2+0.2, i+dy/2, ist, fontsize=14)         \n",
    "        \n",
    "    p.xticks(np.arange(startyear-2, np.max(yy)+2, 1.0))\n",
    "    p.xlim(startyear-2, np.max(yy)+1)\n",
    "    p.yticks(np.arange(0, len(available), 1.0), ())\n",
    "    p.ylim(0, len(available))\n",
    "    p.grid(axis='y')\n",
    "    p.tick_params(labeltop=True,labelsize=14)\n",
    "    p.title('Available STILT footprints (size proportional to number of months per year)\\n\\n\\n', fontsize=18)\n",
    "    p.figtext(0.4, 0.9, 'ICOS class 1 stations in red', color='r', fontsize=16, ha ='right')\n",
    "    p.figtext(0.6, 0.9, 'ICOS class 2 stations in blue', color='b', fontsize=16, ha ='left')\n",
    "    p.show()\n",
    "    p.close()\n",
    "    if len(pngfile)>0:\n",
    "        plotdir='plots'\n",
    "        if not os.path.exists(plotdir):\n",
    "            os.mkdir(plotdir)\n",
    "        fig.savefig(plotdir+'/'+pngfile+'_'+dt.datetime.now().strftime('%Y%m%d')+'.png',dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert station longitude and latitude to STILT grid indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert station longitude and latitude (slat, slon) to indices of STILT model grid (ix,jy)\n",
    "def lonlat_2_ixjy(slon,slat,mlon,mlat):\n",
    "    #slon, slat: longitude and latitude of station\n",
    "    #mlon, mlat: 1-dim. longitude and latitude of model grid\n",
    "    ix = (np.abs(mlon-slon)).argmin()\n",
    "    jy = (np.abs(mlat-slat)).argmin()\n",
    "    return ix,jy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read STILT time series (new format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read STILT concentration time series (new format of STILT results)\n",
    "def read_stilt_timeseries(station,date_range):\n",
    "    url = 'https://stilt.icos-cp.eu/viewer/stiltresult'\n",
    "    headers = {'Content-Type': 'application/json', 'Accept-Charset': 'UTF-8'}\n",
    "    # check if STILT results exist\n",
    "    new_range=[]\n",
    "    for zDate in date_range:\n",
    "        if os.path.exists('/data/stiltweb/slots/'+stations[station]['locIdent']+'/'+str(zDate.year)+'/'+str(zDate.month).zfill(2)+'/'\n",
    "                    +str(zDate.year)+'x'+str(zDate.month).zfill(2)+'x'+str(zDate.day).zfill(2)+'x'+str(zDate.hour).zfill(2)+'/'):\n",
    "            new_range.append(zDate)\n",
    "    if len(new_range) > 0:\n",
    "        date_range = new_range\n",
    "        fromDate = date_range[0].strftime('%Y-%m-%d')\n",
    "        toDate = date_range[-1].strftime('%Y-%m-%d')\n",
    "        columns = ('[\"isodate\",\"co2.stilt\",\"co2.fuel\",\"co2.bio\",\"co2.fuel.coal\",\"co2.fuel.oil\",'+\n",
    "                   '\"co2.fuel.gas\",\"co2.fuel.bio\",\"co2.energy\",\"co2.transport\", \"co2.industry\",'+\n",
    "                   '\"co2.others\", \"co2.cement\", \"co2.background\",'+\n",
    "                   '\"co.stilt\",\"co.fuel\",\"co.bio\",\"co.fuel.coal\",\"co.fuel.oil\",'+\n",
    "                   '\"co.fuel.gas\",\"co.fuel.bio\",\"co.energy\",\"co.transport\", \"co.industry\",'+\n",
    "                   '\"co.others\", \"co.cement\", \"co.background\",'+\n",
    "                   '\"rn\", \"rn.era\",\"rn.noah\",\"wind.dir\",\"wind.u\",\"wind.v\",\"latstart\",\"lonstart\"]')\n",
    "        data = '{\"columns\": '+columns+', \"fromDate\": \"'+fromDate+'\", \"toDate\": \"'+toDate+'\", \"stationId\": \"'+station+'\"}'\n",
    "        #print (data)\n",
    "        response = requests.post(url, headers=headers, data=data)\n",
    "        if response.status_code != 500:\n",
    "            #print (response.json())\n",
    "            output=np.asarray(response.json())\n",
    "            df = pd.DataFrame(output[:,:], columns=eval(columns))\n",
    "            df = df.replace('null',np.NaN)\n",
    "            df = df.astype(float)\n",
    "            df['date'] = pd.to_datetime(df['isodate'], unit='s')\n",
    "            df.set_index(['date'],inplace=True)\n",
    "            df['name'] = station\n",
    "            df['model'] = 'STILT'\n",
    "            df['wind.speed']=np.sqrt((df['wind.u']**2)+(df['wind.v']**2))\n",
    "            #print (df.columns)\n",
    "    else:\n",
    "        df=pd.DataFrame({'A' : []})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates and adds a column with datetime objects in a dataframe:\n",
    "def createDatetimeObjList(df_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Mon Nov 05 10:27:00 2019\n",
    "    Last Changed:     Mon Nov 05 10:27:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Add a column with datetime objects to the input dataframe. The datetime objects are created \n",
    "                      based on the content of the 'Year', 'Month', 'Day', 'Hour' and 'Minute' columns of every row.\n",
    "    Input parameters: ICOS data pandas dataframe (Atmospheric Level-1 or Level-2 Data)\n",
    "    Output:           pandas dataframe\n",
    "                      columns: \n",
    "                            1.  Station 3-character Code (var_name: 'Site', var_type: String)\n",
    "                            2.  Sampling Height (var_name: 'SamplingHeight', var_type: String)\n",
    "                            3.  Sampling Year (var_name: 'Year', var_type: String)\n",
    "                            4.  Sampling Month (var_name: 'Month', var_type: String)\n",
    "                            5.  Sampling Day (var_name: 'Day', var_type: String)\n",
    "                            6.  Sampling Hour (var_name: 'Hour', var_type: String)\n",
    "                            7.  Sampling Minute (var_name: 'Minute', var_type: String)\n",
    "                            8.  Sampling Decimal Date (var_name: 'DecimalDate', var_type: String)\n",
    "                            9.  Tracer/Gas concentration (var_name: 'ch4' or 'co2' or 'co', var_type: String)\n",
    "                            10. Standard Deviation (var_name: 'Stdev', var_type: String)\n",
    "                            11. Number of Points used for the measurment(var_name: 'NbPoints', var_type: String)\n",
    "                            12. Quality Flag (var_name: 'Flag', var_type: String)\n",
    "                            13. Instrument ID (var_name: 'InstrumentId', var_type: String)\n",
    "                            14. Quality ID (var_name: 'QualityId', var_type: String)\n",
    "                            15. Internal Flag: only for Level-1 data\n",
    "                                (var_name: 'InternalFlag', var_type: String)\n",
    "                            15. LTR: only for Level-2 data (var_name: 'LTR', var_type: String)\n",
    "                            16. Auto-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'AutoDescriptiveFlag', var_type: String)\n",
    "                            16. CMR: only for Level-2 data (var_name: 'CMR', var_type: String)\n",
    "                            17. Manual-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'ManualDescriptiveFlag', var_type: String)\n",
    "                            17: STTB: only for Level-2 data (var_name: 'STTB', var_type: String)\n",
    "                            18: Datetime object (var_name: 'DateTime', var_type: Stri)\n",
    "                            \n",
    "    \"\"\"\n",
    "    \n",
    "    #Create a list with datetime obj:\n",
    "    datetimeObj_list = [datetime.strptime((df_data['Year'][i]+\"/\"+\n",
    "                                           df_data['Month'][i]+\"/\"+\n",
    "                                           df_data['Day'][i]+\" \"+\n",
    "                                           df_data['Hour'][i] +\":\"+\n",
    "                                           df_data['Minute'][i]),'%Y/%m/%d %H:%M')\n",
    "                        for i in range(len(df_data))]\n",
    "    \n",
    "    #Add list with datetime objects to the data dataframe:\n",
    "    df_data['DateTime'] =  datetimeObj_list\n",
    "    \n",
    "    #Return dataframe:\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates a dataframe with the data values:\n",
    "def icosDatadf(data, tracer, level=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Mon Oct 08 09:27:00 2019\n",
    "    Last Changed:     Mon Oct 08 09:27:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Read in observations from ICOS Atmosphere Data File (Level-1 or Level-2) to a pandas dataframe.\n",
    "    \n",
    "    Input parameters: 1. Text containing column names and all observations (var_name: 'data', var_type: String)\n",
    "                      2. Name of gas/tracer (var_name: 'tracer', var_type: String) - e.g. 'co2' or 'co' or 'ch4'\n",
    "                      3. Data level [optional] (var_name: 'level', var_type: Integer)\n",
    "    \n",
    "    Default value for level: The default value for data level is \"2\". Function calls for Level-2 data do not have\n",
    "                             to include a value for the level input parameter.\n",
    "    \n",
    "    Output:           pandas dataframe\n",
    "    \n",
    "                      columns: \n",
    "                            1.  Station 3-character Code (var_name: 'Site', var_type: String)\n",
    "                            2.  Sampling Height (var_name: 'SamplingHeight', var_type: String)\n",
    "                            3.  Sampling Year (var_name: 'Year', var_type: String)\n",
    "                            4.  Sampling Month (var_name: 'Month', var_type: String)\n",
    "                            5.  Sampling Day (var_name: 'Day', var_type: String)\n",
    "                            6.  Sampling Hour (var_name: 'Hour', var_type: String)\n",
    "                            7.  Sampling Minute (var_name: 'Minute', var_type: String)\n",
    "                            8.  Sampling Decimal Date (var_name: 'DecimalDate', var_type: String)\n",
    "                            9.  Tracer/Gas concentration (var_name: 'ch4' or 'co2' or 'co', var_type: String)\n",
    "                            10. Standard Deviation (var_name: 'Stdev', var_type: String)\n",
    "                            11. Number of Points used for the measurment(var_name: 'NbPoints', var_type: String)\n",
    "                            12. Quality Flag (var_name: 'Flag', var_type: String)\n",
    "                            13. Instrument ID (var_name: 'InstrumentId', var_type: String)\n",
    "                            14. Quality ID (var_name: 'QualityId', var_type: String)\n",
    "                            15. Internal Flag: only for Level-1 data\n",
    "                                (var_name: 'InternalFlag', var_type: String)\n",
    "                            15. LTR: only for Level-2 data (var_name: 'LTR', var_type: String)\n",
    "                            16. Auto-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'AutoDescriptiveFlag', var_type: String)\n",
    "                            16. CMR: only for Level-2 data (var_name: 'CMR', var_type: String)\n",
    "                            17. Manual-Descriptive Flag: only for Level-1 data\n",
    "                                (var_name: 'ManualDescriptiveFlag', var_type: String)\n",
    "                            17: STTB: only for Level-2 data (var_name: 'STTB', var_type: String)\n",
    "                            18: Datetime object (var_name: 'DateTime', var_type: Stri)\n",
    "                            \n",
    "    \"\"\"\n",
    "    \n",
    "    #Split data to rows:\n",
    "    data_rows = data.split('\\n')\n",
    "    \n",
    "    #Remove the first row (contains the column names)\n",
    "    #split its contents and add them to a list:\n",
    "    data_labels = data_rows.pop(0).split(';')\n",
    "    \n",
    "    #Sepparate the columns of the remaining rows:\n",
    "    d_rows_list = [data_rows[x].split(';') for x in range(len(data_rows))] \n",
    "    \n",
    "    #Create dataframe:\n",
    "    df_data = pd.DataFrame.from_records(d_rows_list, columns=data_labels)\n",
    "    \n",
    "    #Add column with DateTime objects:\n",
    "    df_data = createDatetimeObjList(df_data) \n",
    "    \n",
    "    #Check what Level the data belong to:\n",
    "    if(level==2):\n",
    "        \n",
    "        #Filter values based on flag 'O', which corresponds to quality controlled data:\n",
    "        df_data = df_data.loc[df_data['Flag']=='O']\n",
    "    \n",
    "    else:\n",
    "        #Set missing values (e.g. \"-999.99\") to NaN:\n",
    "        df_data.loc[df_data[tracer].astype(np.float16)<0, tracer] = np.nan\n",
    "        df_data.loc[df_data['Stdev'].astype(np.float16)<0, 'Stdev'] = np.nan\n",
    "    \n",
    "    #Return Data dataframe:\n",
    "    return df_data.set_index('DateTime') #set column 'DateTime' as index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates a dataframe with the metadata values:\n",
    "def icosMetadatadf(data):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Mon Oct 08 10:27:00 2019\n",
    "    Last Changed:     Mon Oct 08 10:27:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Read in metadata from ICOS Atmosphere Data File (Level-1 or Level-2) to a pandas dataframe.\n",
    "    \n",
    "    Input parameters: Text containing column names and all observations (var_name: 'data', var_type: String)\n",
    "    \n",
    "    Output:           pandas dataframe\n",
    "    \n",
    "                      columns: \n",
    "                            1. MetadataLabel (var_name: 'MetadataLabel', var_type: String)\n",
    "                                i.       Dataset Title (var_name: 'TITLE', var_type: String)\n",
    "                                ii.      Dataset File Name (var_name: 'FILE NAME', var_type: String)\n",
    "                                iii.     Data Format (var_name: 'DATA FORMAT', var_type: String)\n",
    "                                iv.      Total num of records (var_name: 'TOTAL LINES', var_type: String)\n",
    "                                v.       Metadata Header Lines (var_name: 'HEADER LINES', var_type: String)\n",
    "                                vi.      Project Data Version (var_name: 'PROJECT DATA VERSION', var_type: String)\n",
    "                                vii.     Data Product Type (var_name: 'DATA PRODUCT TYPE', var_type: String)\n",
    "                                viii.    Station 3-character Code (var_name: 'STATION CODE', var_type: String)\n",
    "                                ix.      Station Full Name (var_name: 'STATION NAME', var_type: String)\n",
    "                                x.       Station Category (var_name: 'STATION CATEGORY', var_type: String)\n",
    "                                xi.      Observation Category (var_name: 'OBSERVATION CATEGORY', var_type: String)\n",
    "                                xii.     Country (var_name: 'COUNTRY/TERRITORY', var_type: String)\n",
    "                                xiii.    Contributor (var_name: 'CONTRIBUTOR', var_type: String)\n",
    "                                xiv.     Latitude (var_name: 'LATITUDE', var_type: String)\n",
    "                                xv.      Longitude (var_name: 'LONGITUDE', var_type: String)\n",
    "                                xvi.     Altitude (var_name: 'ALTITUDE', var_type: String)\n",
    "                                xvii.    Number of Sampling Heights\n",
    "                                         (var_name: 'NUMBER OF SAMPLING HEIGHTS', var_type: String)\n",
    "                                xviii.   Sampling Height (var_name: 'SAMPLING HEIGHTS', var_type: String)\n",
    "                                xix.     Contact Information - email (var_name: 'CONTACT POINT', var_type: String)\n",
    "                                xx.      Observation Parameter - e.g. 'CO' (var_name: 'PARAMETER', var_type: String)\n",
    "                                xxi.     Covering Time Period (var_name: 'COVERING PERIOD', var_type: String)\n",
    "                                xxii.    Sampling Freequency (var_name: 'TIME INTERVAL', var_type: String)\n",
    "                                xxiii.   Measurement Unit (var_name: 'MEASUREMENT UNIT', var_type: String)\n",
    "                                xxiv.    Measurement Method (var_name: 'MEASUREMENT METHOD', var_type: String)\n",
    "                                xxv.     Sampling Type (var_name: 'SAMPLING TYPE', var_type: String)\n",
    "                                xxvi.    Time Zone (var_name: 'TIME ZONE', var_type: String)\n",
    "                                xxvii.   Measurement Scale (var_name: 'MEASUREMENT SCALE', var_type: String)\n",
    "                                xxviii.  Data Policy (var_name: 'DATA POLICY', var_type: String)\n",
    "                                xxix.    Comment Notes (var_name: 'COMMENT', var_type: String)\n",
    "                                \n",
    "                            2. MetadataInfo (var_name: 'MetadataInfo', var_type: String)\n",
    "                            \n",
    "                            \n",
    "    \"\"\"\n",
    "    \n",
    "    #Split the metadata values for label \"comment\":\n",
    "    metadata_split = data.split('\\n#   ')\n",
    "    \n",
    "    #Get the metadata rows:\n",
    "    metadata_rows = metadata_split.pop(0).split('\\n')\n",
    "    \n",
    "    #Remove \"# \" from rows:\n",
    "    metadata_rows = [metadata_rows[i].replace(\"# \", \"\") for i in range(len(metadata_rows))]\n",
    "    \n",
    "    #Split labels from values:\n",
    "    metadata_rows_split = [metadata_rows[i].split(': ') for i in range(len(metadata_rows))]\n",
    "    \n",
    "    #Get the metadata labels:\n",
    "    metadata_labels = [metadata_rows_split[i].pop(0) for i in range(len(metadata_rows_split))]\n",
    "    \n",
    "    #Remove \":\" from label \"COMMENT:\"\n",
    "    metadata_labels[-1]= metadata_labels[-1].replace(\":\", \"\")\n",
    "    \n",
    "    #Join the metadata values for the label \"comment\" to one string:\n",
    "    comment_values = \", \".join(metadata_split)\n",
    "    \n",
    "    #Add the metadata values for the label \"comment\" to the metadata-values list:\n",
    "    metadata_rows_split[-1] = [comment_values]\n",
    "    \n",
    "    #Construct the metadata-values list as a list of strings instead of a list of lists:\n",
    "    metadata_values = [metadata_rows_split[i].pop(0) for i in range(len(metadata_rows_split))]\n",
    "    \n",
    "    #Create a dictionary with the metadata-label and -values lists:\n",
    "    metadata_dict = {'MetadataLabel':metadata_labels,\n",
    "                     'MetadataInfo':metadata_values}\n",
    "    \n",
    "    #Create Metadata dataframe:\n",
    "    df_metadata = pd.DataFrame(metadata_dict,columns=['MetadataLabel', 'MetadataInfo'])\n",
    "    \n",
    "    #Make the 'MetadataLabel'-column an index:\n",
    "    meta_df_i = df_metadata.set_index(keys=['MetadataLabel'],inplace=False)\n",
    "\n",
    "    #Return Metadata dataframe:\n",
    "    return meta_df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2dataframe(data, tracer, level=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:40:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:40:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Split an ICOS Level-1 or Level-2 Amtosheric Data text file to metadata and data parts.\n",
    "                      Call functions to read-in the metadata-text to a pandas dataframe and the data-text to\n",
    "                      another pandas dataframe. Return the newly created dataframes as output.\n",
    "                      \n",
    "    Input parameters: 1. Text containing ICOS Level-1 or Level-2 Atmospheric metadata and observation-data\n",
    "                         (var_name: 'data', var_type: String)\n",
    "                      2. Name of gas/tracer - e.g. 'co2' or 'co' or 'ch4'\n",
    "                         (var_name: 'tracer', var_type: String)\n",
    "                      3. Data level [optional]\n",
    "                         (var_name: 'level', var_type: Integer)\n",
    "    \n",
    "    Default value for level: The default value for data level is \"2\". Function calls for Level-2 data do not have\n",
    "                             to include a value for the level input parameter.\n",
    "    \n",
    "    Output:           2 pandas dataframes: metadata pandas dataframe, data pandas dataframe \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Split data to a list containing metadata and data values:\n",
    "    data_split = data.split('\\n#\\n#')\n",
    "    \n",
    "    #datasplit[0] -- > contains metadata\n",
    "    #datasplit[1] -- > contains data\n",
    "    \n",
    "    #Call function to create the ICOS Metadata dataframe:\n",
    "    df_metadata = icosMetadatadf(data_split[0])\n",
    "    \n",
    "    #Call function to create the ICOS Data dataframe:\n",
    "    df_data = icosDatadf(data_split[1], tracer, level)\n",
    "    \n",
    "    return df_metadata, df_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that converts data with data type \"bytes\" to data type \"string\":\n",
    "def byte2string(databytes):\n",
    "   \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:40:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:40:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Convert data from bytes to string.\n",
    "                      \n",
    "    Input parameters: Binary data containing ICOS Level-1 or Level-2 Atmospheric metadata and observation-data \n",
    "                      (var_name: 'data', var_type: String)\n",
    "    \n",
    "    Output:           Text data containing ICOS Level-1 or Level-2 Atmospheric metadata and observation-data.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert data to string:\n",
    "    datastring = databytes.decode(\"utf-8\")\n",
    "    \n",
    "    #Return converted data:\n",
    "    return datastring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that unzips a file at a given directory:\n",
    "def unzip(fullpath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:35:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:35:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Unzip zipped ICOS Level-1 or Level-2 Atmospheric Data Files.\n",
    "                      \n",
    "    Input parameters: Path to ICOS Level-1 or Level-2 Atmospheric Data file (var_name: 'fullpath', var_type: String)\n",
    "    \n",
    "    Output:           Unzipped data file.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Open zipfile in reading mode:\n",
    "    with zipfile.ZipFile(fullpath, 'r') as zf:\n",
    "        \n",
    "        #Store the unzipped file in the same directory as the zipped one\n",
    "        #zip_ref.extractall(pathtodir)\n",
    "        try:\n",
    "            data = zf.read(zf.namelist()[0])\n",
    "        except KeyError:\n",
    "            print('ERROR: Did not find %s in zip file' % zf.namelist()[0])\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ICOS_zipfile(filename, tracer, level=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Project:         'ICOS Carbon Portal'\n",
    "    Created:          Tue Oct 08 08:30:00 2019\n",
    "    Last Changed:     Tue Oct 08 08:30:00 2019\n",
    "    Version:          1.0.0\n",
    "    Author(s):        Karolina\n",
    "    \n",
    "    Description:      Function that unzips an ICOS data file, checks if the unzipped file contain binary or text data,\n",
    "                      converts the binary data to to text and returns a pandas dataframe with metadata & \n",
    "                      a pandas dataframe with observation data.\n",
    "                      \n",
    "    Input parameters: 1. File name for ICOS Level-1 or Level-2 Atmospheric Data File\n",
    "                         (var_name: 'filename', var_type: String).\n",
    "                      2. Name of gas/tracer - e.g. 'co2' or 'co' or 'ch4'\n",
    "                         (var_name: 'tracer', var_type: String)\n",
    "                      3. Data level [optional]\n",
    "                         (var_name: 'level', var_type: Integer)\n",
    "    \n",
    "    Default value for level: The default value for data level is \"2\". Function calls for Level-2 data do not have\n",
    "                             to include a value for the level input parameter.                      2. \n",
    "    \n",
    "    Output:           2 pandas dataframes: metadata pandas dataframe, data pandas dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Unzip file: \n",
    "    data = unzip(filename) #retunrs data type bytes in python 3.x (instead of string)\n",
    "\n",
    "    #Check data type & convert to string:\n",
    "    if (type(data) == bytes):\n",
    "        data = byte2string(data)\n",
    "\n",
    "    #Call function to create a pandas dataframe for metadata & one for data:\n",
    "    df_metadata, df_data = str2dataframe(data, tracer, level)\n",
    "    \n",
    "    #Return data & metadata dataframes:\n",
    "    return df_metadata, df_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ute/Stations/Claudio/helper_functions.py\n",
    "\"\"\"\n",
    "Created on Wed Oct  3 2018\n",
    "Last change on Nov  1 2018\n",
    "@author: Claudio D'Onofrio\n",
    "\"\"\"\n",
    "\n",
    "__version__= \"0.1.0\"\n",
    "\n",
    "# create helper functions\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "def is_number(num):\n",
    "    \"\"\" check if we deal with a number \"\"\"\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "def checklib(module):\n",
    "    \"\"\" load a list of modoules if available, otherwise throw exception \"\"\"\n",
    "    import imp\n",
    "    for mod in module:\n",
    "        try:\n",
    "            imp.find_module(mod)\n",
    "            ret = 1\n",
    "        except ImportError as imperror:\n",
    "            print(imperror)\n",
    "            ret = 0\n",
    "    return ret\n",
    "\n",
    "#---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ute/Stations/Claudio/sparqls.py\n",
    "\"\"\"\n",
    "Created on Thu Nov 22 16:35:50 2018\n",
    "contains functions, returning complete sparql queries,\n",
    "to run against the ICOS Carbon Portal RDF Triple Store\n",
    "@author: Claudio D'Onofrio\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__version__ = \"0.1.0\"\n",
    "\n",
    "# personal storage of sparql queries \n",
    "# -----------------------------------------\n",
    "\n",
    "def atc_query(tracer,level=2):\n",
    "    \"\"\"\n",
    "        Return SPARQL query to get a list of\n",
    "        ICOS Atmospheric CO2, CO or MTO, level 2 or level 1 (=NRT) data objects\n",
    "       :return: SPARQL query to get all ATC Level <level> products for tracer <tracer>\n",
    "       :rtype: string \n",
    "    \"\"\"\n",
    "    tracer = tracer.lower().title()\n",
    "    dataobject = [\"NrtGrowingDataObject\",\"L2DataObject\"]\n",
    "    \n",
    "    query = \"\"\"\n",
    "        prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        select ?dobj ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd\n",
    "        FROM <http://meta.icos-cp.eu/resources/atmprodcsv/>\n",
    "        where {\n",
    "                BIND(<http://meta.icos-cp.eu/resources/cpmeta/atc\"\"\"+tracer+dataobject[level-1]+\"\"\"> AS ?spec)\n",
    "                ?dobj cpmeta:hasObjectSpec ?spec .\n",
    "\t\n",
    "                FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}\n",
    "                ?dobj cpmeta:hasSizeInBytes ?size .\n",
    "                ?dobj cpmeta:hasName ?fileName .\n",
    "                ?dobj cpmeta:wasSubmittedBy [\n",
    "                prov:endedAtTime ?submTime ;\n",
    "                prov:wasAssociatedWith ?submitter\n",
    "                ] .\n",
    "                ?dobj cpmeta:hasStartTime | (cpmeta:wasAcquiredBy / prov:startedAtTime) ?timeStart .\n",
    "                ?dobj cpmeta:hasEndTime | (cpmeta:wasAcquiredBy / prov:endedAtTime) ?timeEnd .\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "    return query\n",
    "##------------------------------------------------------------------------------\n",
    "    \n",
    "def atc_stationlist(station,tracer='co2',level=2):\n",
    "    \"\"\"\n",
    "        Return SPARQL query to get a list of\n",
    "        ICOS Atmospheric CO2, CO or MTO, level 2 or level 1 (=NRT) data objects\n",
    "        for all stations in list\n",
    "       :return: SPARQL query to get all ATC products for specific stations, tracer and ICOS-level\n",
    "       :rtype: string \n",
    "    \"\"\"\n",
    "    tracer = tracer.lower().title()\n",
    "    dataobject = [\"NrtGrowingDataObject\",\"L2DataObject\"]\n",
    "    \n",
    "    if type(station) == str:\n",
    "        station = [station]\n",
    "    strUrl=\" \"\n",
    "    for ist in station:\n",
    "        strUrl = strUrl + \" \" + \"\"\"<http://meta.icos-cp.eu/resources/stations/AS_\"\"\"+ist+\"\"\">\"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/>\n",
    "    prefix prov: <http://www.w3.org/ns/prov#>\n",
    "    select ?dobj ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd\n",
    "    FROM <http://meta.icos-cp.eu/resources/atmprodcsv/>\n",
    "    where {\n",
    "        BIND(<http://meta.icos-cp.eu/resources/cpmeta/atc\"\"\"+tracer+dataobject[level-1]+\"\"\"> AS ?spec)\n",
    "        ?dobj cpmeta:hasObjectSpec ?spec .\n",
    "        VALUES ?station {\"\"\"+strUrl+\"\"\"} ?dobj cpmeta:wasAcquiredBy/prov:wasAssociatedWith ?station .\n",
    "        FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}\n",
    "        ?dobj cpmeta:hasSizeInBytes ?size .\n",
    "        ?dobj cpmeta:hasName ?fileName .\n",
    "        ?dobj cpmeta:wasSubmittedBy [\n",
    "            prov:endedAtTime ?submTime ;\n",
    "            prov:wasAssociatedWith ?submitter\n",
    "        ] .\n",
    "        ?dobj cpmeta:hasStartTime | (cpmeta:wasAcquiredBy / prov:startedAtTime) ?timeStart .\n",
    "        ?dobj cpmeta:hasEndTime | (cpmeta:wasAcquiredBy / prov:endedAtTime) ?timeEnd .\n",
    "        }\n",
    "        \"\"\"\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ICOS_filename(station,tracer='co2',level=2,download=False,path_data=''):\n",
    "    # %load /home/ute/Stations/Claudio/atc_co2_l2.py\n",
    "    \"\"\"\n",
    "    hack the carbon portal\n",
    "    download data files directly from \n",
    "    the carbon portal. \n",
    "    Created on Thu Nov 22 17:17:27 2018\n",
    "\n",
    "    @author: Claudio\n",
    "    \"\"\"\n",
    "\n",
    "    __version__ = \"0.1.0\"\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "    import sys\n",
    "    #import sparqls\n",
    "    #import helper_functions as h\n",
    "\n",
    "    # set the list of necessary modules to run the code\n",
    "    modules = [\"os\", \"requests\", \"pandas\", \"tqdm\"]\n",
    "\n",
    "    # check if the modules are available and load them, otherwise stop execution\n",
    "    #if not h.checklib(modules):\n",
    "    if not checklib(modules):\n",
    "        sys.exit(\"module dependencies are not fulfilled\")\n",
    "\n",
    "    else:\n",
    "        import os\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "        from tqdm import tqdm\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # this is the bit, where the sparql query is sent and we expect\n",
    "    # a list of dataobject    \n",
    "    url = 'https://meta.icos-cp.eu/sparql'\n",
    "\n",
    "    r = requests.get(url, params={\n",
    "        'format': 'json',\n",
    "        'query': atc_stationlist(station,tracer=tracer,level=level)})\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # convert the the result into a table\n",
    "    # output is an array, where each row contains\n",
    "    # information about the data object\n",
    "\n",
    "    cols = data['head']['vars']\n",
    "    datatable = []\n",
    "\n",
    "    for row in data['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "\n",
    "        datatable.append(item)\n",
    "\n",
    "    # print the table if you want\n",
    "    dt = pd.DataFrame(datatable, columns=cols)\n",
    "    #print(dt.head(5))\n",
    "    #print(dt.fileName)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # download all ATC CO2 L2 files\n",
    "\n",
    "    if download:\n",
    "        print(\"download all files: \",dt.fileName)\n",
    "        if path_data == '':\n",
    "            HOME = os.path.expanduser(\"~\")\n",
    "            path_data = HOME+'/tmp_icos/'\n",
    "        if not os.path.exists(path_data):\n",
    "            os.makedirs(path_data, exist_ok=True)\n",
    "        print(\"download all files: \",dt.fileName, ' to ', path_data)\n",
    "\n",
    "        # now loop through the results, and download the corresponding file\n",
    "        # files are download directly to the folder where this script is\n",
    "        # located. If the file does already exist it will be skipped.\n",
    "\n",
    "        for idx in dt.index:\n",
    "            if os.path.isfile(path_data+dt.fileName[idx]):\n",
    "                print(\"file already exists, skip...\"+dt.fileName[idx])\n",
    "            else:    \n",
    "                # a little hack to provide \"yes\" to the licence agreement\n",
    "                prefix = \"https://data.icos-cp.eu/licence_accept?ids=%5B%22\"\n",
    "                suffix = \"%22%5D\"\n",
    "                url = dt.dobj[idx]\n",
    "                url = url.split(\"/\")\n",
    "                url = prefix + url[4] + suffix\n",
    "        \n",
    "                # print(url)\n",
    "                response = requests.get(url, stream=True)\n",
    "                with open(path_data+dt.fileName[idx], \"wb\") as handle:\n",
    "                    for data in tqdm(response.iter_content()):\n",
    "                        handle.write(data)\n",
    "\n",
    "        print(\"all done\")\n",
    "\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ICOS_list(tracer,level=2):\n",
    "    # %load /home/ute/Stations/Claudio/atc_co2_l2.py\n",
    "    \"\"\"\n",
    "    hack the carbon portal\n",
    "    download data files directly from \n",
    "    the carbon portal. \n",
    "    Created on Thu Nov 22 17:17:27 2018\n",
    "\n",
    "    @author: Claudio\n",
    "    \"\"\"\n",
    "\n",
    "    __version__ = \"0.1.0\"\n",
    "\n",
    "    # download all ATC CO2 L2 files\n",
    "    # ---------------------------------------\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    import sys\n",
    "    #import sparqls\n",
    "    #import helper_functions as h\n",
    "\n",
    "    # set the list of necessary modules to run the code\n",
    "    modules = [\"os\", \"requests\", \"pandas\", \"tqdm\"]\n",
    "\n",
    "    # check if the modules are available and load them, otherwise stop execution\n",
    "    if not checklib(modules):\n",
    "        sys.exit(\"module dependencies are not fulfilled\")\n",
    "\n",
    "    else:\n",
    "        import os\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "        from tqdm import tqdm\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # this is the bit, where the sparql query is sent and we expect\n",
    "    # a list of dataobject    \n",
    "    url = 'https://meta.icos-cp.eu/sparql'\n",
    "    r = requests.get(url, params={\n",
    "        'format': 'json',\n",
    "        'query': atc_query(tracer,level)})\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # convert the the result into a table\n",
    "    # output is an array, where each row contains\n",
    "    # information about the data object\n",
    "\n",
    "    cols = data['head']['vars']\n",
    "    datatable = []\n",
    "\n",
    "    for row in data['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "\n",
    "        datatable.append(item)\n",
    "\n",
    "    # print the table if you want\n",
    "    dt = pd.DataFrame(datatable, columns=cols)\n",
    "    #print(dt.head(5))\n",
    "\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFunctions defined for handling STILT output:\u001b[0;0m\n",
      "atc_query\n",
      "atc_stationlist\n",
      "available_STILT_dictionary\n",
      "byte2string\n",
      "checklib\n",
      "createDatetimeObjList\n",
      "create_STILT_dictionary\n",
      "get_ICOS_filename\n",
      "get_ICOS_list\n",
      "get_station_class\n",
      "icosDatadf\n",
      "icosMetadatadf\n",
      "is_number\n",
      "lonlat_2_ixjy\n",
      "plot_available_STILT\n",
      "read_ICOS_zipfile\n",
      "read_STILT_dictionary\n",
      "read_emissions\n",
      "read_stilt_timeseries\n",
      "str2dataframe\n",
      "unzip\n"
     ]
    }
   ],
   "source": [
    "# list all defined functions\n",
    "func = %who_ls function\n",
    "print (\"\\033[1m\" + \"Functions defined for handling STILT output:\" + \"\\033[0;0m\")\n",
    "for ff in func:\n",
    "    print (ff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
