{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d91472a-fd22-47f5-aa06-e9ae5c334723",
   "metadata": {},
   "source": [
    "<img src='https://www.icos-cp.eu/sites/default/files/2017-11/ICOS_CP_logo.png' width=400 align=right>\n",
    "\n",
    "# ICOS Carbon Portal Python Libraries\n",
    "\n",
    "This example uses a foundational library called `icoscp_core` which can be used to access time-series ICOS data that are <i>previewable</i> in the ICOS Data Portal. \"Previewable\" means that it is possible to visualize the data variables in the preview plot. The library can also be used to access (meta-)data from [ICOS Cities](https://citydata.icos-cp.eu/portal/) and [SITES](https://data.fieldsites.se/portal/) data repositories. \n",
    "\n",
    "General information on all ICOS Carbon Portal Python libraries can be found on our [help pages](https://icos-carbon-portal.github.io/pylib/). \n",
    "\n",
    "Documentation of the `icoscp_core` library, including information on running it locally, can also be found on [PyPI.org](https://pypi.org/project/icoscp_core/).\n",
    "\n",
    "Note that for running this example locally, authentication is required (see the `how_to_authenticate.ipynb` notebook).\n",
    "\n",
    "# Example: Access and work with atmospheric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de460f0-1516-493f-8d79-12deded0fc0f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb8e94-2b2c-48bf-8bde-60191e27a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icoscp_core.icos import data, meta, ATMO_STATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165704b-de55-4ae4-b8ce-c52781c63b65",
   "metadata": {},
   "source": [
    "## List stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c176133-d59f-4d59-a09f-cd922c2bb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations specific for the atmosphere domain (see example 1b and 1c for examples for the ecosystem and ocean domains) \n",
    "stations = meta.list_stations(ATMO_STATION)\n",
    "\n",
    "# Filter stations by country (e.g., Sweden, 'SE')\n",
    "filtered_stations = [\n",
    "    s for s in stations\n",
    "    if s.country_code == 'SE'\n",
    "]\n",
    "\n",
    "print(\"Filtered stations list:\")\n",
    "pd.DataFrame(filtered_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fec332-1a46-4f25-b504-c72069a74cdc",
   "metadata": {},
   "source": [
    "## View metadata for a selected station \n",
    "\n",
    "The example shows how to access some of the metadata associated with the station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e605437-0d45-4433-adff-0929c01933a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a station uri from list above\n",
    "station_uri = 'http://meta.icos-cp.eu/resources/stations/AS_HTM'\n",
    "station_meta = meta.get_station_meta(station_uri)\n",
    "\n",
    "# Print the station name\n",
    "print('Name:', station_meta.org.name)\n",
    "\n",
    "# Print the ICOS labeling date (available on ICOS stations)\n",
    "print('Got ICOS label on:', station_meta.specificInfo.labelingDate)\n",
    "\n",
    "print('Known staff, possibly former:')\n",
    "pd.DataFrame([\n",
    "    {\n",
    "        'first_name': memb.person.firstName,\n",
    "        'last_name': memb.person.lastName,\n",
    "        'role': memb.role.role.label,\n",
    "        'start': memb.role.start,\n",
    "        'end': memb.role.end\n",
    "    }\n",
    "    for memb in station_meta.staff\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e082e-2f12-444f-85b9-33fa31dcc947",
   "metadata": {
    "tags": []
   },
   "source": [
    "## See a list of data types\n",
    "\n",
    "Data types are the most important category of classification of data objects. They exist to combine a number of other metadata elements. Data objects are associated with data types instead of being linked to these numerous other metadata. In the following example, filters are applied so that only data types associated with ICOS Level 2 data from the atmospheric domain that are previewable are shown. See more information [about data levels](https://www.icos-cp.eu/data-services/data-collection/data-levels-quality) here. Additional filters can be applied. Please refer to the documentation for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439711aa-fe18-481c-bf45-8b680ce87966",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_types = meta.list_datatypes()\n",
    "\n",
    "# filters applied:\n",
    "# data types with data access (possible to view with Python)\n",
    "# data types with level 2 data\n",
    "# data types associated with the atmospheric theme\n",
    "selected_datatypes = [\n",
    "    dt for dt in all_data_types\n",
    "    if dt.has_data_access and dt.data_level==2 and dt.theme.uri=='http://meta.icos-cp.eu/resources/themes/atmosphere'\n",
    "]\n",
    "for data_type in selected_datatypes:\n",
    "    print(f\"{data_type.label} ({data_type.uri})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fee990-be4e-40cf-9a5f-33e72d9aafab",
   "metadata": {},
   "source": [
    "## Find data objects based on the selected station and a specified data type\n",
    "\n",
    "This example shows how to get a list of data objects associated with the selected station and the data type \"ICOS ATC/CAL Flask Release\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f17fd-b918-4200-9f32-e527f9f0b2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify a data type from the list above \n",
    "data_type = 'http://meta.icos-cp.eu/resources/cpmeta/atcFlaskDataObject'\n",
    "\n",
    "station_data_objects = meta.list_data_objects(datatype = data_type, \n",
    "                                         station = station_uri)\n",
    "\n",
    "for station_data_object in station_data_objects:\n",
    "    print(station_data_object.filename)\n",
    "\n",
    "if len(station_data_objects) == 0:\n",
    "    print(f'No available objects with data type {data_type} at station {station_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbda2ca-dc1e-41d2-b807-427c91dd3e69",
   "metadata": {},
   "source": [
    "## Access data for a single data object \n",
    "\n",
    "This example shows how to access the data and metadata from ICOS_ATC_L2_L2-2024.1_HTM_150.0_CTS_FLASK_CO2.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af3541-a30e-4e22-aad4-259c391cc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select a filename from the list above\n",
    "filename = 'ICOS_ATC_L2_L2-2024.1_HTM_150.0_CTS_FLASK_CO2.zip'\n",
    "selected_data_object = next((station_data_object for station_data_object in station_data_objects if station_data_object.filename == filename), None)\n",
    "\n",
    "if selected_data_object:\n",
    "\n",
    "    # Access full metadata associated with the data object\n",
    "    # NOTE this is an expensive operation, ~ 100 ms\n",
    "    dobj_meta = meta.get_dobj_meta(selected_data_object)\n",
    "    \n",
    "    # Access the object's data; relies on full metadata, which is expensive to fetch\n",
    "    # see below for example of batch data fetching\n",
    "    dobj_arrays = data.get_columns_as_arrays(dobj_meta)\n",
    "    \n",
    "    # Convert to a pandas dataframe\n",
    "    df = pd.DataFrame(dobj_arrays)\n",
    "\n",
    "    display(df)\n",
    "else:\n",
    "    print('The list of objects was empty, check the variable \"filename\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae5b52-b023-4bc1-b1d2-384fedf9c376",
   "metadata": {},
   "source": [
    "## Make a plot: single data column\n",
    "\n",
    "The selected data_object that has been accessed contains data for CO2 (stored in the \"co2\" column in the DataFrame above). For this particular data type (ICOS ATC/CAL Flask Release), different data objects contain data for different gases which are stored in different columns. It is also worth noting that in different data types the names of the columns containing the observation timestamp and quality flag may also differ (depends on the conventions used by corresponding thematic center or scientific community).\n",
    "\n",
    "Before the data is plotted, the \"Flag\" column is used to exclude data that has not undergone/passed manual quality control. It should be noted that by default, the data-fetching methods of the ICOS Python library automatically exclude data points that have been explicitly flagged as bad. This behaviour is controlled by `keep_bad_data` boolean parameter of methods `get_columns_as_arrays` and `batch_get_columns_as_arrays` (equals to `False` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92c2b8-5ccc-4319-8382-627f5bf4f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to ensure presence of a column in a pandas DataFrame\n",
    "def assert_col(df: pd.DataFrame, col: str) -> None:\n",
    "    assert col in df.columns, f\"Column '{col}' not found in data. Choose among {list(df.columns)}\"\n",
    "\n",
    "def assert_cols(df: pd.DataFrame, cols: list[str]) -> None:\n",
    "    for col in cols:\n",
    "        assert_col(df, col)\n",
    "\n",
    "# List all the programmatically accessible columns\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc56ddf-adf0-4cd3-a45a-89ac9a7d4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column = 'SamplingStart'\n",
    "data_column = 'co2'\n",
    "quality_flag = 'Flag'\n",
    "value_accept_quality = 'O'\n",
    "\n",
    "# make sure the required columns are present\n",
    "assert_cols(df, [time_column, data_column, quality_flag])\n",
    "\n",
    "# use the quality flag to keep only data marked as good after manual quality control\n",
    "df_quality = df[df[quality_flag] == value_accept_quality]\n",
    "\n",
    "# metadata part specific to observational time series data collected at stations\n",
    "# (dobj_meta was initialized in \"Access data\" section above)\n",
    "ts_meta = dobj_meta.specificInfo\n",
    "\n",
    "station_name = ts_meta.acquisition.station.org.name\n",
    "\n",
    "# dictionary to look up value type information by dataset column\n",
    "column_value_types = {\n",
    "    col.label: col.valueType\n",
    "    for col in ts_meta.columns\n",
    "}\n",
    "\n",
    "# find metadata associated with the selected columns (data_column and time_column)\n",
    "x_value_type = column_value_types[time_column]\n",
    "y_value_type = column_value_types[data_column]\n",
    "\n",
    "# create axis labels based on the metadata\n",
    "x_axis_label = f\"{x_value_type.self.label} ({time_column})\"\n",
    "y_axis_label = f\"{y_value_type.self.label} [{y_value_type.unit}]\"\n",
    "\n",
    "plot = df_quality.plot(x=time_column, y=data_column, grid=True, title=station_name, style='o', markersize=3)\n",
    "plot.set(xlabel=x_axis_label, ylabel=y_axis_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6b6f9-b63c-4ec2-9ef7-060a411a626f",
   "metadata": {},
   "source": [
    "## Combine data for all data objects given the station and data type\n",
    "\n",
    "Combination of selected data columns from objects in the list \"station_data_objects\".\n",
    "\n",
    "All objects will be considered, but will only be added if they have data stored in columns with the names listed in \"data_columns\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c4e1a-0f7c-403b-9b23-ad1344937ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column = 'SamplingStart'\n",
    "# In the example list of station_data_objects, additional columns include 'co', 'sf6', 'ch4', and 'h2'. These can be added to this list.\n",
    "# For other selections, consider the print statements in the output of this cell.\n",
    "data_columns = ['co2', '14C', 'n2o']\n",
    "# all data objects have the same column and value for the quality flag \n",
    "quality_flag = 'Flag'\n",
    "value_accept_quality = 'O'\n",
    "\n",
    "# Save the names of the data_columns that are found in the object dataframes\n",
    "# Their associated metadata is saved in y_axis labels for use in later plots.\n",
    "renamed_data_columns = []\n",
    "y_axis_labels = []\n",
    "\n",
    "# initiate the final df\n",
    "merged_df = pd.DataFrame(columns=[time_column])\n",
    "\n",
    "for dobj, arrs in data.batch_get_columns_as_arrays(station_data_objects):\n",
    "\n",
    "    # Convert the arrays into a DataFrame\n",
    "    df = pd.DataFrame(arrs)\n",
    "\n",
    "    # use the quality flag to keep only best-quality data (maked \"O\" in column \"Flag\")\n",
    "    df_quality = df[df[quality_flag] == value_accept_quality].copy()\n",
    "\n",
    "    # Check if the time_column is available in the dataframe columns associated with the data object\n",
    "    # If not, the users need to look at available columns and find correct column names\n",
    "    if time_column not in df_quality.columns:\n",
    "        print(f\"The column given for time ('{time_column}') is not found in {dobj.filename}. Skipping this object.\")\n",
    "        print(f\"Available columnns are '{list(df_quality.columns)}'\")\n",
    "        continue\n",
    "\n",
    "    # New column names for the final df_quality (to distinuigh between the different data objects)\n",
    "    # based on station's id and sampling height (if available)\n",
    "    station_id = dobj.station_uri.split('_')[-1]\n",
    "\n",
    "    if dobj.sampling_height:\n",
    "        suffix = f\"_{station_id}_{dobj.sampling_height}\"\n",
    "\n",
    "    else:\n",
    "        suffix = f\"_{station_id}\"\n",
    "\n",
    "    # See which of the desired data_columns are available in this data object\n",
    "    # Rename these for unique column names in the final merged_df that is updated with each iteration of the data objects\n",
    "    # Save metadata associated with the found columns\n",
    "    found_columns = []\n",
    "\n",
    "    for data_column in data_columns:\n",
    "        if data_column in df_quality.columns:\n",
    "\n",
    "            found_columns.append(data_column)\n",
    "\n",
    "            # Rename the column with the suffix\n",
    "            df_quality.rename(columns={data_column: data_column + suffix}, inplace=True)\n",
    "            \n",
    "            if data_column + suffix not in renamed_data_columns:\n",
    "                renamed_data_columns.append(data_column + suffix)\n",
    "                \n",
    "                # find y-axis label for column (used in graph)\n",
    "                dobj_meta = meta.get_dobj_meta(dobj)\n",
    "                columns_meta = dobj_meta.specificInfo.columns\n",
    "                dobj_value_type = [col for col in columns_meta if col.label==data_column][0].valueType\n",
    "                y_axis_labels.append(f\"{dobj_value_type.self.label} [{dobj_value_type.unit}]\")\n",
    "\n",
    "    # If any of the columns were found, merge them with the merged DataFrame\n",
    "    if found_columns:\n",
    "        # Select only the relevant columns (timestamp + renamed columns)\n",
    "        columns_to_merge = [time_column] + [col + suffix for col in found_columns]\n",
    "        merged_df = pd.merge(merged_df, df_quality[columns_to_merge], on=time_column, how='outer')\n",
    "\n",
    "    else:\n",
    "        print(f\"None of '{data_columns}' found for {dobj.filename}. Skipping this object.\")\n",
    "        print(f\"Available columns are '{list(df_quality.columns)}'\")\n",
    "\n",
    "# time_column should be in datetime format. If not alrady, it will be convert to it here:\n",
    "merged_df[time_column] = pd.to_datetime(merged_df[time_column])\n",
    "\n",
    "# Make sure it is in the right order\n",
    "merged_df = merged_df.sort_values(time_column)\n",
    "\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce662b-155d-4ef7-b10d-9d836c3747a7",
   "metadata": {},
   "source": [
    "## Make a plot: multiple data columns\n",
    "\n",
    "Not suitable for plotting of different species, as they often have different value ranges. A better plot for our example selection will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bb134-c7e1-4708-90e4-11c953b0fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = [f\"{col} ({unit})\" for col, unit in zip(renamed_data_columns, y_axis_labels)]\n",
    "\n",
    "# Plot the data\n",
    "ax = merged_df.plot(x=time_column, y=renamed_data_columns, grid=True, style='o', markersize=3)\n",
    "\n",
    "# Update the legend with the new labels\n",
    "ax.legend(legend_labels)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e82739-642f-40e3-81a1-81fbbf7ded88",
   "metadata": {},
   "source": [
    "### Zoomed in to latest year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0cca8-7612-4991-98aa-19b931397ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest year based on the time_column\n",
    "latest_year = merged_df[time_column].dt.year.max()\n",
    "\n",
    "# Filter the DataFrame to include only rows from the latest year\n",
    "merged_df_latest_year = merged_df[merged_df[time_column].dt.year == latest_year]\n",
    "\n",
    "# Plot the data\n",
    "ax = merged_df_latest_year.plot(x=time_column, y=renamed_data_columns, grid=True, style='o', markersize=3)\n",
    "\n",
    "# Update the legend with the new labels\n",
    "ax.legend(legend_labels)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852d7d6-de2d-43d7-be29-93e8599ded97",
   "metadata": {},
   "source": [
    "## Make a plot: two data columns on different axes\n",
    "\n",
    "Possible for two of the data columns. Even if more are given, only the first two in the list \"selected_data_columns\" will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f08eb-5ab6-4df6-8e80-f040c742c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column = 'SamplingStart'\n",
    "\n",
    "# Select two of the data column in dataframe \"merged_df\"\n",
    "selected_data_columns = ['14C_HTM_150.0', 'co2_HTM_150.0']\n",
    "\n",
    "# Check if all selected columns are in renamed_data_columns\n",
    "missing_columns = set(selected_data_columns) - set(renamed_data_columns)\n",
    "\n",
    "if missing_columns or time_column not in merged_df.columns:\n",
    "    \n",
    "    print(f\"One or more of the columns ({selected_data_columns}), or the time_column ({time_column}), are not in merged_df.\")\n",
    "    print(f\"Available columns are: {merged_df.columns}\")  \n",
    "    \n",
    "else:\n",
    "\n",
    "    # Set up the plot with the first variable\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the first variable on the primary y-axis\n",
    "\n",
    "    # Find the unit \n",
    "    col_index_1 = renamed_data_columns.index(selected_data_columns[0])\n",
    "    y_axis_label_1 = y_axis_labels[col_index_1]\n",
    "\n",
    "    # b stands for blue and \".\" for circle markers\n",
    "    ax1.plot(merged_df[time_column], merged_df[selected_data_columns[0]], 'b.', markersize = 3)\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel(y_axis_label_1, color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Set the title with the station name\n",
    "    ax1.set_title(station_name)\n",
    "\n",
    "    if len(selected_data_columns) > 1:\n",
    "        \n",
    "        # Create a secondary y-axis for the second variable\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        # Plot the second variable on the secondary y-axis\n",
    "        col_index_2 = renamed_data_columns.index(selected_data_columns[1])\n",
    "        y_axis_label_2 = y_axis_labels[col_index_2]\n",
    "\n",
    "        # r stands for red and \".\" for circle markers\n",
    "        ax2.plot(merged_df[time_column], merged_df[selected_data_columns[1]], 'r.', markersize = 3)\n",
    "        ax2.set_ylabel(y_axis_label_2, color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "        \n",
    "    # show the dates in this specific format (YYYY-MM-DD)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    \n",
    "    # Rotate the dates to fit better\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Add grid\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.tight_layout()  # to make sure labels/axes don't overlap\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71fcd7-068a-4628-b299-ffe11b8cc77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
